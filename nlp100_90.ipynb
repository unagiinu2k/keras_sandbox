{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git add nlp100_90.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git commit -a -m \"added nlp100_90\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "conda install word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** 0. word2vecによる学習 ****\n",
    "\n",
    "81で作成したコーパスに対してword2vecを適用し，単語ベクトルを学習せよ．さらに，学習した単語ベクトルの形式を変換し，86-89のプログラムを動かせ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[python word2vecパッケージの公式ページ](https://github.com/danielfrg/word2vec)\n",
    "\n",
    "[official example](http://nbviewer.jupyter.org/github/danielfrg/word2vec/blob/master/examples/word2vec.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_output81 = \"nlp100data/output81.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_phrases90 = 'nlp100data/phrases90.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_wordvecs90bin = 'nlp100data/wordvecs90.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_wordvecs90 = 'nlp100data/wordvecs90.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file nlp100data/output81.txt\n",
      "Words processed: 11800K     Vocab size: 4364K  \n",
      "Vocab size (unigrams + bigrams): 2372876\n",
      "Words in train file: 11869261\n",
      "Words written: 11800K\r"
     ]
    }
   ],
   "source": [
    "word2vec.word2phrase(fname_output81 , fname_phrases90 , verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training using file nlp100data/phrases90.txt\n",
      "Vocab size: 105991\n",
      "Words in train file: 11192135\n",
      "Alpha: 0.000019  Progress: 99.94%  Words/thread/sec: 543.23k  "
     ]
    }
   ],
   "source": [
    "word2vec.word2vec(train = fname_phrases90 , output = fname_wordvecs90bin , size = 50 , threads = 4 ,  verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.load?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.load(fname = fname_wordvecs90bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** 86のword2vec版 ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06594494, -0.06648544, -0.02266973, -0.26417384,  0.0348038 ,\n",
       "        0.16276203, -0.10152841, -0.20559365,  0.11061635, -0.05128405,\n",
       "       -0.1579421 ,  0.18112618, -0.29654896,  0.11253238, -0.17636853,\n",
       "        0.06542736,  0.20164117,  0.07633649,  0.07534473, -0.24391665,\n",
       "       -0.09185803,  0.06577205, -0.01773164, -0.2031363 ,  0.08185295,\n",
       "       -0.02703902, -0.07227903, -0.06967267, -0.11732481, -0.12979949,\n",
       "       -0.14991304, -0.12434923,  0.23231378,  0.16840932,  0.25385424,\n",
       "        0.11206607,  0.08833051, -0.04896447, -0.00366609,  0.30139831,\n",
       "       -0.04291305,  0.04288597,  0.05653698,  0.11127364, -0.17320317,\n",
       "        0.02737914, -0.10143059, -0.04345994, -0.22382653,  0.08549703])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['United_States']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** 87のword2vec版 ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "v0 = model['United_States']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = model['U.S']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cosine関数の出力は２（逆向き）、１（直行）,0(一致)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity is : 0.13559367595294014\n"
     ]
    }
   ],
   "source": [
    "print(\"cosine similarity is : {}\".format(cosine(v0,v1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** 88のword2vec版 ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices , metrics = model.similar('England')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Scotland', 'France', 'Spain', 'Italy', 'Ireland', 'Germany',\n",
       "       'Athens', 'Norway', 'Wales', 'Denmark'], dtype='<U78')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** 89のword2vec版 ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes, metrics = model.analogy(pos=['Spain', 'Athens'], neg=['Madrid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Scotland', 'France', 'Spain', 'Italy', 'Ireland', 'Germany',\n",
       "       'Athens', 'Norway', 'Wales', 'Denmark'], dtype='<U78')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greece : 0.33474457800489593\n",
      "Portugal : 0.33438595937500565\n",
      "Russia : 0.31797912382704324\n",
      "Italy : 0.3117456270663885\n",
      "Hungary : 0.31073009398754203\n",
      "Austria : 0.31002071735408143\n",
      "Egypt : 0.3092890374345109\n",
      "Denmark : 0.3073950638275555\n",
      "France : 0.30542400875021986\n",
      "Poland : 0.3040470604395909\n"
     ]
    }
   ],
   "source": [
    "for idx , mtx in zip(indexes , metrics):\n",
    "    print(\"{} : {}\".format(model.vocab[idx] , mtx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** sandbox ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.word2vec(train = fname_output81 , output = fname_wordvecs90 , size = 50 , threads = 4 , binary = False , verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 57b5cda] 90 done\n",
      " 2 files changed, 243 insertions(+), 147 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "git commit -a -m \"90 done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install word2vec -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(fname_phrases90 , 'r' , 'utf-8') as rf:\n",
    "    for i, l in enumerate(rf):\n",
    "        print(l)\n",
    "        if i > 2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### アナロジーデータの準備\n",
    "\n",
    "単語アナロジーの評価データをダウンロードせよ．\n",
    "\n",
    "このデータ中で\": \"で始まる行はセクション名を表す．\n",
    "\n",
    "例えば，\": capital-common-countries\"という行は，\"capital-common-countries\"というセクションの開始を表している．\n",
    "\n",
    "ダウンロードした評価データの中で，\"family\"というセクションに含まれる評価事例を抜き出してファイルに保存せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/svn2github/word2vec/blob/master/questions-words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://raw.githubusercontent.com/svn2github/word2vec/master/questions-words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_out91 = 'nlp100data/family91.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nlp100data/questions-words.txt', 'rt') as rf , open(fname_out91 , 'wt') as wf:\n",
    "    isOutput = False\n",
    "    toWrite = list()\n",
    "    for l in rf:\n",
    "        if l[0] == \":\":\n",
    "            isOutput = False\n",
    "        \n",
    "        if isOutput:\n",
    "            toWrite.append(l)        \n",
    "        \n",
    "        if l.strip(': \\n') == 'family':\n",
    "            isOutput = True\n",
    "    wf.write(\"\".join(toWrite))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master f840836] done 91\n",
      " 1 file changed, 108 insertions(+), 3 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "git commit -a -m \"done 91\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### アナロジーデータへの適用\n",
    "\n",
    "91で作成した評価データの各事例に対して，vec(2列目の単語) - vec(1列目の単語) + vec(3列目の単語)を計算し，そのベクトルと類似度が最も高い単語と，その類似度を求めよ．\n",
    "\n",
    "求めた単語と類似度は，各事例の末尾に追記せよ．\n",
    "\n",
    "このプログラムを85で作成した単語ベクトル，90で作成した単語ベクトルに対して適用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_out92 = 'nlp100data/out92.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.load(fname = fname_wordvecs90bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fname_out91 , 'rt') as rf , open(fname_out92 , 'wt') as wf:\n",
    "    toWrite = list()\n",
    "    for l in rf:\n",
    "        words = l.strip('\\n').split(' ')\n",
    "        try:\n",
    "            indexes, metrics = model.analogy(pos=[words[1] , words[2]], neg=[words[0]] , n = 1)\n",
    "            found_word = model.vocab[indexes][0]\n",
    "            metric = metrics[0]\n",
    "        except:\n",
    "            found_word = \"\"\n",
    "            metric = \"NaN\"\n",
    "            \n",
    "        toWrite.append(\"{} {} {}\\n\".format(l.strip('\\n') , found_word , metric))\n",
    "    wf.write(\"\".join(toWrite))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uncle', 'aunt', 'stepson', 'stepdaughter']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.strip('\\n').split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'boy girl brother sister father 0.29267610246189557\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toWrite[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\".join(toWrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 4e78a89] 92 done\n",
      " 1 file changed, 139 insertions(+), 1 deletion(-)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "git commit -a -m \"92 done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### アナロジータスクの正解率の計算\n",
    "92で作ったデータを用い，各モデルのアナロジータスクの正解率を求めよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct / total = 185 / 506\n"
     ]
    }
   ],
   "source": [
    "with open(fname_out92 , 'rt') as rf:\n",
    "    t = 0\n",
    "    c = 0\n",
    "    for l in rf:\n",
    "        t += 1\n",
    "        words = l.strip('\\n').split(' ')\n",
    "        if words[3] == words[4]:\n",
    "            c += 1\n",
    "    print(\"correct / total = {} / {}\".format(c,t))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
