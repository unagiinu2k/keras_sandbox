{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "英語のテキスト（nlp.txt）に対して，以下の処理を実行せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** coreNLP **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** server ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "起動の仕方は：\n",
    "\n",
    "https://stanfordnlp.github.io/CoreNLP/corenlp-server.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/toshinao/PycharmProjects/keras_sandbox\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[main] INFO CoreNLP - --- StanfordCoreNLPServer#main() called ---\n",
      "[main] INFO CoreNLP - setting default constituency parser\n",
      "[main] INFO CoreNLP - warning: cannot find edu/stanford/nlp/models/srparser/englishSR.ser.gz\n",
      "[main] INFO CoreNLP - using: edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz instead\n",
      "[main] INFO CoreNLP - to use shift reduce parser download English models jar from:\n",
      "[main] INFO CoreNLP - http://stanfordnlp.github.io/CoreNLP/download.html\n",
      "[main] INFO CoreNLP -     Threads: 8\n",
      "[main] INFO CoreNLP - Starting server...\n",
      "java.net.BindException: Address already in use\n",
      "\tat sun.nio.ch.Net.bind0(Native Method)\n",
      "\tat sun.nio.ch.Net.bind(Net.java:433)\n",
      "\tat sun.nio.ch.Net.bind(Net.java:425)\n",
      "\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)\n",
      "\tat sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)\n",
      "\tat sun.net.httpserver.ServerImpl.<init>(ServerImpl.java:100)\n",
      "\tat sun.net.httpserver.HttpServerImpl.<init>(HttpServerImpl.java:50)\n",
      "\tat sun.net.httpserver.DefaultHttpServerProvider.createHttpServer(DefaultHttpServerProvider.java:35)\n",
      "\tat com.sun.net.httpserver.HttpServer.create(HttpServer.java:130)\n",
      "\tat edu.stanford.nlp.pipeline.StanfordCoreNLPServer.run(StanfordCoreNLPServer.java:1423)\n",
      "\tat edu.stanford.nlp.pipeline.StanfordCoreNLPServer.main(StanfordCoreNLPServer.java:1515)\n",
      "[Thread-0] INFO CoreNLP - CoreNLP Server is shutting down.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ~/Tools/stanford-corenlp-full-2018-02-27/\n",
    "java -cp \"*\" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** python binding ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "https://www.khalidalnajjar.com/setup-use-stanford-corenlp-server-python/\n",
    "に従って、\n",
    "\n",
    "https://github.com/Lynten/stanford-corenlp\n",
    "を試してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のようにすれば予めサーバを立てておかなくても良い："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corenlp = StanfordCoreNLP(r'/home/toshinao/Tools/stanford-corenlp-full-2018-02-27')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(ROOT\\n  (NP (NN nlp100data/nlp) (. .)))'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corenlp.parse(r'nlp100data/nlp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Guangdong University of Foreign Studies is located in Guangzhou.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Guangdong', 'University', 'of', 'Foreign', 'Studies', 'is', 'located', 'in', 'Guangzhou', '.']\n"
     ]
    }
   ],
   "source": [
    "print(corenlp.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Guangdong', 'NNP'), ('University', 'NNP'), ('of', 'IN'), ('Foreign', 'NNP'), ('Studies', 'NNPS'), ('is', 'VBZ'), ('located', 'JJ'), ('in', 'IN'), ('Guangzhou', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(corenlp.pos_tag(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Guangdong', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('of', 'ORGANIZATION'), ('Foreign', 'ORGANIZATION'), ('Studies', 'ORGANIZATION'), ('is', 'O'), ('located', 'O'), ('in', 'O'), ('Guangzhou', 'CITY'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(corenlp.ner(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP\n",
      "      (NP (NNP Guangdong) (NNP University))\n",
      "      (PP (IN of)\n",
      "        (NP (NNP Foreign) (NNPS Studies))))\n",
      "    (VP (VBZ is)\n",
      "      (ADJP (JJ located)\n",
      "        (PP (IN in)\n",
      "          (NP (NNP Guangzhou)))))\n",
      "    (. .)))\n"
     ]
    }
   ],
   "source": [
    "print(nlp.parse(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ROOT', 0, 7), ('compound', 2, 1), ('nsubjpass', 7, 2), ('case', 5, 3), ('compound', 5, 4), ('nmod', 2, 5), ('auxpass', 7, 6), ('case', 9, 8), ('nmod', 7, 9), ('punct', 7, 10)]\n"
     ]
    }
   ],
   "source": [
    "print(corenlp.dependency_parse(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 0. 文区切り **\n",
    "\n",
    "(. or ; or : or ? or !) → 空白文字 → 英大文字というパターンを文の区切りと見なし，入力された文書を1行1文の形式で出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_txt_path = 'nlp100data/nlp.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_lines(file_path):\n",
    "    r = re.compile(\"(^.*?[\\.|;|:|\\?|!])\\s([A-Z].*)\")\n",
    "    with codecs.open(file_path , 'r', 'utf-8') as rf:\n",
    "        for l in rf:\n",
    "            l = l.strip()\n",
    "            while len(l) > 0:\n",
    "                m = r.match(l)\n",
    "                if m:\n",
    "                    s0 = m.group(1)\n",
    "                    l = m.group(2)\n",
    "                    yield s0\n",
    "                else:\n",
    "                    yield l\n",
    "                    l = \"\"\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing\n",
      "From Wikipedia, the free encyclopedia\n",
      "Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages.\n",
      "As such, NLP is related to the area of humani-computer interaction.\n",
      "Many challenges in NLP involve natural language understanding, that is, enabling computers to derive meaning from human or natural language input, and others involve natural language generation.\n",
      "History\n",
      "The history of NLP generally starts in the 1950s, although work can be found from earlier periods.\n",
      "In 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence.\n",
      "The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English.\n",
      "The authors claimed that within three or five years, machine translation would be a solved problem.\n",
      "However, real progress was much slower, and after the ALPAC report in 1966, which found that ten year long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced.\n",
      "Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.\n",
      "Some notably successful NLP systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 to 1966.\n",
      "Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction.\n",
      "When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\".\n",
      "During the 1970s many programmers began to write 'conceptual ontologies', which structured real-world information into computer-understandable data.\n",
      "Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981).\n",
      "During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.\n",
      "Up to the 1980s, most NLP systems were based on complex sets of hand-written rules.\n",
      "Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing.\n",
      "This was due to both the steady increase in computational power resulting from Moore's Law and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.\n",
      "Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules.\n",
      "However, Part of speech tagging introduced the use of Hidden Markov Models to NLP, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data.\n",
      "The cache language models upon which many speech recognition systems now rely are examples of such statistical models.\n",
      "Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.\n",
      "Many of the notable early successes occurred in the field of machine translation, due especially to work at IBM Research, where successively more complicated statistical models were developed.\n",
      "These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government.\n",
      "However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems.\n",
      "As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.\n",
      "Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms.\n",
      "Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data.\n",
      "Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data.\n",
      "However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results.\n",
      "NLP using machine learning\n",
      "Modern NLP algorithms are based on machine learning, especially statistical machine learning.\n",
      "The paradigm of machine learning is different from that of most prior attempts at language processing.\n",
      "Prior implementations of language-processing tasks typically involved the direct hand coding of large sets of rules.\n",
      "The machine-learning paradigm calls instead for using general learning algorithms - often, although not always, grounded in statistical inference - to automatically learn such rules through the analysis of large corpora of typical real-world examples.\n",
      "A corpus (plural, \"corpora\") is a set of documents (or sometimes, individual sentences) that have been hand-annotated with the correct values to be learned.\n",
      "Many different classes of machine learning algorithms have been applied to NLP tasks.\n",
      "These algorithms take as input a large set of \"features\" that are generated from the input data.\n",
      "Some of the earliest-used algorithms, such as decision trees, produced systems of hard if-then rules similar to the systems of hand-written rules that were then common.\n",
      "Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature.\n",
      "Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.\n",
      "Systems based on machine-learning algorithms have many advantages over hand-produced rules:\n",
      "The learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not obvious at all where the effort should be directed.\n",
      "Automatic learning procedures can make use of statistical inference algorithms to produce models that are robust to unfamiliar input (e.g. containing words or structures that have not been seen before) and to erroneous input (e.g. with misspelled words or words accidentally omitted).\n",
      "Generally, handling such input gracefully with hand-written rules -- or more generally, creating systems of hand-written rules that make soft decisions -- extremely difficult, error-prone and time-consuming.\n",
      "Systems based on automatically learning the rules can be made more accurate simply by supplying more input data.\n",
      "However, systems based on hand-written rules can only be made more accurate by increasing the complexity of the rules, which is a much more difficult task.\n",
      "In particular, there is a limit to the complexity of systems based on hand-crafted rules, beyond which the systems become more and more unmanageable.\n",
      "However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process.\n",
      "The subfield of NLP devoted to learning approaches is known as Natural Language Learning (NLL) and its conference CoNLL and peak body SIGNLL are sponsored by ACL, recognizing also their links with Computational Linguistics and Language Acquisition.\n",
      "When the aims of computational language learning research is to understand more about human language acquisition, or psycholinguistics, NLL overlaps into the related field of Computational Psycholinguistics.\n"
     ]
    }
   ],
   "source": [
    "for l in nlp_lines(nlp_txt_path):\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** sandbox ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile(\"(^.*?[\\.|;|:|\\?|!])\\s([A-Z].*)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = r.match(run_list[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As such, NLP is related to the area of humani-computer interaction. Many challenges in NLP involve natural language understanding, that is, enabling computers to derive meaning from human or natural language input, and others involve natural language generation.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.group(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing\n",
      "\n",
      "From Wikipedia, the free encyclopedia\n",
      "\n",
      "\n",
      "\n",
      "Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages. As such, NLP is related to the area of humani-computer interaction. Many challenges in NLP involve natural language understanding, that is, enabling computers to derive meaning from human or natural language input, and others involve natural language generation.\n",
      "\n",
      "\n",
      "\n",
      "History\n",
      "\n",
      "\n",
      "\n",
      "The history of NLP generally starts in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence.\n",
      "\n",
      "\n",
      "\n",
      "The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem. However, real progress was much slower, and after the ALPAC report in 1966, which found that ten year long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.\n",
      "\n",
      "\n",
      "\n",
      "Some notably successful NLP systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 to 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\".\n",
      "\n",
      "\n",
      "\n",
      "During the 1970s many programmers began to write 'conceptual ontologies', which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.\n",
      "\n",
      "\n",
      "\n",
      "Up to the 1980s, most NLP systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power resulting from Moore's Law and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing. Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, Part of speech tagging introduced the use of Hidden Markov Models to NLP, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.\n",
      "\n",
      "\n",
      "\n",
      "Many of the notable early successes occurred in the field of machine translation, due especially to work at IBM Research, where successively more complicated statistical models were developed. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.\n",
      "\n",
      "\n",
      "\n",
      "Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results.\n",
      "\n",
      "\n",
      "\n",
      "NLP using machine learning\n",
      "\n",
      "\n",
      "\n",
      "Modern NLP algorithms are based on machine learning, especially statistical machine learning. The paradigm of machine learning is different from that of most prior attempts at language processing. Prior implementations of language-processing tasks typically involved the direct hand coding of large sets of rules. The machine-learning paradigm calls instead for using general learning algorithms - often, although not always, grounded in statistical inference - to automatically learn such rules through the analysis of large corpora of typical real-world examples. A corpus (plural, \"corpora\") is a set of documents (or sometimes, individual sentences) that have been hand-annotated with the correct values to be learned.\n",
      "\n",
      "\n",
      "\n",
      "Many different classes of machine learning algorithms have been applied to NLP tasks. These algorithms take as input a large set of \"features\" that are generated from the input data. Some of the earliest-used algorithms, such as decision trees, produced systems of hard if-then rules similar to the systems of hand-written rules that were then common. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature. Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.\n",
      "\n",
      "\n",
      "\n",
      "Systems based on machine-learning algorithms have many advantages over hand-produced rules:\n",
      "\n",
      "\n",
      "\n",
      "The learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not obvious at all where the effort should be directed.\n",
      "\n",
      "Automatic learning procedures can make use of statistical inference algorithms to produce models that are robust to unfamiliar input (e.g. containing words or structures that have not been seen before) and to erroneous input (e.g. with misspelled words or words accidentally omitted). Generally, handling such input gracefully with hand-written rules -- or more generally, creating systems of hand-written rules that make soft decisions -- extremely difficult, error-prone and time-consuming.\n",
      "\n",
      "Systems based on automatically learning the rules can be made more accurate simply by supplying more input data. However, systems based on hand-written rules can only be made more accurate by increasing the complexity of the rules, which is a much more difficult task. In particular, there is a limit to the complexity of systems based on hand-crafted rules, beyond which the systems become more and more unmanageable. However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process.\n",
      "\n",
      "The subfield of NLP devoted to learning approaches is known as Natural Language Learning (NLL) and its conference CoNLL and peak body SIGNLL are sponsored by ACL, recognizing also their links with Computational Linguistics and Language Acquisition. When the aims of computational language learning research is to understand more about human language acquisition, or psycholinguistics, NLL overlaps into the related field of Computational Psycholinguistics.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with codecs.open(nlp_txt_path , 'r' , 'utf-8') as rf:\n",
    "    for l in rf:\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_list = []\n",
    "with codecs.open(nlp_txt_path' , 'r' , 'utf-8') as rf:\n",
    "    for l in rf:\n",
    "        run_list.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages. As such, NLP is related to the area of humani-computer interaction. Many challenges in NLP involve natural language understanding, that is, enabling computers to derive meaning from human or natural language input, and others involve natural language generation.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_list[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 7a8881a] from notebook\n",
      " 1 file changed, 2222 insertions(+), 32 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "git commit -a -m \"from notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 単語の切り出し\n",
    "空白を単語の区切りとみなし，50の出力を入力として受け取り，1行1単語の形式で出力せよ．ただし，文の終端では空行を出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural\n",
      "language\n",
      "processing\n",
      "\n",
      "From\n",
      "Wikipedia,\n",
      "the\n",
      "free\n",
      "encyclopedia\n",
      "\n",
      "Natural\n",
      "language\n",
      "processing\n",
      "(NLP)\n",
      "is\n",
      "a\n",
      "field\n",
      "of\n",
      "computer\n",
      "science,\n",
      "artificial\n",
      "intelligence,\n",
      "and\n",
      "linguistics\n",
      "concerned\n",
      "with\n",
      "the\n",
      "interactions\n",
      "between\n",
      "computers\n",
      "and\n",
      "human\n",
      "(natural)\n",
      "languages.\n",
      "\n",
      "As\n",
      "such,\n",
      "NLP\n",
      "is\n",
      "related\n",
      "to\n",
      "the\n",
      "area\n",
      "of\n",
      "humani-computer\n",
      "interaction.\n",
      "\n",
      "Many\n",
      "challenges\n",
      "in\n",
      "NLP\n",
      "involve\n",
      "natural\n",
      "language\n",
      "understanding,\n",
      "that\n",
      "is,\n",
      "enabling\n",
      "computers\n",
      "to\n",
      "derive\n",
      "meaning\n",
      "from\n",
      "human\n",
      "or\n",
      "natural\n",
      "language\n",
      "input,\n",
      "and\n",
      "others\n",
      "involve\n",
      "natural\n",
      "language\n",
      "generation.\n",
      "\n",
      "History\n",
      "\n",
      "The\n",
      "history\n",
      "of\n",
      "NLP\n",
      "generally\n",
      "starts\n",
      "in\n",
      "the\n",
      "1950s,\n",
      "although\n",
      "work\n",
      "can\n",
      "be\n",
      "found\n",
      "from\n",
      "earlier\n",
      "periods.\n",
      "\n",
      "In\n",
      "1950,\n",
      "Alan\n",
      "Turing\n",
      "published\n",
      "an\n",
      "article\n",
      "titled\n",
      "\"Computing\n",
      "Machinery\n",
      "and\n",
      "Intelligence\"\n",
      "which\n",
      "proposed\n",
      "what\n",
      "is\n",
      "now\n",
      "called\n",
      "the\n",
      "Turing\n",
      "test\n",
      "as\n",
      "a\n",
      "criterion\n",
      "of\n",
      "intelligence.\n",
      "\n",
      "The\n",
      "Georgetown\n",
      "experiment\n",
      "in\n",
      "1954\n",
      "involved\n",
      "fully\n",
      "automatic\n",
      "translation\n",
      "of\n",
      "more\n",
      "than\n",
      "sixty\n",
      "Russian\n",
      "sentences\n",
      "into\n",
      "English.\n",
      "\n",
      "The\n",
      "authors\n",
      "claimed\n",
      "that\n",
      "within\n",
      "three\n",
      "or\n",
      "five\n",
      "years,\n",
      "machine\n",
      "translation\n",
      "would\n",
      "be\n",
      "a\n",
      "solved\n",
      "problem.\n",
      "\n",
      "However,\n",
      "real\n",
      "progress\n",
      "was\n",
      "much\n",
      "slower,\n",
      "and\n",
      "after\n",
      "the\n",
      "ALPAC\n",
      "report\n",
      "in\n",
      "1966,\n",
      "which\n",
      "found\n",
      "that\n",
      "ten\n",
      "year\n",
      "long\n",
      "research\n",
      "had\n",
      "failed\n",
      "to\n",
      "fulfill\n",
      "the\n",
      "expectations,\n",
      "funding\n",
      "for\n",
      "machine\n",
      "translation\n",
      "was\n",
      "dramatically\n",
      "reduced.\n",
      "\n",
      "Little\n",
      "further\n",
      "research\n",
      "in\n",
      "machine\n",
      "translation\n",
      "was\n",
      "conducted\n",
      "until\n",
      "the\n",
      "late\n",
      "1980s,\n",
      "when\n",
      "the\n",
      "first\n",
      "statistical\n",
      "machine\n",
      "translation\n",
      "systems\n",
      "were\n",
      "developed.\n",
      "\n",
      "Some\n",
      "notably\n",
      "successful\n",
      "NLP\n",
      "systems\n",
      "developed\n",
      "in\n",
      "the\n",
      "1960s\n",
      "were\n",
      "SHRDLU,\n",
      "a\n",
      "natural\n",
      "language\n",
      "system\n",
      "working\n",
      "in\n",
      "restricted\n",
      "\"blocks\n",
      "worlds\"\n",
      "with\n",
      "restricted\n",
      "vocabularies,\n",
      "and\n",
      "ELIZA,\n",
      "a\n",
      "simulation\n",
      "of\n",
      "a\n",
      "Rogerian\n",
      "psychotherapist,\n",
      "written\n",
      "by\n",
      "Joseph\n",
      "Weizenbaum\n",
      "between\n",
      "1964\n",
      "to\n",
      "1966.\n",
      "\n",
      "Using\n",
      "almost\n",
      "no\n",
      "information\n",
      "about\n",
      "human\n",
      "thought\n",
      "or\n",
      "emotion,\n",
      "ELIZA\n",
      "sometimes\n",
      "provided\n",
      "a\n",
      "startlingly\n",
      "human-like\n",
      "interaction.\n",
      "\n",
      "When\n",
      "the\n",
      "\"patient\"\n",
      "exceeded\n",
      "the\n",
      "very\n",
      "small\n",
      "knowledge\n",
      "base,\n",
      "ELIZA\n",
      "might\n",
      "provide\n",
      "a\n",
      "generic\n",
      "response,\n",
      "for\n",
      "example,\n",
      "responding\n",
      "to\n",
      "\"My\n",
      "head\n",
      "hurts\"\n",
      "with\n",
      "\"Why\n",
      "do\n",
      "you\n",
      "say\n",
      "your\n",
      "head\n",
      "hurts?\".\n",
      "\n",
      "During\n",
      "the\n",
      "1970s\n",
      "many\n",
      "programmers\n",
      "began\n",
      "to\n",
      "write\n",
      "'conceptual\n",
      "ontologies',\n",
      "which\n",
      "structured\n",
      "real-world\n",
      "information\n",
      "into\n",
      "computer-understandable\n",
      "data.\n",
      "\n",
      "Examples\n",
      "are\n",
      "MARGIE\n",
      "(Schank,\n",
      "1975),\n",
      "SAM\n",
      "(Cullingford,\n",
      "1978),\n",
      "PAM\n",
      "(Wilensky,\n",
      "1978),\n",
      "TaleSpin\n",
      "(Meehan,\n",
      "1976),\n",
      "QUALM\n",
      "(Lehnert,\n",
      "1977),\n",
      "Politics\n",
      "(Carbonell,\n",
      "1979),\n",
      "and\n",
      "Plot\n",
      "Units\n",
      "(Lehnert\n",
      "1981).\n",
      "\n",
      "During\n",
      "this\n",
      "time,\n",
      "many\n",
      "chatterbots\n",
      "were\n",
      "written\n",
      "including\n",
      "PARRY,\n",
      "Racter,\n",
      "and\n",
      "Jabberwacky.\n",
      "\n",
      "Up\n",
      "to\n",
      "the\n",
      "1980s,\n",
      "most\n",
      "NLP\n",
      "systems\n",
      "were\n",
      "based\n",
      "on\n",
      "complex\n",
      "sets\n",
      "of\n",
      "hand-written\n",
      "rules.\n",
      "\n",
      "Starting\n",
      "in\n",
      "the\n",
      "late\n",
      "1980s,\n",
      "however,\n",
      "there\n",
      "was\n",
      "a\n",
      "revolution\n",
      "in\n",
      "NLP\n",
      "with\n",
      "the\n",
      "introduction\n",
      "of\n",
      "machine\n",
      "learning\n",
      "algorithms\n",
      "for\n",
      "language\n",
      "processing.\n",
      "\n",
      "This\n",
      "was\n",
      "due\n",
      "to\n",
      "both\n",
      "the\n",
      "steady\n",
      "increase\n",
      "in\n",
      "computational\n",
      "power\n",
      "resulting\n",
      "from\n",
      "Moore's\n",
      "Law\n",
      "and\n",
      "the\n",
      "gradual\n",
      "lessening\n",
      "of\n",
      "the\n",
      "dominance\n",
      "of\n",
      "Chomskyan\n",
      "theories\n",
      "of\n",
      "linguistics\n",
      "(e.g.\n",
      "transformational\n",
      "grammar),\n",
      "whose\n",
      "theoretical\n",
      "underpinnings\n",
      "discouraged\n",
      "the\n",
      "sort\n",
      "of\n",
      "corpus\n",
      "linguistics\n",
      "that\n",
      "underlies\n",
      "the\n",
      "machine-learning\n",
      "approach\n",
      "to\n",
      "language\n",
      "processing.\n",
      "\n",
      "Some\n",
      "of\n",
      "the\n",
      "earliest-used\n",
      "machine\n",
      "learning\n",
      "algorithms,\n",
      "such\n",
      "as\n",
      "decision\n",
      "trees,\n",
      "produced\n",
      "systems\n",
      "of\n",
      "hard\n",
      "if-then\n",
      "rules\n",
      "similar\n",
      "to\n",
      "existing\n",
      "hand-written\n",
      "rules.\n",
      "\n",
      "However,\n",
      "Part\n",
      "of\n",
      "speech\n",
      "tagging\n",
      "introduced\n",
      "the\n",
      "use\n",
      "of\n",
      "Hidden\n",
      "Markov\n",
      "Models\n",
      "to\n",
      "NLP,\n",
      "and\n",
      "increasingly,\n",
      "research\n",
      "has\n",
      "focused\n",
      "on\n",
      "statistical\n",
      "models,\n",
      "which\n",
      "make\n",
      "soft,\n",
      "probabilistic\n",
      "decisions\n",
      "based\n",
      "on\n",
      "attaching\n",
      "real-valued\n",
      "weights\n",
      "to\n",
      "the\n",
      "features\n",
      "making\n",
      "up\n",
      "the\n",
      "input\n",
      "data.\n",
      "\n",
      "The\n",
      "cache\n",
      "language\n",
      "models\n",
      "upon\n",
      "which\n",
      "many\n",
      "speech\n",
      "recognition\n",
      "systems\n",
      "now\n",
      "rely\n",
      "are\n",
      "examples\n",
      "of\n",
      "such\n",
      "statistical\n",
      "models.\n",
      "\n",
      "Such\n",
      "models\n",
      "are\n",
      "generally\n",
      "more\n",
      "robust\n",
      "when\n",
      "given\n",
      "unfamiliar\n",
      "input,\n",
      "especially\n",
      "input\n",
      "that\n",
      "contains\n",
      "errors\n",
      "(as\n",
      "is\n",
      "very\n",
      "common\n",
      "for\n",
      "real-world\n",
      "data),\n",
      "and\n",
      "produce\n",
      "more\n",
      "reliable\n",
      "results\n",
      "when\n",
      "integrated\n",
      "into\n",
      "a\n",
      "larger\n",
      "system\n",
      "comprising\n",
      "multiple\n",
      "subtasks.\n",
      "\n",
      "Many\n",
      "of\n",
      "the\n",
      "notable\n",
      "early\n",
      "successes\n",
      "occurred\n",
      "in\n",
      "the\n",
      "field\n",
      "of\n",
      "machine\n",
      "translation,\n",
      "due\n",
      "especially\n",
      "to\n",
      "work\n",
      "at\n",
      "IBM\n",
      "Research,\n",
      "where\n",
      "successively\n",
      "more\n",
      "complicated\n",
      "statistical\n",
      "models\n",
      "were\n",
      "developed.\n",
      "\n",
      "These\n",
      "systems\n",
      "were\n",
      "able\n",
      "to\n",
      "take\n",
      "advantage\n",
      "of\n",
      "existing\n",
      "multilingual\n",
      "textual\n",
      "corpora\n",
      "that\n",
      "had\n",
      "been\n",
      "produced\n",
      "by\n",
      "the\n",
      "Parliament\n",
      "of\n",
      "Canada\n",
      "and\n",
      "the\n",
      "European\n",
      "Union\n",
      "as\n",
      "a\n",
      "result\n",
      "of\n",
      "laws\n",
      "calling\n",
      "for\n",
      "the\n",
      "translation\n",
      "of\n",
      "all\n",
      "governmental\n",
      "proceedings\n",
      "into\n",
      "all\n",
      "official\n",
      "languages\n",
      "of\n",
      "the\n",
      "corresponding\n",
      "systems\n",
      "of\n",
      "government.\n",
      "\n",
      "However,\n",
      "most\n",
      "other\n",
      "systems\n",
      "depended\n",
      "on\n",
      "corpora\n",
      "specifically\n",
      "developed\n",
      "for\n",
      "the\n",
      "tasks\n",
      "implemented\n",
      "by\n",
      "these\n",
      "systems,\n",
      "which\n",
      "was\n",
      "(and\n",
      "often\n",
      "continues\n",
      "to\n",
      "be)\n",
      "a\n",
      "major\n",
      "limitation\n",
      "in\n",
      "the\n",
      "success\n",
      "of\n",
      "these\n",
      "systems.\n",
      "\n",
      "As\n",
      "a\n",
      "result,\n",
      "a\n",
      "great\n",
      "deal\n",
      "of\n",
      "research\n",
      "has\n",
      "gone\n",
      "into\n",
      "methods\n",
      "of\n",
      "more\n",
      "effectively\n",
      "learning\n",
      "from\n",
      "limited\n",
      "amounts\n",
      "of\n",
      "data.\n",
      "\n",
      "Recent\n",
      "research\n",
      "has\n",
      "increasingly\n",
      "focused\n",
      "on\n",
      "unsupervised\n",
      "and\n",
      "semi-supervised\n",
      "learning\n",
      "algorithms.\n",
      "\n",
      "Such\n",
      "algorithms\n",
      "are\n",
      "able\n",
      "to\n",
      "learn\n",
      "from\n",
      "data\n",
      "that\n",
      "has\n",
      "not\n",
      "been\n",
      "hand-annotated\n",
      "with\n",
      "the\n",
      "desired\n",
      "answers,\n",
      "or\n",
      "using\n",
      "a\n",
      "combination\n",
      "of\n",
      "annotated\n",
      "and\n",
      "non-annotated\n",
      "data.\n",
      "\n",
      "Generally,\n",
      "this\n",
      "task\n",
      "is\n",
      "much\n",
      "more\n",
      "difficult\n",
      "than\n",
      "supervised\n",
      "learning,\n",
      "and\n",
      "typically\n",
      "produces\n",
      "less\n",
      "accurate\n",
      "results\n",
      "for\n",
      "a\n",
      "given\n",
      "amount\n",
      "of\n",
      "input\n",
      "data.\n",
      "\n",
      "However,\n",
      "there\n",
      "is\n",
      "an\n",
      "enormous\n",
      "amount\n",
      "of\n",
      "non-annotated\n",
      "data\n",
      "available\n",
      "(including,\n",
      "among\n",
      "other\n",
      "things,\n",
      "the\n",
      "entire\n",
      "content\n",
      "of\n",
      "the\n",
      "World\n",
      "Wide\n",
      "Web),\n",
      "which\n",
      "can\n",
      "often\n",
      "make\n",
      "up\n",
      "for\n",
      "the\n",
      "inferior\n",
      "results.\n",
      "\n",
      "NLP\n",
      "using\n",
      "machine\n",
      "learning\n",
      "\n",
      "Modern\n",
      "NLP\n",
      "algorithms\n",
      "are\n",
      "based\n",
      "on\n",
      "machine\n",
      "learning,\n",
      "especially\n",
      "statistical\n",
      "machine\n",
      "learning.\n",
      "\n",
      "The\n",
      "paradigm\n",
      "of\n",
      "machine\n",
      "learning\n",
      "is\n",
      "different\n",
      "from\n",
      "that\n",
      "of\n",
      "most\n",
      "prior\n",
      "attempts\n",
      "at\n",
      "language\n",
      "processing.\n",
      "\n",
      "Prior\n",
      "implementations\n",
      "of\n",
      "language-processing\n",
      "tasks\n",
      "typically\n",
      "involved\n",
      "the\n",
      "direct\n",
      "hand\n",
      "coding\n",
      "of\n",
      "large\n",
      "sets\n",
      "of\n",
      "rules.\n",
      "\n",
      "The\n",
      "machine-learning\n",
      "paradigm\n",
      "calls\n",
      "instead\n",
      "for\n",
      "using\n",
      "general\n",
      "learning\n",
      "algorithms\n",
      "-\n",
      "often,\n",
      "although\n",
      "not\n",
      "always,\n",
      "grounded\n",
      "in\n",
      "statistical\n",
      "inference\n",
      "-\n",
      "to\n",
      "automatically\n",
      "learn\n",
      "such\n",
      "rules\n",
      "through\n",
      "the\n",
      "analysis\n",
      "of\n",
      "large\n",
      "corpora\n",
      "of\n",
      "typical\n",
      "real-world\n",
      "examples.\n",
      "\n",
      "A\n",
      "corpus\n",
      "(plural,\n",
      "\"corpora\")\n",
      "is\n",
      "a\n",
      "set\n",
      "of\n",
      "documents\n",
      "(or\n",
      "sometimes,\n",
      "individual\n",
      "sentences)\n",
      "that\n",
      "have\n",
      "been\n",
      "hand-annotated\n",
      "with\n",
      "the\n",
      "correct\n",
      "values\n",
      "to\n",
      "be\n",
      "learned.\n",
      "\n",
      "Many\n",
      "different\n",
      "classes\n",
      "of\n",
      "machine\n",
      "learning\n",
      "algorithms\n",
      "have\n",
      "been\n",
      "applied\n",
      "to\n",
      "NLP\n",
      "tasks.\n",
      "\n",
      "These\n",
      "algorithms\n",
      "take\n",
      "as\n",
      "input\n",
      "a\n",
      "large\n",
      "set\n",
      "of\n",
      "\"features\"\n",
      "that\n",
      "are\n",
      "generated\n",
      "from\n",
      "the\n",
      "input\n",
      "data.\n",
      "\n",
      "Some\n",
      "of\n",
      "the\n",
      "earliest-used\n",
      "algorithms,\n",
      "such\n",
      "as\n",
      "decision\n",
      "trees,\n",
      "produced\n",
      "systems\n",
      "of\n",
      "hard\n",
      "if-then\n",
      "rules\n",
      "similar\n",
      "to\n",
      "the\n",
      "systems\n",
      "of\n",
      "hand-written\n",
      "rules\n",
      "that\n",
      "were\n",
      "then\n",
      "common.\n",
      "\n",
      "Increasingly,\n",
      "however,\n",
      "research\n",
      "has\n",
      "focused\n",
      "on\n",
      "statistical\n",
      "models,\n",
      "which\n",
      "make\n",
      "soft,\n",
      "probabilistic\n",
      "decisions\n",
      "based\n",
      "on\n",
      "attaching\n",
      "real-valued\n",
      "weights\n",
      "to\n",
      "each\n",
      "input\n",
      "feature.\n",
      "\n",
      "Such\n",
      "models\n",
      "have\n",
      "the\n",
      "advantage\n",
      "that\n",
      "they\n",
      "can\n",
      "express\n",
      "the\n",
      "relative\n",
      "certainty\n",
      "of\n",
      "many\n",
      "different\n",
      "possible\n",
      "answers\n",
      "rather\n",
      "than\n",
      "only\n",
      "one,\n",
      "producing\n",
      "more\n",
      "reliable\n",
      "results\n",
      "when\n",
      "such\n",
      "a\n",
      "model\n",
      "is\n",
      "included\n",
      "as\n",
      "a\n",
      "component\n",
      "of\n",
      "a\n",
      "larger\n",
      "system.\n",
      "\n",
      "Systems\n",
      "based\n",
      "on\n",
      "machine-learning\n",
      "algorithms\n",
      "have\n",
      "many\n",
      "advantages\n",
      "over\n",
      "hand-produced\n",
      "rules:\n",
      "\n",
      "The\n",
      "learning\n",
      "procedures\n",
      "used\n",
      "during\n",
      "machine\n",
      "learning\n",
      "automatically\n",
      "focus\n",
      "on\n",
      "the\n",
      "most\n",
      "common\n",
      "cases,\n",
      "whereas\n",
      "when\n",
      "writing\n",
      "rules\n",
      "by\n",
      "hand\n",
      "it\n",
      "is\n",
      "often\n",
      "not\n",
      "obvious\n",
      "at\n",
      "all\n",
      "where\n",
      "the\n",
      "effort\n",
      "should\n",
      "be\n",
      "directed.\n",
      "\n",
      "Automatic\n",
      "learning\n",
      "procedures\n",
      "can\n",
      "make\n",
      "use\n",
      "of\n",
      "statistical\n",
      "inference\n",
      "algorithms\n",
      "to\n",
      "produce\n",
      "models\n",
      "that\n",
      "are\n",
      "robust\n",
      "to\n",
      "unfamiliar\n",
      "input\n",
      "(e.g.\n",
      "containing\n",
      "words\n",
      "or\n",
      "structures\n",
      "that\n",
      "have\n",
      "not\n",
      "been\n",
      "seen\n",
      "before)\n",
      "and\n",
      "to\n",
      "erroneous\n",
      "input\n",
      "(e.g.\n",
      "with\n",
      "misspelled\n",
      "words\n",
      "or\n",
      "words\n",
      "accidentally\n",
      "omitted).\n",
      "\n",
      "Generally,\n",
      "handling\n",
      "such\n",
      "input\n",
      "gracefully\n",
      "with\n",
      "hand-written\n",
      "rules\n",
      "--\n",
      "or\n",
      "more\n",
      "generally,\n",
      "creating\n",
      "systems\n",
      "of\n",
      "hand-written\n",
      "rules\n",
      "that\n",
      "make\n",
      "soft\n",
      "decisions\n",
      "--\n",
      "extremely\n",
      "difficult,\n",
      "error-prone\n",
      "and\n",
      "time-consuming.\n",
      "\n",
      "Systems\n",
      "based\n",
      "on\n",
      "automatically\n",
      "learning\n",
      "the\n",
      "rules\n",
      "can\n",
      "be\n",
      "made\n",
      "more\n",
      "accurate\n",
      "simply\n",
      "by\n",
      "supplying\n",
      "more\n",
      "input\n",
      "data.\n",
      "\n",
      "However,\n",
      "systems\n",
      "based\n",
      "on\n",
      "hand-written\n",
      "rules\n",
      "can\n",
      "only\n",
      "be\n",
      "made\n",
      "more\n",
      "accurate\n",
      "by\n",
      "increasing\n",
      "the\n",
      "complexity\n",
      "of\n",
      "the\n",
      "rules,\n",
      "which\n",
      "is\n",
      "a\n",
      "much\n",
      "more\n",
      "difficult\n",
      "task.\n",
      "\n",
      "In\n",
      "particular,\n",
      "there\n",
      "is\n",
      "a\n",
      "limit\n",
      "to\n",
      "the\n",
      "complexity\n",
      "of\n",
      "systems\n",
      "based\n",
      "on\n",
      "hand-crafted\n",
      "rules,\n",
      "beyond\n",
      "which\n",
      "the\n",
      "systems\n",
      "become\n",
      "more\n",
      "and\n",
      "more\n",
      "unmanageable.\n",
      "\n",
      "However,\n",
      "creating\n",
      "more\n",
      "data\n",
      "to\n",
      "input\n",
      "to\n",
      "machine-learning\n",
      "systems\n",
      "simply\n",
      "requires\n",
      "a\n",
      "corresponding\n",
      "increase\n",
      "in\n",
      "the\n",
      "number\n",
      "of\n",
      "man-hours\n",
      "worked,\n",
      "generally\n",
      "without\n",
      "significant\n",
      "increases\n",
      "in\n",
      "the\n",
      "complexity\n",
      "of\n",
      "the\n",
      "annotation\n",
      "process.\n",
      "\n",
      "The\n",
      "subfield\n",
      "of\n",
      "NLP\n",
      "devoted\n",
      "to\n",
      "learning\n",
      "approaches\n",
      "is\n",
      "known\n",
      "as\n",
      "Natural\n",
      "Language\n",
      "Learning\n",
      "(NLL)\n",
      "and\n",
      "its\n",
      "conference\n",
      "CoNLL\n",
      "and\n",
      "peak\n",
      "body\n",
      "SIGNLL\n",
      "are\n",
      "sponsored\n",
      "by\n",
      "ACL,\n",
      "recognizing\n",
      "also\n",
      "their\n",
      "links\n",
      "with\n",
      "Computational\n",
      "Linguistics\n",
      "and\n",
      "Language\n",
      "Acquisition.\n",
      "\n",
      "When\n",
      "the\n",
      "aims\n",
      "of\n",
      "computational\n",
      "language\n",
      "learning\n",
      "research\n",
      "is\n",
      "to\n",
      "understand\n",
      "more\n",
      "about\n",
      "human\n",
      "language\n",
      "acquisition,\n",
      "or\n",
      "psycholinguistics,\n",
      "NLL\n",
      "overlaps\n",
      "into\n",
      "the\n",
      "related\n",
      "field\n",
      "of\n",
      "Computational\n",
      "Psycholinguistics.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for l in nlp_lines(nlp_txt_path):\n",
    "    for x in l.split(\" \"):\n",
    "        print(x)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_words(nlp_txt_path):\n",
    "    for l in nlp_lines(nlp_txt_path):\n",
    "        for x in l.split(\" \"):\n",
    "            yield x.rstrip('.,;:?!')\n",
    "        yield ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "別解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_words(nlp_txt_path):\n",
    "    r = re.compile('[\\.|;|:|?|\\!]')\n",
    "    for l in nlp_lines(nlp_txt_path):\n",
    "        for x in l.split(' '):\n",
    "            yield r.sub(\"\" , x)\n",
    "        yield ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural\n",
      "language\n",
      "processing\n",
      "\n",
      "From\n",
      "Wikipedia,\n",
      "the\n",
      "free\n",
      "encyclopedia\n",
      "\n",
      "Natural\n",
      "language\n",
      "processing\n",
      "(NLP)\n",
      "is\n",
      "a\n",
      "field\n",
      "of\n",
      "computer\n",
      "science,\n",
      "artificial\n",
      "intelligence,\n",
      "and\n",
      "linguistics\n",
      "concerned\n",
      "with\n",
      "the\n",
      "interactions\n",
      "between\n",
      "computers\n",
      "and\n",
      "human\n",
      "(natural)\n",
      "languages\n",
      "\n",
      "As\n",
      "such,\n",
      "NLP\n",
      "is\n",
      "related\n",
      "to\n",
      "the\n",
      "area\n",
      "of\n",
      "humani-computer\n",
      "interaction\n",
      "\n",
      "Many\n",
      "challenges\n",
      "in\n",
      "NLP\n",
      "involve\n",
      "natural\n",
      "language\n",
      "understanding,\n",
      "that\n",
      "is,\n",
      "enabling\n",
      "computers\n",
      "to\n",
      "derive\n",
      "meaning\n",
      "from\n",
      "human\n",
      "or\n",
      "natural\n",
      "language\n",
      "input,\n",
      "and\n",
      "others\n",
      "involve\n",
      "natural\n",
      "language\n",
      "generation\n",
      "\n",
      "History\n",
      "\n",
      "The\n",
      "history\n",
      "of\n",
      "NLP\n",
      "generally\n",
      "starts\n",
      "in\n",
      "the\n",
      "1950s,\n",
      "although\n",
      "work\n",
      "can\n",
      "be\n",
      "found\n",
      "from\n",
      "earlier\n",
      "periods\n",
      "\n",
      "In\n",
      "1950,\n",
      "Alan\n",
      "Turing\n",
      "published\n",
      "an\n",
      "article\n",
      "titled\n",
      "\"Computing\n",
      "Machinery\n",
      "and\n",
      "Intelligence\"\n",
      "which\n",
      "proposed\n",
      "what\n",
      "is\n",
      "now\n",
      "called\n",
      "the\n",
      "Turing\n",
      "test\n",
      "as\n",
      "a\n",
      "criterion\n",
      "of\n",
      "intelligence\n",
      "\n",
      "The\n",
      "Georgetown\n",
      "experiment\n",
      "in\n",
      "1954\n",
      "involved\n",
      "fully\n",
      "automatic\n",
      "translation\n",
      "of\n",
      "more\n",
      "than\n",
      "sixty\n",
      "Russian\n",
      "sentences\n",
      "into\n",
      "English\n",
      "\n",
      "The\n",
      "authors\n",
      "claimed\n",
      "that\n",
      "within\n",
      "three\n",
      "or\n",
      "five\n",
      "years,\n",
      "machine\n",
      "translation\n",
      "would\n",
      "be\n",
      "a\n",
      "solved\n",
      "problem\n",
      "\n",
      "However,\n",
      "real\n",
      "progress\n",
      "was\n",
      "much\n",
      "slower,\n",
      "and\n",
      "after\n",
      "the\n",
      "ALPAC\n",
      "report\n",
      "in\n",
      "1966,\n",
      "which\n",
      "found\n",
      "that\n",
      "ten\n",
      "year\n",
      "long\n",
      "research\n",
      "had\n",
      "failed\n",
      "to\n",
      "fulfill\n",
      "the\n",
      "expectations,\n",
      "funding\n",
      "for\n",
      "machine\n",
      "translation\n",
      "was\n",
      "dramatically\n",
      "reduced\n",
      "\n",
      "Little\n",
      "further\n",
      "research\n",
      "in\n",
      "machine\n",
      "translation\n",
      "was\n",
      "conducted\n",
      "until\n",
      "the\n",
      "late\n",
      "1980s,\n",
      "when\n",
      "the\n",
      "first\n",
      "statistical\n",
      "machine\n",
      "translation\n",
      "systems\n",
      "were\n",
      "developed\n",
      "\n",
      "Some\n",
      "notably\n",
      "successful\n",
      "NLP\n",
      "systems\n",
      "developed\n",
      "in\n",
      "the\n",
      "1960s\n",
      "were\n",
      "SHRDLU,\n",
      "a\n",
      "natural\n",
      "language\n",
      "system\n",
      "working\n",
      "in\n",
      "restricted\n",
      "\"blocks\n",
      "worlds\"\n",
      "with\n",
      "restricted\n",
      "vocabularies,\n",
      "and\n",
      "ELIZA,\n",
      "a\n",
      "simulation\n",
      "of\n",
      "a\n",
      "Rogerian\n",
      "psychotherapist,\n",
      "written\n",
      "by\n",
      "Joseph\n",
      "Weizenbaum\n",
      "between\n",
      "1964\n",
      "to\n",
      "1966\n",
      "\n",
      "Using\n",
      "almost\n",
      "no\n",
      "information\n",
      "about\n",
      "human\n",
      "thought\n",
      "or\n",
      "emotion,\n",
      "ELIZA\n",
      "sometimes\n",
      "provided\n",
      "a\n",
      "startlingly\n",
      "human-like\n",
      "interaction\n",
      "\n",
      "When\n",
      "the\n",
      "\"patient\"\n",
      "exceeded\n",
      "the\n",
      "very\n",
      "small\n",
      "knowledge\n",
      "base,\n",
      "ELIZA\n",
      "might\n",
      "provide\n",
      "a\n",
      "generic\n",
      "response,\n",
      "for\n",
      "example,\n",
      "responding\n",
      "to\n",
      "\"My\n",
      "head\n",
      "hurts\"\n",
      "with\n",
      "\"Why\n",
      "do\n",
      "you\n",
      "say\n",
      "your\n",
      "head\n",
      "hurts\"\n",
      "\n",
      "During\n",
      "the\n",
      "1970s\n",
      "many\n",
      "programmers\n",
      "began\n",
      "to\n",
      "write\n",
      "'conceptual\n",
      "ontologies',\n",
      "which\n",
      "structured\n",
      "real-world\n",
      "information\n",
      "into\n",
      "computer-understandable\n",
      "data\n",
      "\n",
      "Examples\n",
      "are\n",
      "MARGIE\n",
      "(Schank,\n",
      "1975),\n",
      "SAM\n",
      "(Cullingford,\n",
      "1978),\n",
      "PAM\n",
      "(Wilensky,\n",
      "1978),\n",
      "TaleSpin\n",
      "(Meehan,\n",
      "1976),\n",
      "QUALM\n",
      "(Lehnert,\n",
      "1977),\n",
      "Politics\n",
      "(Carbonell,\n",
      "1979),\n",
      "and\n",
      "Plot\n",
      "Units\n",
      "(Lehnert\n",
      "1981)\n",
      "\n",
      "During\n",
      "this\n",
      "time,\n",
      "many\n",
      "chatterbots\n",
      "were\n",
      "written\n",
      "including\n",
      "PARRY,\n",
      "Racter,\n",
      "and\n",
      "Jabberwacky\n",
      "\n",
      "Up\n",
      "to\n",
      "the\n",
      "1980s,\n",
      "most\n",
      "NLP\n",
      "systems\n",
      "were\n",
      "based\n",
      "on\n",
      "complex\n",
      "sets\n",
      "of\n",
      "hand-written\n",
      "rules\n",
      "\n",
      "Starting\n",
      "in\n",
      "the\n",
      "late\n",
      "1980s,\n",
      "however,\n",
      "there\n",
      "was\n",
      "a\n",
      "revolution\n",
      "in\n",
      "NLP\n",
      "with\n",
      "the\n",
      "introduction\n",
      "of\n",
      "machine\n",
      "learning\n",
      "algorithms\n",
      "for\n",
      "language\n",
      "processing\n",
      "\n",
      "This\n",
      "was\n",
      "due\n",
      "to\n",
      "both\n",
      "the\n",
      "steady\n",
      "increase\n",
      "in\n",
      "computational\n",
      "power\n",
      "resulting\n",
      "from\n",
      "Moore's\n",
      "Law\n",
      "and\n",
      "the\n",
      "gradual\n",
      "lessening\n",
      "of\n",
      "the\n",
      "dominance\n",
      "of\n",
      "Chomskyan\n",
      "theories\n",
      "of\n",
      "linguistics\n",
      "(eg\n",
      "transformational\n",
      "grammar),\n",
      "whose\n",
      "theoretical\n",
      "underpinnings\n",
      "discouraged\n",
      "the\n",
      "sort\n",
      "of\n",
      "corpus\n",
      "linguistics\n",
      "that\n",
      "underlies\n",
      "the\n",
      "machine-learning\n",
      "approach\n",
      "to\n",
      "language\n",
      "processing\n",
      "\n",
      "Some\n",
      "of\n",
      "the\n",
      "earliest-used\n",
      "machine\n",
      "learning\n",
      "algorithms,\n",
      "such\n",
      "as\n",
      "decision\n",
      "trees,\n",
      "produced\n",
      "systems\n",
      "of\n",
      "hard\n",
      "if-then\n",
      "rules\n",
      "similar\n",
      "to\n",
      "existing\n",
      "hand-written\n",
      "rules\n",
      "\n",
      "However,\n",
      "Part\n",
      "of\n",
      "speech\n",
      "tagging\n",
      "introduced\n",
      "the\n",
      "use\n",
      "of\n",
      "Hidden\n",
      "Markov\n",
      "Models\n",
      "to\n",
      "NLP,\n",
      "and\n",
      "increasingly,\n",
      "research\n",
      "has\n",
      "focused\n",
      "on\n",
      "statistical\n",
      "models,\n",
      "which\n",
      "make\n",
      "soft,\n",
      "probabilistic\n",
      "decisions\n",
      "based\n",
      "on\n",
      "attaching\n",
      "real-valued\n",
      "weights\n",
      "to\n",
      "the\n",
      "features\n",
      "making\n",
      "up\n",
      "the\n",
      "input\n",
      "data\n",
      "\n",
      "The\n",
      "cache\n",
      "language\n",
      "models\n",
      "upon\n",
      "which\n",
      "many\n",
      "speech\n",
      "recognition\n",
      "systems\n",
      "now\n",
      "rely\n",
      "are\n",
      "examples\n",
      "of\n",
      "such\n",
      "statistical\n",
      "models\n",
      "\n",
      "Such\n",
      "models\n",
      "are\n",
      "generally\n",
      "more\n",
      "robust\n",
      "when\n",
      "given\n",
      "unfamiliar\n",
      "input,\n",
      "especially\n",
      "input\n",
      "that\n",
      "contains\n",
      "errors\n",
      "(as\n",
      "is\n",
      "very\n",
      "common\n",
      "for\n",
      "real-world\n",
      "data),\n",
      "and\n",
      "produce\n",
      "more\n",
      "reliable\n",
      "results\n",
      "when\n",
      "integrated\n",
      "into\n",
      "a\n",
      "larger\n",
      "system\n",
      "comprising\n",
      "multiple\n",
      "subtasks\n",
      "\n",
      "Many\n",
      "of\n",
      "the\n",
      "notable\n",
      "early\n",
      "successes\n",
      "occurred\n",
      "in\n",
      "the\n",
      "field\n",
      "of\n",
      "machine\n",
      "translation,\n",
      "due\n",
      "especially\n",
      "to\n",
      "work\n",
      "at\n",
      "IBM\n",
      "Research,\n",
      "where\n",
      "successively\n",
      "more\n",
      "complicated\n",
      "statistical\n",
      "models\n",
      "were\n",
      "developed\n",
      "\n",
      "These\n",
      "systems\n",
      "were\n",
      "able\n",
      "to\n",
      "take\n",
      "advantage\n",
      "of\n",
      "existing\n",
      "multilingual\n",
      "textual\n",
      "corpora\n",
      "that\n",
      "had\n",
      "been\n",
      "produced\n",
      "by\n",
      "the\n",
      "Parliament\n",
      "of\n",
      "Canada\n",
      "and\n",
      "the\n",
      "European\n",
      "Union\n",
      "as\n",
      "a\n",
      "result\n",
      "of\n",
      "laws\n",
      "calling\n",
      "for\n",
      "the\n",
      "translation\n",
      "of\n",
      "all\n",
      "governmental\n",
      "proceedings\n",
      "into\n",
      "all\n",
      "official\n",
      "languages\n",
      "of\n",
      "the\n",
      "corresponding\n",
      "systems\n",
      "of\n",
      "government\n",
      "\n",
      "However,\n",
      "most\n",
      "other\n",
      "systems\n",
      "depended\n",
      "on\n",
      "corpora\n",
      "specifically\n",
      "developed\n",
      "for\n",
      "the\n",
      "tasks\n",
      "implemented\n",
      "by\n",
      "these\n",
      "systems,\n",
      "which\n",
      "was\n",
      "(and\n",
      "often\n",
      "continues\n",
      "to\n",
      "be)\n",
      "a\n",
      "major\n",
      "limitation\n",
      "in\n",
      "the\n",
      "success\n",
      "of\n",
      "these\n",
      "systems\n",
      "\n",
      "As\n",
      "a\n",
      "result,\n",
      "a\n",
      "great\n",
      "deal\n",
      "of\n",
      "research\n",
      "has\n",
      "gone\n",
      "into\n",
      "methods\n",
      "of\n",
      "more\n",
      "effectively\n",
      "learning\n",
      "from\n",
      "limited\n",
      "amounts\n",
      "of\n",
      "data\n",
      "\n",
      "Recent\n",
      "research\n",
      "has\n",
      "increasingly\n",
      "focused\n",
      "on\n",
      "unsupervised\n",
      "and\n",
      "semi-supervised\n",
      "learning\n",
      "algorithms\n",
      "\n",
      "Such\n",
      "algorithms\n",
      "are\n",
      "able\n",
      "to\n",
      "learn\n",
      "from\n",
      "data\n",
      "that\n",
      "has\n",
      "not\n",
      "been\n",
      "hand-annotated\n",
      "with\n",
      "the\n",
      "desired\n",
      "answers,\n",
      "or\n",
      "using\n",
      "a\n",
      "combination\n",
      "of\n",
      "annotated\n",
      "and\n",
      "non-annotated\n",
      "data\n",
      "\n",
      "Generally,\n",
      "this\n",
      "task\n",
      "is\n",
      "much\n",
      "more\n",
      "difficult\n",
      "than\n",
      "supervised\n",
      "learning,\n",
      "and\n",
      "typically\n",
      "produces\n",
      "less\n",
      "accurate\n",
      "results\n",
      "for\n",
      "a\n",
      "given\n",
      "amount\n",
      "of\n",
      "input\n",
      "data\n",
      "\n",
      "However,\n",
      "there\n",
      "is\n",
      "an\n",
      "enormous\n",
      "amount\n",
      "of\n",
      "non-annotated\n",
      "data\n",
      "available\n",
      "(including,\n",
      "among\n",
      "other\n",
      "things,\n",
      "the\n",
      "entire\n",
      "content\n",
      "of\n",
      "the\n",
      "World\n",
      "Wide\n",
      "Web),\n",
      "which\n",
      "can\n",
      "often\n",
      "make\n",
      "up\n",
      "for\n",
      "the\n",
      "inferior\n",
      "results\n",
      "\n",
      "NLP\n",
      "using\n",
      "machine\n",
      "learning\n",
      "\n",
      "Modern\n",
      "NLP\n",
      "algorithms\n",
      "are\n",
      "based\n",
      "on\n",
      "machine\n",
      "learning,\n",
      "especially\n",
      "statistical\n",
      "machine\n",
      "learning\n",
      "\n",
      "The\n",
      "paradigm\n",
      "of\n",
      "machine\n",
      "learning\n",
      "is\n",
      "different\n",
      "from\n",
      "that\n",
      "of\n",
      "most\n",
      "prior\n",
      "attempts\n",
      "at\n",
      "language\n",
      "processing\n",
      "\n",
      "Prior\n",
      "implementations\n",
      "of\n",
      "language-processing\n",
      "tasks\n",
      "typically\n",
      "involved\n",
      "the\n",
      "direct\n",
      "hand\n",
      "coding\n",
      "of\n",
      "large\n",
      "sets\n",
      "of\n",
      "rules\n",
      "\n",
      "The\n",
      "machine-learning\n",
      "paradigm\n",
      "calls\n",
      "instead\n",
      "for\n",
      "using\n",
      "general\n",
      "learning\n",
      "algorithms\n",
      "-\n",
      "often,\n",
      "although\n",
      "not\n",
      "always,\n",
      "grounded\n",
      "in\n",
      "statistical\n",
      "inference\n",
      "-\n",
      "to\n",
      "automatically\n",
      "learn\n",
      "such\n",
      "rules\n",
      "through\n",
      "the\n",
      "analysis\n",
      "of\n",
      "large\n",
      "corpora\n",
      "of\n",
      "typical\n",
      "real-world\n",
      "examples\n",
      "\n",
      "A\n",
      "corpus\n",
      "(plural,\n",
      "\"corpora\")\n",
      "is\n",
      "a\n",
      "set\n",
      "of\n",
      "documents\n",
      "(or\n",
      "sometimes,\n",
      "individual\n",
      "sentences)\n",
      "that\n",
      "have\n",
      "been\n",
      "hand-annotated\n",
      "with\n",
      "the\n",
      "correct\n",
      "values\n",
      "to\n",
      "be\n",
      "learned\n",
      "\n",
      "Many\n",
      "different\n",
      "classes\n",
      "of\n",
      "machine\n",
      "learning\n",
      "algorithms\n",
      "have\n",
      "been\n",
      "applied\n",
      "to\n",
      "NLP\n",
      "tasks\n",
      "\n",
      "These\n",
      "algorithms\n",
      "take\n",
      "as\n",
      "input\n",
      "a\n",
      "large\n",
      "set\n",
      "of\n",
      "\"features\"\n",
      "that\n",
      "are\n",
      "generated\n",
      "from\n",
      "the\n",
      "input\n",
      "data\n",
      "\n",
      "Some\n",
      "of\n",
      "the\n",
      "earliest-used\n",
      "algorithms,\n",
      "such\n",
      "as\n",
      "decision\n",
      "trees,\n",
      "produced\n",
      "systems\n",
      "of\n",
      "hard\n",
      "if-then\n",
      "rules\n",
      "similar\n",
      "to\n",
      "the\n",
      "systems\n",
      "of\n",
      "hand-written\n",
      "rules\n",
      "that\n",
      "were\n",
      "then\n",
      "common\n",
      "\n",
      "Increasingly,\n",
      "however,\n",
      "research\n",
      "has\n",
      "focused\n",
      "on\n",
      "statistical\n",
      "models,\n",
      "which\n",
      "make\n",
      "soft,\n",
      "probabilistic\n",
      "decisions\n",
      "based\n",
      "on\n",
      "attaching\n",
      "real-valued\n",
      "weights\n",
      "to\n",
      "each\n",
      "input\n",
      "feature\n",
      "\n",
      "Such\n",
      "models\n",
      "have\n",
      "the\n",
      "advantage\n",
      "that\n",
      "they\n",
      "can\n",
      "express\n",
      "the\n",
      "relative\n",
      "certainty\n",
      "of\n",
      "many\n",
      "different\n",
      "possible\n",
      "answers\n",
      "rather\n",
      "than\n",
      "only\n",
      "one,\n",
      "producing\n",
      "more\n",
      "reliable\n",
      "results\n",
      "when\n",
      "such\n",
      "a\n",
      "model\n",
      "is\n",
      "included\n",
      "as\n",
      "a\n",
      "component\n",
      "of\n",
      "a\n",
      "larger\n",
      "system\n",
      "\n",
      "Systems\n",
      "based\n",
      "on\n",
      "machine-learning\n",
      "algorithms\n",
      "have\n",
      "many\n",
      "advantages\n",
      "over\n",
      "hand-produced\n",
      "rules\n",
      "\n",
      "The\n",
      "learning\n",
      "procedures\n",
      "used\n",
      "during\n",
      "machine\n",
      "learning\n",
      "automatically\n",
      "focus\n",
      "on\n",
      "the\n",
      "most\n",
      "common\n",
      "cases,\n",
      "whereas\n",
      "when\n",
      "writing\n",
      "rules\n",
      "by\n",
      "hand\n",
      "it\n",
      "is\n",
      "often\n",
      "not\n",
      "obvious\n",
      "at\n",
      "all\n",
      "where\n",
      "the\n",
      "effort\n",
      "should\n",
      "be\n",
      "directed\n",
      "\n",
      "Automatic\n",
      "learning\n",
      "procedures\n",
      "can\n",
      "make\n",
      "use\n",
      "of\n",
      "statistical\n",
      "inference\n",
      "algorithms\n",
      "to\n",
      "produce\n",
      "models\n",
      "that\n",
      "are\n",
      "robust\n",
      "to\n",
      "unfamiliar\n",
      "input\n",
      "(eg\n",
      "containing\n",
      "words\n",
      "or\n",
      "structures\n",
      "that\n",
      "have\n",
      "not\n",
      "been\n",
      "seen\n",
      "before)\n",
      "and\n",
      "to\n",
      "erroneous\n",
      "input\n",
      "(eg\n",
      "with\n",
      "misspelled\n",
      "words\n",
      "or\n",
      "words\n",
      "accidentally\n",
      "omitted)\n",
      "\n",
      "Generally,\n",
      "handling\n",
      "such\n",
      "input\n",
      "gracefully\n",
      "with\n",
      "hand-written\n",
      "rules\n",
      "--\n",
      "or\n",
      "more\n",
      "generally,\n",
      "creating\n",
      "systems\n",
      "of\n",
      "hand-written\n",
      "rules\n",
      "that\n",
      "make\n",
      "soft\n",
      "decisions\n",
      "--\n",
      "extremely\n",
      "difficult,\n",
      "error-prone\n",
      "and\n",
      "time-consuming\n",
      "\n",
      "Systems\n",
      "based\n",
      "on\n",
      "automatically\n",
      "learning\n",
      "the\n",
      "rules\n",
      "can\n",
      "be\n",
      "made\n",
      "more\n",
      "accurate\n",
      "simply\n",
      "by\n",
      "supplying\n",
      "more\n",
      "input\n",
      "data\n",
      "\n",
      "However,\n",
      "systems\n",
      "based\n",
      "on\n",
      "hand-written\n",
      "rules\n",
      "can\n",
      "only\n",
      "be\n",
      "made\n",
      "more\n",
      "accurate\n",
      "by\n",
      "increasing\n",
      "the\n",
      "complexity\n",
      "of\n",
      "the\n",
      "rules,\n",
      "which\n",
      "is\n",
      "a\n",
      "much\n",
      "more\n",
      "difficult\n",
      "task\n",
      "\n",
      "In\n",
      "particular,\n",
      "there\n",
      "is\n",
      "a\n",
      "limit\n",
      "to\n",
      "the\n",
      "complexity\n",
      "of\n",
      "systems\n",
      "based\n",
      "on\n",
      "hand-crafted\n",
      "rules,\n",
      "beyond\n",
      "which\n",
      "the\n",
      "systems\n",
      "become\n",
      "more\n",
      "and\n",
      "more\n",
      "unmanageable\n",
      "\n",
      "However,\n",
      "creating\n",
      "more\n",
      "data\n",
      "to\n",
      "input\n",
      "to\n",
      "machine-learning\n",
      "systems\n",
      "simply\n",
      "requires\n",
      "a\n",
      "corresponding\n",
      "increase\n",
      "in\n",
      "the\n",
      "number\n",
      "of\n",
      "man-hours\n",
      "worked,\n",
      "generally\n",
      "without\n",
      "significant\n",
      "increases\n",
      "in\n",
      "the\n",
      "complexity\n",
      "of\n",
      "the\n",
      "annotation\n",
      "process\n",
      "\n",
      "The\n",
      "subfield\n",
      "of\n",
      "NLP\n",
      "devoted\n",
      "to\n",
      "learning\n",
      "approaches\n",
      "is\n",
      "known\n",
      "as\n",
      "Natural\n",
      "Language\n",
      "Learning\n",
      "(NLL)\n",
      "and\n",
      "its\n",
      "conference\n",
      "CoNLL\n",
      "and\n",
      "peak\n",
      "body\n",
      "SIGNLL\n",
      "are\n",
      "sponsored\n",
      "by\n",
      "ACL,\n",
      "recognizing\n",
      "also\n",
      "their\n",
      "links\n",
      "with\n",
      "Computational\n",
      "Linguistics\n",
      "and\n",
      "Language\n",
      "Acquisition\n",
      "\n",
      "When\n",
      "the\n",
      "aims\n",
      "of\n",
      "computational\n",
      "language\n",
      "learning\n",
      "research\n",
      "is\n",
      "to\n",
      "understand\n",
      "more\n",
      "about\n",
      "human\n",
      "language\n",
      "acquisition,\n",
      "or\n",
      "psycholinguistics,\n",
      "NLL\n",
      "overlaps\n",
      "into\n",
      "the\n",
      "related\n",
      "field\n",
      "of\n",
      "Computational\n",
      "Psycholinguistics\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for w in nlp_words(nlp_txt_path):\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ステミング\n",
    "51の出力を入力として受け取り，Porterのステミングアルゴリズムを適用し，単語と語幹をタブ区切り形式で出力せよ． Pythonでは，Porterのステミングアルゴリズムの実装としてstemmingモジュールを利用するとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowballstemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = snowballstemmer.stemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural\tNatur\n",
      "language\tlanguag\n",
      "processing\tprocess\n",
      "\t\n",
      "From\tFrom\n",
      "Wikipedia,\tWikipedia,\n",
      "the\tthe\n",
      "free\tfree\n",
      "encyclopedia\tencyclopedia\n",
      "\t\n",
      "Natural\tNatur\n",
      "language\tlanguag\n",
      "processing\tprocess\n",
      "(NLP)\t(NLP)\n",
      "is\tis\n",
      "a\ta\n",
      "field\tfield\n",
      "of\tof\n",
      "computer\tcomput\n",
      "science,\tscience,\n",
      "artificial\tartifici\n",
      "intelligence,\tintelligence,\n",
      "and\tand\n",
      "linguistics\tlinguist\n",
      "concerned\tconcern\n",
      "with\twith\n",
      "the\tthe\n",
      "interactions\tinteract\n",
      "between\tbetween\n",
      "computers\tcomput\n",
      "and\tand\n",
      "human\thuman\n",
      "(natural)\t(natural)\n",
      "languages\tlanguag\n",
      "\t\n",
      "As\tAs\n",
      "such,\tsuch,\n",
      "NLP\tNLP\n",
      "is\tis\n",
      "related\trelat\n",
      "to\tto\n",
      "the\tthe\n",
      "area\tarea\n",
      "of\tof\n",
      "humani-computer\thumani-comput\n",
      "interaction\tinteract\n",
      "\t\n",
      "Many\tMani\n",
      "challenges\tchalleng\n",
      "in\tin\n",
      "NLP\tNLP\n",
      "involve\tinvolv\n",
      "natural\tnatur\n",
      "language\tlanguag\n",
      "understanding,\tunderstanding,\n",
      "that\tthat\n",
      "is,\tis,\n",
      "enabling\tenabl\n",
      "computers\tcomput\n",
      "to\tto\n",
      "derive\tderiv\n",
      "meaning\tmean\n",
      "from\tfrom\n",
      "human\thuman\n",
      "or\tor\n",
      "natural\tnatur\n",
      "language\tlanguag\n",
      "input,\tinput,\n",
      "and\tand\n",
      "others\tother\n",
      "involve\tinvolv\n",
      "natural\tnatur\n",
      "language\tlanguag\n",
      "generation\tgenerat\n",
      "\t\n",
      "History\tHistori\n",
      "\t\n",
      "The\tThe\n",
      "history\thistori\n",
      "of\tof\n",
      "NLP\tNLP\n",
      "generally\tgeneral\n",
      "starts\tstart\n",
      "in\tin\n",
      "the\tthe\n",
      "1950s,\t1950s,\n",
      "although\talthough\n",
      "work\twork\n",
      "can\tcan\n",
      "be\tbe\n",
      "found\tfound\n",
      "from\tfrom\n",
      "earlier\tearlier\n",
      "periods\tperiod\n",
      "\t\n",
      "In\tIn\n",
      "1950,\t1950,\n",
      "Alan\tAlan\n",
      "Turing\tTure\n",
      "published\tpublish\n",
      "an\tan\n",
      "article\tarticl\n",
      "titled\ttitl\n",
      "\"Computing\t\"Comput\n",
      "Machinery\tMachineri\n",
      "and\tand\n",
      "Intelligence\"\tIntelligence\"\n",
      "which\twhich\n",
      "proposed\tpropos\n",
      "what\twhat\n",
      "is\tis\n",
      "now\tnow\n",
      "called\tcall\n",
      "the\tthe\n",
      "Turing\tTure\n",
      "test\ttest\n",
      "as\tas\n",
      "a\ta\n",
      "criterion\tcriterion\n",
      "of\tof\n",
      "intelligence\tintellig\n",
      "\t\n",
      "The\tThe\n",
      "Georgetown\tGeorgetown\n",
      "experiment\texperi\n",
      "in\tin\n",
      "1954\t1954\n",
      "involved\tinvolv\n",
      "fully\tfulli\n",
      "automatic\tautomat\n",
      "translation\ttranslat\n",
      "of\tof\n",
      "more\tmore\n",
      "than\tthan\n",
      "sixty\tsixti\n",
      "Russian\tRussian\n",
      "sentences\tsentenc\n",
      "into\tinto\n",
      "English\tEnglish\n",
      "\t\n",
      "The\tThe\n",
      "authors\tauthor\n",
      "claimed\tclaim\n",
      "that\tthat\n",
      "within\twithin\n",
      "three\tthree\n",
      "or\tor\n",
      "five\tfive\n",
      "years,\tyears,\n",
      "machine\tmachin\n",
      "translation\ttranslat\n",
      "would\twould\n",
      "be\tbe\n",
      "a\ta\n",
      "solved\tsolv\n",
      "problem\tproblem\n",
      "\t\n",
      "However,\tHowever,\n",
      "real\treal\n",
      "progress\tprogress\n",
      "was\twas\n",
      "much\tmuch\n",
      "slower,\tslower,\n",
      "and\tand\n",
      "after\tafter\n",
      "the\tthe\n",
      "ALPAC\tALPAC\n",
      "report\treport\n",
      "in\tin\n",
      "1966,\t1966,\n",
      "which\twhich\n",
      "found\tfound\n",
      "that\tthat\n",
      "ten\tten\n",
      "year\tyear\n",
      "long\tlong\n",
      "research\tresearch\n",
      "had\thad\n",
      "failed\tfail\n",
      "to\tto\n",
      "fulfill\tfulfil\n",
      "the\tthe\n",
      "expectations,\texpectations,\n",
      "funding\tfund\n",
      "for\tfor\n",
      "machine\tmachin\n",
      "translation\ttranslat\n",
      "was\twas\n",
      "dramatically\tdramat\n",
      "reduced\treduc\n",
      "\t\n",
      "Little\tLittl\n",
      "further\tfurther\n",
      "research\tresearch\n",
      "in\tin\n",
      "machine\tmachin\n",
      "translation\ttranslat\n",
      "was\twas\n",
      "conducted\tconduct\n",
      "until\tuntil\n",
      "the\tthe\n",
      "late\tlate\n",
      "1980s,\t1980s,\n",
      "when\twhen\n",
      "the\tthe\n",
      "first\tfirst\n",
      "statistical\tstatist\n",
      "machine\tmachin\n",
      "translation\ttranslat\n",
      "systems\tsystem\n",
      "were\twere\n",
      "developed\tdevelop\n",
      "\t\n",
      "Some\tSome\n",
      "notably\tnotabl\n",
      "successful\tsuccess\n",
      "NLP\tNLP\n",
      "systems\tsystem\n",
      "developed\tdevelop\n",
      "in\tin\n",
      "the\tthe\n",
      "1960s\t1960s\n",
      "were\twere\n",
      "SHRDLU,\tSHRDLU,\n",
      "a\ta\n",
      "natural\tnatur\n",
      "language\tlanguag\n",
      "system\tsystem\n",
      "working\twork\n",
      "in\tin\n",
      "restricted\trestrict\n",
      "\"blocks\t\"block\n",
      "worlds\"\tworlds\"\n",
      "with\twith\n",
      "restricted\trestrict\n",
      "vocabularies,\tvocabularies,\n",
      "and\tand\n",
      "ELIZA,\tELIZA,\n",
      "a\ta\n",
      "simulation\tsimul\n",
      "of\tof\n",
      "a\ta\n",
      "Rogerian\tRogerian\n",
      "psychotherapist,\tpsychotherapist,\n",
      "written\twritten\n",
      "by\tby\n",
      "Joseph\tJoseph\n",
      "Weizenbaum\tWeizenbaum\n",
      "between\tbetween\n",
      "1964\t1964\n",
      "to\tto\n",
      "1966\t1966\n",
      "\t\n",
      "Using\tUsing\n",
      "almost\talmost\n",
      "no\tno\n",
      "information\tinform\n",
      "about\tabout\n",
      "human\thuman\n",
      "thought\tthought\n",
      "or\tor\n",
      "emotion,\temotion,\n",
      "ELIZA\tELIZA\n",
      "sometimes\tsometim\n",
      "provided\tprovid\n",
      "a\ta\n",
      "startlingly\tstartl\n",
      "human-like\thuman-lik\n",
      "interaction\tinteract\n",
      "\t\n",
      "When\tWhen\n",
      "the\tthe\n",
      "\"patient\"\t\"patient\"\n",
      "exceeded\texceed\n",
      "the\tthe\n",
      "very\tveri\n",
      "small\tsmall\n",
      "knowledge\tknowledg\n",
      "base,\tbase,\n",
      "ELIZA\tELIZA\n",
      "might\tmight\n",
      "provide\tprovid\n",
      "a\ta\n",
      "generic\tgeneric\n",
      "response,\tresponse,\n",
      "for\tfor\n",
      "example,\texample,\n",
      "responding\trespond\n",
      "to\tto\n",
      "\"My\t\"Mi\n",
      "head\thead\n",
      "hurts\"\thurts\"\n",
      "with\twith\n",
      "\"Why\t\"Whi\n",
      "do\tdo\n",
      "you\tyou\n",
      "say\tsay\n",
      "your\tyour\n",
      "head\thead\n",
      "hurts\"\thurts\"\n",
      "\t\n",
      "During\tDure\n",
      "the\tthe\n",
      "1970s\t1970s\n",
      "many\tmani\n",
      "programmers\tprogramm\n",
      "began\tbegan\n",
      "to\tto\n",
      "write\twrite\n",
      "'conceptual\tconceptu\n",
      "ontologies',\tontologies',\n",
      "which\twhich\n",
      "structured\tstructur\n",
      "real-world\treal-world\n",
      "information\tinform\n",
      "into\tinto\n",
      "computer-understandable\tcomputer-understand\n",
      "data\tdata\n",
      "\t\n",
      "Examples\tExampl\n",
      "are\tare\n",
      "MARGIE\tMARGIE\n",
      "(Schank,\t(Schank,\n",
      "1975),\t1975),\n",
      "SAM\tSAM\n",
      "(Cullingford,\t(Cullingford,\n",
      "1978),\t1978),\n",
      "PAM\tPAM\n",
      "(Wilensky,\t(Wilensky,\n",
      "1978),\t1978),\n",
      "TaleSpin\tTaleSpin\n",
      "(Meehan,\t(Meehan,\n",
      "1976),\t1976),\n",
      "QUALM\tQUALM\n",
      "(Lehnert,\t(Lehnert,\n",
      "1977),\t1977),\n",
      "Politics\tPolit\n",
      "(Carbonell,\t(Carbonell,\n",
      "1979),\t1979),\n",
      "and\tand\n",
      "Plot\tPlot\n",
      "Units\tUnit\n",
      "(Lehnert\t(Lehnert\n",
      "1981)\t1981)\n",
      "\t\n",
      "During\tDure\n",
      "this\tthis\n",
      "time,\ttime,\n",
      "many\tmani\n",
      "chatterbots\tchatterbot\n",
      "were\twere\n",
      "written\twritten\n",
      "including\tinclud\n",
      "PARRY,\tPARRY,\n",
      "Racter,\tRacter,\n",
      "and\tand\n",
      "Jabberwacky\tJabberwacki\n",
      "\t\n",
      "Up\tUp\n",
      "to\tto\n",
      "the\tthe\n",
      "1980s,\t1980s,\n",
      "most\tmost\n",
      "NLP\tNLP\n",
      "systems\tsystem\n",
      "were\twere\n",
      "based\tbase\n",
      "on\ton\n",
      "complex\tcomplex\n",
      "sets\tset\n",
      "of\tof\n",
      "hand-written\thand-written\n",
      "rules\trule\n",
      "\t\n",
      "Starting\tStart\n",
      "in\tin\n",
      "the\tthe\n",
      "late\tlate\n",
      "1980s,\t1980s,\n",
      "however,\thowever,\n",
      "there\tthere\n",
      "was\twas\n",
      "a\ta\n",
      "revolution\trevolut\n",
      "in\tin\n",
      "NLP\tNLP\n",
      "with\twith\n",
      "the\tthe\n",
      "introduction\tintroduct\n",
      "of\tof\n",
      "machine\tmachin\n",
      "learning\tlearn\n",
      "algorithms\talgorithm\n",
      "for\tfor\n",
      "language\tlanguag\n",
      "processing\tprocess\n",
      "\t\n",
      "This\tThis\n",
      "was\twas\n",
      "due\tdue\n",
      "to\tto\n",
      "both\tboth\n",
      "the\tthe\n",
      "steady\tsteadi\n",
      "increase\tincreas\n",
      "in\tin\n",
      "computational\tcomput\n",
      "power\tpower\n",
      "resulting\tresult\n",
      "from\tfrom\n",
      "Moore's\tMoor\n",
      "Law\tLaw\n",
      "and\tand\n",
      "the\tthe\n",
      "gradual\tgradual\n",
      "lessening\tlessen\n",
      "of\tof\n",
      "the\tthe\n",
      "dominance\tdomin\n",
      "of\tof\n",
      "Chomskyan\tChomskyan\n",
      "theories\ttheori\n",
      "of\tof\n",
      "linguistics\tlinguist\n",
      "(eg\t(eg\n",
      "transformational\ttransform\n",
      "grammar),\tgrammar),\n",
      "whose\twhose\n",
      "theoretical\ttheoret\n",
      "underpinnings\tunderpin\n",
      "discouraged\tdiscourag\n",
      "the\tthe\n",
      "sort\tsort\n",
      "of\tof\n",
      "corpus\tcorpus\n",
      "linguistics\tlinguist\n",
      "that\tthat\n",
      "underlies\tunder\n",
      "the\tthe\n",
      "machine-learning\tmachine-learn\n",
      "approach\tapproach\n",
      "to\tto\n",
      "language\tlanguag\n",
      "processing\tprocess\n",
      "\t\n",
      "Some\tSome\n",
      "of\tof\n",
      "the\tthe\n",
      "earliest-used\tearliest-us\n",
      "machine\tmachin\n",
      "learning\tlearn\n",
      "algorithms,\talgorithms,\n",
      "such\tsuch\n",
      "as\tas\n",
      "decision\tdecis\n",
      "trees,\ttrees,\n",
      "produced\tproduc\n",
      "systems\tsystem\n",
      "of\tof\n",
      "hard\thard\n",
      "if-then\tif-then\n",
      "rules\trule\n",
      "similar\tsimilar\n",
      "to\tto\n",
      "existing\texist\n",
      "hand-written\thand-written\n",
      "rules\trule\n",
      "\t\n",
      "However,\tHowever,\n",
      "Part\tPart\n",
      "of\tof\n",
      "speech\tspeech\n",
      "tagging\ttag\n",
      "introduced\tintroduc\n",
      "the\tthe\n",
      "use\tuse\n",
      "of\tof\n",
      "Hidden\tHidden\n",
      "Markov\tMarkov\n",
      "Models\tModel\n",
      "to\tto\n",
      "NLP,\tNLP,\n",
      "and\tand\n",
      "increasingly,\tincreasingly,\n",
      "research\tresearch\n",
      "has\thas\n",
      "focused\tfocus\n",
      "on\ton\n",
      "statistical\tstatist\n",
      "models,\tmodels,\n",
      "which\twhich\n",
      "make\tmake\n",
      "soft,\tsoft,\n",
      "probabilistic\tprobabilist\n",
      "decisions\tdecis\n",
      "based\tbase\n",
      "on\ton\n",
      "attaching\tattach\n",
      "real-valued\treal-valu\n",
      "weights\tweight\n",
      "to\tto\n",
      "the\tthe\n",
      "features\tfeatur\n",
      "making\tmake\n",
      "up\tup\n",
      "the\tthe\n",
      "input\tinput\n",
      "data\tdata\n",
      "\t\n",
      "The\tThe\n",
      "cache\tcach\n",
      "language\tlanguag\n",
      "models\tmodel\n",
      "upon\tupon\n",
      "which\twhich\n",
      "many\tmani\n",
      "speech\tspeech\n",
      "recognition\trecognit\n",
      "systems\tsystem\n",
      "now\tnow\n",
      "rely\treli\n",
      "are\tare\n",
      "examples\texampl\n",
      "of\tof\n",
      "such\tsuch\n",
      "statistical\tstatist\n",
      "models\tmodel\n",
      "\t\n",
      "Such\tSuch\n",
      "models\tmodel\n",
      "are\tare\n",
      "generally\tgeneral\n",
      "more\tmore\n",
      "robust\trobust\n",
      "when\twhen\n",
      "given\tgiven\n",
      "unfamiliar\tunfamiliar\n",
      "input,\tinput,\n",
      "especially\tespeci\n",
      "input\tinput\n",
      "that\tthat\n",
      "contains\tcontain\n",
      "errors\terror\n",
      "(as\t(as\n",
      "is\tis\n",
      "very\tveri\n",
      "common\tcommon\n",
      "for\tfor\n",
      "real-world\treal-world\n",
      "data),\tdata),\n",
      "and\tand\n",
      "produce\tproduc\n",
      "more\tmore\n",
      "reliable\treliabl\n",
      "results\tresult\n",
      "when\twhen\n",
      "integrated\tintegr\n",
      "into\tinto\n",
      "a\ta\n",
      "larger\tlarger\n",
      "system\tsystem\n",
      "comprising\tcompris\n",
      "multiple\tmultipl\n",
      "subtasks\tsubtask\n",
      "\t\n",
      "Many\tMani\n",
      "of\tof\n",
      "the\tthe\n",
      "notable\tnotabl\n",
      "early\tearli\n",
      "successes\tsuccess\n",
      "occurred\toccur\n",
      "in\tin\n",
      "the\tthe\n",
      "field\tfield\n",
      "of\tof\n",
      "machine\tmachin\n",
      "translation,\ttranslation,\n",
      "due\tdue\n",
      "especially\tespeci\n",
      "to\tto\n",
      "work\twork\n",
      "at\tat\n",
      "IBM\tIBM\n",
      "Research,\tResearch,\n",
      "where\twhere\n",
      "successively\tsuccess\n",
      "more\tmore\n",
      "complicated\tcomplic\n",
      "statistical\tstatist\n",
      "models\tmodel\n",
      "were\twere\n",
      "developed\tdevelop\n",
      "\t\n",
      "These\tThese\n",
      "systems\tsystem\n",
      "were\twere\n",
      "able\tabl\n",
      "to\tto\n",
      "take\ttake\n",
      "advantage\tadvantag\n",
      "of\tof\n",
      "existing\texist\n",
      "multilingual\tmultilingu\n",
      "textual\ttextual\n",
      "corpora\tcorpora\n",
      "that\tthat\n",
      "had\thad\n",
      "been\tbeen\n",
      "produced\tproduc\n",
      "by\tby\n",
      "the\tthe\n",
      "Parliament\tParliament\n",
      "of\tof\n",
      "Canada\tCanada\n",
      "and\tand\n",
      "the\tthe\n",
      "European\tEuropean\n",
      "Union\tUnion\n",
      "as\tas\n",
      "a\ta\n",
      "result\tresult\n",
      "of\tof\n",
      "laws\tlaw\n",
      "calling\tcall\n",
      "for\tfor\n",
      "the\tthe\n",
      "translation\ttranslat\n",
      "of\tof\n",
      "all\tall\n",
      "governmental\tgovernment\n",
      "proceedings\tproceed\n",
      "into\tinto\n",
      "all\tall\n",
      "official\toffici\n",
      "languages\tlanguag\n",
      "of\tof\n",
      "the\tthe\n",
      "corresponding\tcorrespond\n",
      "systems\tsystem\n",
      "of\tof\n",
      "government\tgovern\n",
      "\t\n",
      "However,\tHowever,\n",
      "most\tmost\n",
      "other\tother\n",
      "systems\tsystem\n",
      "depended\tdepend\n",
      "on\ton\n",
      "corpora\tcorpora\n",
      "specifically\tspecif\n",
      "developed\tdevelop\n",
      "for\tfor\n",
      "the\tthe\n",
      "tasks\ttask\n",
      "implemented\timplement\n",
      "by\tby\n",
      "these\tthese\n",
      "systems,\tsystems,\n",
      "which\twhich\n",
      "was\twas\n",
      "(and\t(and\n",
      "often\toften\n",
      "continues\tcontinu\n",
      "to\tto\n",
      "be)\tbe)\n",
      "a\ta\n",
      "major\tmajor\n",
      "limitation\tlimit\n",
      "in\tin\n",
      "the\tthe\n",
      "success\tsuccess\n",
      "of\tof\n",
      "these\tthese\n",
      "systems\tsystem\n",
      "\t\n",
      "As\tAs\n",
      "a\ta\n",
      "result,\tresult,\n",
      "a\ta\n",
      "great\tgreat\n",
      "deal\tdeal\n",
      "of\tof\n",
      "research\tresearch\n",
      "has\thas\n",
      "gone\tgone\n",
      "into\tinto\n",
      "methods\tmethod\n",
      "of\tof\n",
      "more\tmore\n",
      "effectively\teffect\n",
      "learning\tlearn\n",
      "from\tfrom\n",
      "limited\tlimit\n",
      "amounts\tamount\n",
      "of\tof\n",
      "data\tdata\n",
      "\t\n",
      "Recent\tRecent\n",
      "research\tresearch\n",
      "has\thas\n",
      "increasingly\tincreas\n",
      "focused\tfocus\n",
      "on\ton\n",
      "unsupervised\tunsupervis\n",
      "and\tand\n",
      "semi-supervised\tsemi-supervis\n",
      "learning\tlearn\n",
      "algorithms\talgorithm\n",
      "\t\n",
      "Such\tSuch\n",
      "algorithms\talgorithm\n",
      "are\tare\n",
      "able\tabl\n",
      "to\tto\n",
      "learn\tlearn\n",
      "from\tfrom\n",
      "data\tdata\n",
      "that\tthat\n",
      "has\thas\n",
      "not\tnot\n",
      "been\tbeen\n",
      "hand-annotated\thand-annot\n",
      "with\twith\n",
      "the\tthe\n",
      "desired\tdesir\n",
      "answers,\tanswers,\n",
      "or\tor\n",
      "using\tuse\n",
      "a\ta\n",
      "combination\tcombin\n",
      "of\tof\n",
      "annotated\tannot\n",
      "and\tand\n",
      "non-annotated\tnon-annot\n",
      "data\tdata\n",
      "\t\n",
      "Generally,\tGenerally,\n",
      "this\tthis\n",
      "task\ttask\n",
      "is\tis\n",
      "much\tmuch\n",
      "more\tmore\n",
      "difficult\tdifficult\n",
      "than\tthan\n",
      "supervised\tsupervis\n",
      "learning,\tlearning,\n",
      "and\tand\n",
      "typically\ttypic\n",
      "produces\tproduc\n",
      "less\tless\n",
      "accurate\taccur\n",
      "results\tresult\n",
      "for\tfor\n",
      "a\ta\n",
      "given\tgiven\n",
      "amount\tamount\n",
      "of\tof\n",
      "input\tinput\n",
      "data\tdata\n",
      "\t\n",
      "However,\tHowever,\n",
      "there\tthere\n",
      "is\tis\n",
      "an\tan\n",
      "enormous\tenorm\n",
      "amount\tamount\n",
      "of\tof\n",
      "non-annotated\tnon-annot\n",
      "data\tdata\n",
      "available\tavail\n",
      "(including,\t(including,\n",
      "among\tamong\n",
      "other\tother\n",
      "things,\tthings,\n",
      "the\tthe\n",
      "entire\tentir\n",
      "content\tcontent\n",
      "of\tof\n",
      "the\tthe\n",
      "World\tWorld\n",
      "Wide\tWide\n",
      "Web),\tWeb),\n",
      "which\twhich\n",
      "can\tcan\n",
      "often\toften\n",
      "make\tmake\n",
      "up\tup\n",
      "for\tfor\n",
      "the\tthe\n",
      "inferior\tinferior\n",
      "results\tresult\n",
      "\t\n",
      "NLP\tNLP\n",
      "using\tuse\n",
      "machine\tmachin\n",
      "learning\tlearn\n",
      "\t\n",
      "Modern\tModern\n",
      "NLP\tNLP\n",
      "algorithms\talgorithm\n",
      "are\tare\n",
      "based\tbase\n",
      "on\ton\n",
      "machine\tmachin\n",
      "learning,\tlearning,\n",
      "especially\tespeci\n",
      "statistical\tstatist\n",
      "machine\tmachin\n",
      "learning\tlearn\n",
      "\t\n",
      "The\tThe\n",
      "paradigm\tparadigm\n",
      "of\tof\n",
      "machine\tmachin\n",
      "learning\tlearn\n",
      "is\tis\n",
      "different\tdiffer\n",
      "from\tfrom\n",
      "that\tthat\n",
      "of\tof\n",
      "most\tmost\n",
      "prior\tprior\n",
      "attempts\tattempt\n",
      "at\tat\n",
      "language\tlanguag\n",
      "processing\tprocess\n",
      "\t\n",
      "Prior\tPrior\n",
      "implementations\timplement\n",
      "of\tof\n",
      "language-processing\tlanguage-process\n",
      "tasks\ttask\n",
      "typically\ttypic\n",
      "involved\tinvolv\n",
      "the\tthe\n",
      "direct\tdirect\n",
      "hand\thand\n",
      "coding\tcode\n",
      "of\tof\n",
      "large\tlarg\n",
      "sets\tset\n",
      "of\tof\n",
      "rules\trule\n",
      "\t\n",
      "The\tThe\n",
      "machine-learning\tmachine-learn\n",
      "paradigm\tparadigm\n",
      "calls\tcall\n",
      "instead\tinstead\n",
      "for\tfor\n",
      "using\tuse\n",
      "general\tgeneral\n",
      "learning\tlearn\n",
      "algorithms\talgorithm\n",
      "-\t-\n",
      "often,\toften,\n",
      "although\talthough\n",
      "not\tnot\n",
      "always,\talways,\n",
      "grounded\tground\n",
      "in\tin\n",
      "statistical\tstatist\n",
      "inference\tinfer\n",
      "-\t-\n",
      "to\tto\n",
      "automatically\tautomat\n",
      "learn\tlearn\n",
      "such\tsuch\n",
      "rules\trule\n",
      "through\tthrough\n",
      "the\tthe\n",
      "analysis\tanalysi\n",
      "of\tof\n",
      "large\tlarg\n",
      "corpora\tcorpora\n",
      "of\tof\n",
      "typical\ttypic\n",
      "real-world\treal-world\n",
      "examples\texampl\n",
      "\t\n",
      "A\tA\n",
      "corpus\tcorpus\n",
      "(plural,\t(plural,\n",
      "\"corpora\")\t\"corpora\")\n",
      "is\tis\n",
      "a\ta\n",
      "set\tset\n",
      "of\tof\n",
      "documents\tdocument\n",
      "(or\t(or\n",
      "sometimes,\tsometimes,\n",
      "individual\tindividu\n",
      "sentences)\tsentences)\n",
      "that\tthat\n",
      "have\thave\n",
      "been\tbeen\n",
      "hand-annotated\thand-annot\n",
      "with\twith\n",
      "the\tthe\n",
      "correct\tcorrect\n",
      "values\tvalu\n",
      "to\tto\n",
      "be\tbe\n",
      "learned\tlearn\n",
      "\t\n",
      "Many\tMani\n",
      "different\tdiffer\n",
      "classes\tclass\n",
      "of\tof\n",
      "machine\tmachin\n",
      "learning\tlearn\n",
      "algorithms\talgorithm\n",
      "have\thave\n",
      "been\tbeen\n",
      "applied\tappli\n",
      "to\tto\n",
      "NLP\tNLP\n",
      "tasks\ttask\n",
      "\t\n",
      "These\tThese\n",
      "algorithms\talgorithm\n",
      "take\ttake\n",
      "as\tas\n",
      "input\tinput\n",
      "a\ta\n",
      "large\tlarg\n",
      "set\tset\n",
      "of\tof\n",
      "\"features\"\t\"features\"\n",
      "that\tthat\n",
      "are\tare\n",
      "generated\tgenerat\n",
      "from\tfrom\n",
      "the\tthe\n",
      "input\tinput\n",
      "data\tdata\n",
      "\t\n",
      "Some\tSome\n",
      "of\tof\n",
      "the\tthe\n",
      "earliest-used\tearliest-us\n",
      "algorithms,\talgorithms,\n",
      "such\tsuch\n",
      "as\tas\n",
      "decision\tdecis\n",
      "trees,\ttrees,\n",
      "produced\tproduc\n",
      "systems\tsystem\n",
      "of\tof\n",
      "hard\thard\n",
      "if-then\tif-then\n",
      "rules\trule\n",
      "similar\tsimilar\n",
      "to\tto\n",
      "the\tthe\n",
      "systems\tsystem\n",
      "of\tof\n",
      "hand-written\thand-written\n",
      "rules\trule\n",
      "that\tthat\n",
      "were\twere\n",
      "then\tthen\n",
      "common\tcommon\n",
      "\t\n",
      "Increasingly,\tIncreasingly,\n",
      "however,\thowever,\n",
      "research\tresearch\n",
      "has\thas\n",
      "focused\tfocus\n",
      "on\ton\n",
      "statistical\tstatist\n",
      "models,\tmodels,\n",
      "which\twhich\n",
      "make\tmake\n",
      "soft,\tsoft,\n",
      "probabilistic\tprobabilist\n",
      "decisions\tdecis\n",
      "based\tbase\n",
      "on\ton\n",
      "attaching\tattach\n",
      "real-valued\treal-valu\n",
      "weights\tweight\n",
      "to\tto\n",
      "each\teach\n",
      "input\tinput\n",
      "feature\tfeatur\n",
      "\t\n",
      "Such\tSuch\n",
      "models\tmodel\n",
      "have\thave\n",
      "the\tthe\n",
      "advantage\tadvantag\n",
      "that\tthat\n",
      "they\tthey\n",
      "can\tcan\n",
      "express\texpress\n",
      "the\tthe\n",
      "relative\trelat\n",
      "certainty\tcertainti\n",
      "of\tof\n",
      "many\tmani\n",
      "different\tdiffer\n",
      "possible\tpossibl\n",
      "answers\tanswer\n",
      "rather\trather\n",
      "than\tthan\n",
      "only\tonli\n",
      "one,\tone,\n",
      "producing\tproduc\n",
      "more\tmore\n",
      "reliable\treliabl\n",
      "results\tresult\n",
      "when\twhen\n",
      "such\tsuch\n",
      "a\ta\n",
      "model\tmodel\n",
      "is\tis\n",
      "included\tinclud\n",
      "as\tas\n",
      "a\ta\n",
      "component\tcompon\n",
      "of\tof\n",
      "a\ta\n",
      "larger\tlarger\n",
      "system\tsystem\n",
      "\t\n",
      "Systems\tSystem\n",
      "based\tbase\n",
      "on\ton\n",
      "machine-learning\tmachine-learn\n",
      "algorithms\talgorithm\n",
      "have\thave\n",
      "many\tmani\n",
      "advantages\tadvantag\n",
      "over\tover\n",
      "hand-produced\thand-produc\n",
      "rules\trule\n",
      "\t\n",
      "The\tThe\n",
      "learning\tlearn\n",
      "procedures\tprocedur\n",
      "used\tuse\n",
      "during\tdure\n",
      "machine\tmachin\n",
      "learning\tlearn\n",
      "automatically\tautomat\n",
      "focus\tfocus\n",
      "on\ton\n",
      "the\tthe\n",
      "most\tmost\n",
      "common\tcommon\n",
      "cases,\tcases,\n",
      "whereas\twherea\n",
      "when\twhen\n",
      "writing\twrite\n",
      "rules\trule\n",
      "by\tby\n",
      "hand\thand\n",
      "it\tit\n",
      "is\tis\n",
      "often\toften\n",
      "not\tnot\n",
      "obvious\tobvious\n",
      "at\tat\n",
      "all\tall\n",
      "where\twhere\n",
      "the\tthe\n",
      "effort\teffort\n",
      "should\tshould\n",
      "be\tbe\n",
      "directed\tdirect\n",
      "\t\n",
      "Automatic\tAutomat\n",
      "learning\tlearn\n",
      "procedures\tprocedur\n",
      "can\tcan\n",
      "make\tmake\n",
      "use\tuse\n",
      "of\tof\n",
      "statistical\tstatist\n",
      "inference\tinfer\n",
      "algorithms\talgorithm\n",
      "to\tto\n",
      "produce\tproduc\n",
      "models\tmodel\n",
      "that\tthat\n",
      "are\tare\n",
      "robust\trobust\n",
      "to\tto\n",
      "unfamiliar\tunfamiliar\n",
      "input\tinput\n",
      "(eg\t(eg\n",
      "containing\tcontain\n",
      "words\tword\n",
      "or\tor\n",
      "structures\tstructur\n",
      "that\tthat\n",
      "have\thave\n",
      "not\tnot\n",
      "been\tbeen\n",
      "seen\tseen\n",
      "before)\tbefore)\n",
      "and\tand\n",
      "to\tto\n",
      "erroneous\terron\n",
      "input\tinput\n",
      "(eg\t(eg\n",
      "with\twith\n",
      "misspelled\tmisspel\n",
      "words\tword\n",
      "or\tor\n",
      "words\tword\n",
      "accidentally\taccident\n",
      "omitted)\tomitted)\n",
      "\t\n",
      "Generally,\tGenerally,\n",
      "handling\thandl\n",
      "such\tsuch\n",
      "input\tinput\n",
      "gracefully\tgrace\n",
      "with\twith\n",
      "hand-written\thand-written\n",
      "rules\trule\n",
      "--\t--\n",
      "or\tor\n",
      "more\tmore\n",
      "generally,\tgenerally,\n",
      "creating\tcreat\n",
      "systems\tsystem\n",
      "of\tof\n",
      "hand-written\thand-written\n",
      "rules\trule\n",
      "that\tthat\n",
      "make\tmake\n",
      "soft\tsoft\n",
      "decisions\tdecis\n",
      "--\t--\n",
      "extremely\textrem\n",
      "difficult,\tdifficult,\n",
      "error-prone\terror-pron\n",
      "and\tand\n",
      "time-consuming\ttime-consum\n",
      "\t\n",
      "Systems\tSystem\n",
      "based\tbase\n",
      "on\ton\n",
      "automatically\tautomat\n",
      "learning\tlearn\n",
      "the\tthe\n",
      "rules\trule\n",
      "can\tcan\n",
      "be\tbe\n",
      "made\tmade\n",
      "more\tmore\n",
      "accurate\taccur\n",
      "simply\tsimpli\n",
      "by\tby\n",
      "supplying\tsuppli\n",
      "more\tmore\n",
      "input\tinput\n",
      "data\tdata\n",
      "\t\n",
      "However,\tHowever,\n",
      "systems\tsystem\n",
      "based\tbase\n",
      "on\ton\n",
      "hand-written\thand-written\n",
      "rules\trule\n",
      "can\tcan\n",
      "only\tonli\n",
      "be\tbe\n",
      "made\tmade\n",
      "more\tmore\n",
      "accurate\taccur\n",
      "by\tby\n",
      "increasing\tincreas\n",
      "the\tthe\n",
      "complexity\tcomplex\n",
      "of\tof\n",
      "the\tthe\n",
      "rules,\trules,\n",
      "which\twhich\n",
      "is\tis\n",
      "a\ta\n",
      "much\tmuch\n",
      "more\tmore\n",
      "difficult\tdifficult\n",
      "task\ttask\n",
      "\t\n",
      "In\tIn\n",
      "particular,\tparticular,\n",
      "there\tthere\n",
      "is\tis\n",
      "a\ta\n",
      "limit\tlimit\n",
      "to\tto\n",
      "the\tthe\n",
      "complexity\tcomplex\n",
      "of\tof\n",
      "systems\tsystem\n",
      "based\tbase\n",
      "on\ton\n",
      "hand-crafted\thand-craft\n",
      "rules,\trules,\n",
      "beyond\tbeyond\n",
      "which\twhich\n",
      "the\tthe\n",
      "systems\tsystem\n",
      "become\tbecom\n",
      "more\tmore\n",
      "and\tand\n",
      "more\tmore\n",
      "unmanageable\tunmanag\n",
      "\t\n",
      "However,\tHowever,\n",
      "creating\tcreat\n",
      "more\tmore\n",
      "data\tdata\n",
      "to\tto\n",
      "input\tinput\n",
      "to\tto\n",
      "machine-learning\tmachine-learn\n",
      "systems\tsystem\n",
      "simply\tsimpli\n",
      "requires\trequir\n",
      "a\ta\n",
      "corresponding\tcorrespond\n",
      "increase\tincreas\n",
      "in\tin\n",
      "the\tthe\n",
      "number\tnumber\n",
      "of\tof\n",
      "man-hours\tman-hour\n",
      "worked,\tworked,\n",
      "generally\tgeneral\n",
      "without\twithout\n",
      "significant\tsignific\n",
      "increases\tincreas\n",
      "in\tin\n",
      "the\tthe\n",
      "complexity\tcomplex\n",
      "of\tof\n",
      "the\tthe\n",
      "annotation\tannot\n",
      "process\tprocess\n",
      "\t\n",
      "The\tThe\n",
      "subfield\tsubfield\n",
      "of\tof\n",
      "NLP\tNLP\n",
      "devoted\tdevot\n",
      "to\tto\n",
      "learning\tlearn\n",
      "approaches\tapproach\n",
      "is\tis\n",
      "known\tknown\n",
      "as\tas\n",
      "Natural\tNatur\n",
      "Language\tLanguag\n",
      "Learning\tLearn\n",
      "(NLL)\t(NLL)\n",
      "and\tand\n",
      "its\tit\n",
      "conference\tconfer\n",
      "CoNLL\tCoNLL\n",
      "and\tand\n",
      "peak\tpeak\n",
      "body\tbodi\n",
      "SIGNLL\tSIGNLL\n",
      "are\tare\n",
      "sponsored\tsponsor\n",
      "by\tby\n",
      "ACL,\tACL,\n",
      "recognizing\trecogn\n",
      "also\talso\n",
      "their\ttheir\n",
      "links\tlink\n",
      "with\twith\n",
      "Computational\tComput\n",
      "Linguistics\tLinguist\n",
      "and\tand\n",
      "Language\tLanguag\n",
      "Acquisition\tAcquisit\n",
      "\t\n",
      "When\tWhen\n",
      "the\tthe\n",
      "aims\taim\n",
      "of\tof\n",
      "computational\tcomput\n",
      "language\tlanguag\n",
      "learning\tlearn\n",
      "research\tresearch\n",
      "is\tis\n",
      "to\tto\n",
      "understand\tunderstand\n",
      "more\tmore\n",
      "about\tabout\n",
      "human\thuman\n",
      "language\tlanguag\n",
      "acquisition,\tacquisition,\n",
      "or\tor\n",
      "psycholinguistics,\tpsycholinguistics,\n",
      "NLL\tNLL\n",
      "overlaps\toverlap\n",
      "into\tinto\n",
      "the\tthe\n",
      "related\trelat\n",
      "field\tfield\n",
      "of\tof\n",
      "Computational\tComput\n",
      "Psycholinguistics\tPsycholinguist\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "for w in nlp_words(nlp_txt_path):\n",
    "    print('{}\\t{}'.format(w , stemmer.stemWord(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'snow'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stemWord(\"snowing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.rstrip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = re.compile('[\\.|;|:|?|\\!]')\n",
    "r.sub(\"\" , \"what?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 2bb9ff8] from notebook\n",
      " 1 file changed, 156 insertions(+), 10 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "git commit -a -m \"from notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "\n",
    "Stanford Core NLPを用い，入力テキストの解析結果をXML形式で得よ．\n",
    "\n",
    "また，このXMLファイルを読み込み，入力テキストを1行1単語の形式で出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "homedir = os.getenv('HOME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bash上でcoreNLPのディレクトリに移動して以下のコマンドはうまくいった。もっとcoreNLPのディレクトリ以下に吐いてしまう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":~/Tools/stanford-corenlp-full-2018-02-27$ java -cp \"*\" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,tokenize,pos,lemma,ner -file ~/PycharmProjects/keras_sandbox/nlp100data/nlp.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下も動きそうで動かない（？）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is terminated.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ~/Tools/stanford-corenlp-full-2018-02-27/\n",
    "java -cp \"*\" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file /home/toshinao/PycharmProjects/keras_sandbox/nlp100data/nlp.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "props_xml={'annotators': 'tokenize,lemma,pos','pipelineLanguage':'en','outputFormat':'xml'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnlp_list = []\n",
    "for l in nlp_lines(nlp_txt_path):\n",
    "    cnlp_list.append(corenlp.annotate(l , properties = props_xml))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "ET.parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### stringからdirectにparseする方法\n",
    "\n",
    "https://docs.python.org/3.4/library/xml.etree.elementtree.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = ET.fromstring(cnlp_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document\n"
     ]
    }
   ],
   "source": [
    "for c in root:\n",
    "    print(c.tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "props={'annotators': 'tokenize,lemma,pos','pipelineLanguage':'en','outputFormat':'text'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence #1 (3 tokens):\n",
      "Natural language processing\n",
      "\n",
      "Tokens:\n",
      "[Text=Natural CharacterOffsetBegin=0 CharacterOffsetEnd=7 PartOfSpeech=JJ Lemma=natural]\n",
      "[Text=language CharacterOffsetBegin=8 CharacterOffsetEnd=16 PartOfSpeech=NN Lemma=language]\n",
      "[Text=processing CharacterOffsetBegin=17 CharacterOffsetEnd=27 PartOfSpeech=NN Lemma=processing]\n",
      "\n",
      "\n",
      "Sentence #1 (6 tokens):\n",
      "From Wikipedia, the free encyclopedia\n",
      "\n",
      "Tokens:\n",
      "[Text=From CharacterOffsetBegin=0 CharacterOffsetEnd=4 PartOfSpeech=IN Lemma=from]\n",
      "[Text=Wikipedia CharacterOffsetBegin=5 CharacterOffsetEnd=14 PartOfSpeech=NNP Lemma=Wikipedia]\n",
      "[Text=, CharacterOffsetBegin=14 CharacterOffsetEnd=15 PartOfSpeech=, Lemma=,]\n",
      "[Text=the CharacterOffsetBegin=16 CharacterOffsetEnd=19 PartOfSpeech=DT Lemma=the]\n",
      "[Text=free CharacterOffsetBegin=20 CharacterOffsetEnd=24 PartOfSpeech=JJ Lemma=free]\n",
      "[Text=encyclopedia CharacterOffsetBegin=25 CharacterOffsetEnd=37 PartOfSpeech=NN Lemma=encyclopedia]\n",
      "\n",
      "\n",
      "Sentence #1 (31 tokens):\n",
      "Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages.\n",
      "\n",
      "Tokens:\n",
      "[Text=Natural CharacterOffsetBegin=0 CharacterOffsetEnd=7 PartOfSpeech=JJ Lemma=natural]\n",
      "[Text=language CharacterOffsetBegin=8 CharacterOffsetEnd=16 PartOfSpeech=NN Lemma=language]\n",
      "[Text=processing CharacterOffsetBegin=17 CharacterOffsetEnd=27 PartOfSpeech=NN Lemma=processing]\n",
      "[Text=-LRB- CharacterOffsetBegin=28 CharacterOffsetEnd=29 PartOfSpeech=-LRB- Lemma=-lrb-]\n",
      "[Text=NLP CharacterOffsetBegin=29 CharacterOffsetEnd=32 PartOfSpeech=NN Lemma=nlp]\n",
      "[Text=-RRB- CharacterOffsetBegin=32 CharacterOffsetEnd=33 PartOfSpeech=-RRB- Lemma=-rrb-]\n",
      "[Text=is CharacterOffsetBegin=34 CharacterOffsetEnd=36 PartOfSpeech=VBZ Lemma=be]\n",
      "[Text=a CharacterOffsetBegin=37 CharacterOffsetEnd=38 PartOfSpeech=DT Lemma=a]\n",
      "[Text=field CharacterOffsetBegin=39 CharacterOffsetEnd=44 PartOfSpeech=NN Lemma=field]\n",
      "[Text=of CharacterOffsetBegin=45 CharacterOffsetEnd=47 PartOfSpeech=IN Lemma=of]\n",
      "[Text=computer CharacterOffsetBegin=48 CharacterOffsetEnd=56 PartOfSpeech=NN Lemma=computer]\n",
      "[Text=science CharacterOffsetBegin=57 CharacterOffsetEnd=64 PartOfSpeech=NN Lemma=science]\n",
      "[Text=, CharacterOffsetBegin=64 CharacterOffsetEnd=65 PartOfSpeech=, Lemma=,]\n",
      "[Text=artificial CharacterOffsetBegin=66 CharacterOffsetEnd=76 PartOfSpeech=JJ Lemma=artificial]\n",
      "[Text=intelligence CharacterOffsetBegin=77 CharacterOffsetEnd=89 PartOfSpeech=NN Lemma=intelligence]\n",
      "[Text=, CharacterOffsetBegin=89 CharacterOffsetEnd=90 PartOfSpeech=, Lemma=,]\n",
      "[Text=and CharacterOffsetBegin=91 CharacterOffsetEnd=94 PartOfSpeech=CC Lemma=and]\n",
      "[Text=linguistics CharacterOffsetBegin=95 CharacterOffsetEnd=106 PartOfSpeech=NNS Lemma=linguistics]\n",
      "[Text=concerned CharacterOffsetBegin=107 CharacterOffsetEnd=116 PartOfSpeech=VBN Lemma=concern]\n",
      "[Text=with CharacterOffsetBegin=117 CharacterOffsetEnd=121 PartOfSpeech=IN Lemma=with]\n",
      "[Text=the CharacterOffsetBegin=122 CharacterOffsetEnd=125 PartOfSpeech=DT Lemma=the]\n",
      "[Text=interactions CharacterOffsetBegin=126 CharacterOffsetEnd=138 PartOfSpeech=NNS Lemma=interaction]\n",
      "[Text=between CharacterOffsetBegin=139 CharacterOffsetEnd=146 PartOfSpeech=IN Lemma=between]\n",
      "[Text=computers CharacterOffsetBegin=147 CharacterOffsetEnd=156 PartOfSpeech=NNS Lemma=computer]\n",
      "[Text=and CharacterOffsetBegin=157 CharacterOffsetEnd=160 PartOfSpeech=CC Lemma=and]\n",
      "[Text=human CharacterOffsetBegin=161 CharacterOffsetEnd=166 PartOfSpeech=JJ Lemma=human]\n",
      "[Text=-LRB- CharacterOffsetBegin=167 CharacterOffsetEnd=168 PartOfSpeech=-LRB- Lemma=-lrb-]\n",
      "[Text=natural CharacterOffsetBegin=168 CharacterOffsetEnd=175 PartOfSpeech=JJ Lemma=natural]\n",
      "[Text=-RRB- CharacterOffsetBegin=175 CharacterOffsetEnd=176 PartOfSpeech=-RRB- Lemma=-rrb-]\n",
      "[Text=languages CharacterOffsetBegin=177 CharacterOffsetEnd=186 PartOfSpeech=NNS Lemma=language]\n",
      "[Text=. CharacterOffsetBegin=186 CharacterOffsetEnd=187 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (13 tokens):\n",
      "As such, NLP is related to the area of humani-computer interaction.\n",
      "\n",
      "Tokens:\n",
      "[Text=As CharacterOffsetBegin=0 CharacterOffsetEnd=2 PartOfSpeech=IN Lemma=as]\n",
      "[Text=such CharacterOffsetBegin=3 CharacterOffsetEnd=7 PartOfSpeech=JJ Lemma=such]\n",
      "[Text=, CharacterOffsetBegin=7 CharacterOffsetEnd=8 PartOfSpeech=, Lemma=,]\n",
      "[Text=NLP CharacterOffsetBegin=9 CharacterOffsetEnd=12 PartOfSpeech=NN Lemma=nlp]\n",
      "[Text=is CharacterOffsetBegin=13 CharacterOffsetEnd=15 PartOfSpeech=VBZ Lemma=be]\n",
      "[Text=related CharacterOffsetBegin=16 CharacterOffsetEnd=23 PartOfSpeech=VBN Lemma=relate]\n",
      "[Text=to CharacterOffsetBegin=24 CharacterOffsetEnd=26 PartOfSpeech=TO Lemma=to]\n",
      "[Text=the CharacterOffsetBegin=27 CharacterOffsetEnd=30 PartOfSpeech=DT Lemma=the]\n",
      "[Text=area CharacterOffsetBegin=31 CharacterOffsetEnd=35 PartOfSpeech=NN Lemma=area]\n",
      "[Text=of CharacterOffsetBegin=36 CharacterOffsetEnd=38 PartOfSpeech=IN Lemma=of]\n",
      "[Text=humani-computer CharacterOffsetBegin=39 CharacterOffsetEnd=54 PartOfSpeech=JJ Lemma=humani-computer]\n",
      "[Text=interaction CharacterOffsetBegin=55 CharacterOffsetEnd=66 PartOfSpeech=NN Lemma=interaction]\n",
      "[Text=. CharacterOffsetBegin=66 CharacterOffsetEnd=67 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (31 tokens):\n",
      "Many challenges in NLP involve natural language understanding, that is, enabling computers to derive meaning from human or natural language input, and others involve natural language generation.\n",
      "\n",
      "Tokens:\n",
      "[Text=Many CharacterOffsetBegin=0 CharacterOffsetEnd=4 PartOfSpeech=JJ Lemma=many]\n",
      "[Text=challenges CharacterOffsetBegin=5 CharacterOffsetEnd=15 PartOfSpeech=NNS Lemma=challenge]\n",
      "[Text=in CharacterOffsetBegin=16 CharacterOffsetEnd=18 PartOfSpeech=IN Lemma=in]\n",
      "[Text=NLP CharacterOffsetBegin=19 CharacterOffsetEnd=22 PartOfSpeech=NN Lemma=nlp]\n",
      "[Text=involve CharacterOffsetBegin=23 CharacterOffsetEnd=30 PartOfSpeech=VBP Lemma=involve]\n",
      "[Text=natural CharacterOffsetBegin=31 CharacterOffsetEnd=38 PartOfSpeech=JJ Lemma=natural]\n",
      "[Text=language CharacterOffsetBegin=39 CharacterOffsetEnd=47 PartOfSpeech=NN Lemma=language]\n",
      "[Text=understanding CharacterOffsetBegin=48 CharacterOffsetEnd=61 PartOfSpeech=NN Lemma=understanding]\n",
      "[Text=, CharacterOffsetBegin=61 CharacterOffsetEnd=62 PartOfSpeech=, Lemma=,]\n",
      "[Text=that CharacterOffsetBegin=63 CharacterOffsetEnd=67 PartOfSpeech=WDT Lemma=that]\n",
      "[Text=is CharacterOffsetBegin=68 CharacterOffsetEnd=70 PartOfSpeech=VBZ Lemma=be]\n",
      "[Text=, CharacterOffsetBegin=70 CharacterOffsetEnd=71 PartOfSpeech=, Lemma=,]\n",
      "[Text=enabling CharacterOffsetBegin=72 CharacterOffsetEnd=80 PartOfSpeech=VBG Lemma=enable]\n",
      "[Text=computers CharacterOffsetBegin=81 CharacterOffsetEnd=90 PartOfSpeech=NNS Lemma=computer]\n",
      "[Text=to CharacterOffsetBegin=91 CharacterOffsetEnd=93 PartOfSpeech=TO Lemma=to]\n",
      "[Text=derive CharacterOffsetBegin=94 CharacterOffsetEnd=100 PartOfSpeech=VB Lemma=derive]\n",
      "[Text=meaning CharacterOffsetBegin=101 CharacterOffsetEnd=108 PartOfSpeech=NN Lemma=meaning]\n",
      "[Text=from CharacterOffsetBegin=109 CharacterOffsetEnd=113 PartOfSpeech=IN Lemma=from]\n",
      "[Text=human CharacterOffsetBegin=114 CharacterOffsetEnd=119 PartOfSpeech=JJ Lemma=human]\n",
      "[Text=or CharacterOffsetBegin=120 CharacterOffsetEnd=122 PartOfSpeech=CC Lemma=or]\n",
      "[Text=natural CharacterOffsetBegin=123 CharacterOffsetEnd=130 PartOfSpeech=JJ Lemma=natural]\n",
      "[Text=language CharacterOffsetBegin=131 CharacterOffsetEnd=139 PartOfSpeech=NN Lemma=language]\n",
      "[Text=input CharacterOffsetBegin=140 CharacterOffsetEnd=145 PartOfSpeech=NN Lemma=input]\n",
      "[Text=, CharacterOffsetBegin=145 CharacterOffsetEnd=146 PartOfSpeech=, Lemma=,]\n",
      "[Text=and CharacterOffsetBegin=147 CharacterOffsetEnd=150 PartOfSpeech=CC Lemma=and]\n",
      "[Text=others CharacterOffsetBegin=151 CharacterOffsetEnd=157 PartOfSpeech=NNS Lemma=other]\n",
      "[Text=involve CharacterOffsetBegin=158 CharacterOffsetEnd=165 PartOfSpeech=VBP Lemma=involve]\n",
      "[Text=natural CharacterOffsetBegin=166 CharacterOffsetEnd=173 PartOfSpeech=JJ Lemma=natural]\n",
      "[Text=language CharacterOffsetBegin=174 CharacterOffsetEnd=182 PartOfSpeech=NN Lemma=language]\n",
      "[Text=generation CharacterOffsetBegin=183 CharacterOffsetEnd=193 PartOfSpeech=NN Lemma=generation]\n",
      "[Text=. CharacterOffsetBegin=193 CharacterOffsetEnd=194 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (1 tokens):\n",
      "History\n",
      "\n",
      "Tokens:\n",
      "[Text=History CharacterOffsetBegin=0 CharacterOffsetEnd=7 PartOfSpeech=NN Lemma=history]\n",
      "\n",
      "\n",
      "Sentence #1 (19 tokens):\n",
      "The history of NLP generally starts in the 1950s, although work can be found from earlier periods.\n",
      "\n",
      "Tokens:\n",
      "[Text=The CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=DT Lemma=the]\n",
      "[Text=history CharacterOffsetBegin=4 CharacterOffsetEnd=11 PartOfSpeech=NN Lemma=history]\n",
      "[Text=of CharacterOffsetBegin=12 CharacterOffsetEnd=14 PartOfSpeech=IN Lemma=of]\n",
      "[Text=NLP CharacterOffsetBegin=15 CharacterOffsetEnd=18 PartOfSpeech=NNP Lemma=NLP]\n",
      "[Text=generally CharacterOffsetBegin=19 CharacterOffsetEnd=28 PartOfSpeech=RB Lemma=generally]\n",
      "[Text=starts CharacterOffsetBegin=29 CharacterOffsetEnd=35 PartOfSpeech=VBZ Lemma=start]\n",
      "[Text=in CharacterOffsetBegin=36 CharacterOffsetEnd=38 PartOfSpeech=IN Lemma=in]\n",
      "[Text=the CharacterOffsetBegin=39 CharacterOffsetEnd=42 PartOfSpeech=DT Lemma=the]\n",
      "[Text=1950s CharacterOffsetBegin=43 CharacterOffsetEnd=48 PartOfSpeech=CD Lemma=1950s]\n",
      "[Text=, CharacterOffsetBegin=48 CharacterOffsetEnd=49 PartOfSpeech=, Lemma=,]\n",
      "[Text=although CharacterOffsetBegin=50 CharacterOffsetEnd=58 PartOfSpeech=IN Lemma=although]\n",
      "[Text=work CharacterOffsetBegin=59 CharacterOffsetEnd=63 PartOfSpeech=NN Lemma=work]\n",
      "[Text=can CharacterOffsetBegin=64 CharacterOffsetEnd=67 PartOfSpeech=MD Lemma=can]\n",
      "[Text=be CharacterOffsetBegin=68 CharacterOffsetEnd=70 PartOfSpeech=VB Lemma=be]\n",
      "[Text=found CharacterOffsetBegin=71 CharacterOffsetEnd=76 PartOfSpeech=VBN Lemma=find]\n",
      "[Text=from CharacterOffsetBegin=77 CharacterOffsetEnd=81 PartOfSpeech=IN Lemma=from]\n",
      "[Text=earlier CharacterOffsetBegin=82 CharacterOffsetEnd=89 PartOfSpeech=JJR Lemma=earlier]\n",
      "[Text=periods CharacterOffsetBegin=90 CharacterOffsetEnd=97 PartOfSpeech=NNS Lemma=period]\n",
      "[Text=. CharacterOffsetBegin=97 CharacterOffsetEnd=98 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (30 tokens):\n",
      "In 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence.\n",
      "\n",
      "Tokens:\n",
      "[Text=In CharacterOffsetBegin=0 CharacterOffsetEnd=2 PartOfSpeech=IN Lemma=in]\n",
      "[Text=1950 CharacterOffsetBegin=3 CharacterOffsetEnd=7 PartOfSpeech=CD Lemma=1950]\n",
      "[Text=, CharacterOffsetBegin=7 CharacterOffsetEnd=8 PartOfSpeech=, Lemma=,]\n",
      "[Text=Alan CharacterOffsetBegin=9 CharacterOffsetEnd=13 PartOfSpeech=NNP Lemma=Alan]\n",
      "[Text=Turing CharacterOffsetBegin=14 CharacterOffsetEnd=20 PartOfSpeech=NNP Lemma=Turing]\n",
      "[Text=published CharacterOffsetBegin=21 CharacterOffsetEnd=30 PartOfSpeech=VBD Lemma=publish]\n",
      "[Text=an CharacterOffsetBegin=31 CharacterOffsetEnd=33 PartOfSpeech=DT Lemma=a]\n",
      "[Text=article CharacterOffsetBegin=34 CharacterOffsetEnd=41 PartOfSpeech=NN Lemma=article]\n",
      "[Text=titled CharacterOffsetBegin=42 CharacterOffsetEnd=48 PartOfSpeech=VBN Lemma=title]\n",
      "[Text=`` CharacterOffsetBegin=49 CharacterOffsetEnd=50 PartOfSpeech=`` Lemma=``]\n",
      "[Text=Computing CharacterOffsetBegin=50 CharacterOffsetEnd=59 PartOfSpeech=NNP Lemma=Computing]\n",
      "[Text=Machinery CharacterOffsetBegin=60 CharacterOffsetEnd=69 PartOfSpeech=NNP Lemma=Machinery]\n",
      "[Text=and CharacterOffsetBegin=70 CharacterOffsetEnd=73 PartOfSpeech=CC Lemma=and]\n",
      "[Text=Intelligence CharacterOffsetBegin=74 CharacterOffsetEnd=86 PartOfSpeech=NNP Lemma=Intelligence]\n",
      "[Text='' CharacterOffsetBegin=86 CharacterOffsetEnd=87 PartOfSpeech='' Lemma='']\n",
      "[Text=which CharacterOffsetBegin=88 CharacterOffsetEnd=93 PartOfSpeech=WDT Lemma=which]\n",
      "[Text=proposed CharacterOffsetBegin=94 CharacterOffsetEnd=102 PartOfSpeech=VBD Lemma=propose]\n",
      "[Text=what CharacterOffsetBegin=103 CharacterOffsetEnd=107 PartOfSpeech=WP Lemma=what]\n",
      "[Text=is CharacterOffsetBegin=108 CharacterOffsetEnd=110 PartOfSpeech=VBZ Lemma=be]\n",
      "[Text=now CharacterOffsetBegin=111 CharacterOffsetEnd=114 PartOfSpeech=RB Lemma=now]\n",
      "[Text=called CharacterOffsetBegin=115 CharacterOffsetEnd=121 PartOfSpeech=VBN Lemma=call]\n",
      "[Text=the CharacterOffsetBegin=122 CharacterOffsetEnd=125 PartOfSpeech=DT Lemma=the]\n",
      "[Text=Turing CharacterOffsetBegin=126 CharacterOffsetEnd=132 PartOfSpeech=JJ Lemma=turing]\n",
      "[Text=test CharacterOffsetBegin=133 CharacterOffsetEnd=137 PartOfSpeech=NN Lemma=test]\n",
      "[Text=as CharacterOffsetBegin=138 CharacterOffsetEnd=140 PartOfSpeech=IN Lemma=as]\n",
      "[Text=a CharacterOffsetBegin=141 CharacterOffsetEnd=142 PartOfSpeech=DT Lemma=a]\n",
      "[Text=criterion CharacterOffsetBegin=143 CharacterOffsetEnd=152 PartOfSpeech=NN Lemma=criterion]\n",
      "[Text=of CharacterOffsetBegin=153 CharacterOffsetEnd=155 PartOfSpeech=IN Lemma=of]\n",
      "[Text=intelligence CharacterOffsetBegin=156 CharacterOffsetEnd=168 PartOfSpeech=NN Lemma=intelligence]\n",
      "[Text=. CharacterOffsetBegin=168 CharacterOffsetEnd=169 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (18 tokens):\n",
      "The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English.\n",
      "\n",
      "Tokens:\n",
      "[Text=The CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=DT Lemma=the]\n",
      "[Text=Georgetown CharacterOffsetBegin=4 CharacterOffsetEnd=14 PartOfSpeech=NNP Lemma=Georgetown]\n",
      "[Text=experiment CharacterOffsetBegin=15 CharacterOffsetEnd=25 PartOfSpeech=NN Lemma=experiment]\n",
      "[Text=in CharacterOffsetBegin=26 CharacterOffsetEnd=28 PartOfSpeech=IN Lemma=in]\n",
      "[Text=1954 CharacterOffsetBegin=29 CharacterOffsetEnd=33 PartOfSpeech=CD Lemma=1954]\n",
      "[Text=involved CharacterOffsetBegin=34 CharacterOffsetEnd=42 PartOfSpeech=VBN Lemma=involve]\n",
      "[Text=fully CharacterOffsetBegin=43 CharacterOffsetEnd=48 PartOfSpeech=RB Lemma=fully]\n",
      "[Text=automatic CharacterOffsetBegin=49 CharacterOffsetEnd=58 PartOfSpeech=JJ Lemma=automatic]\n",
      "[Text=translation CharacterOffsetBegin=59 CharacterOffsetEnd=70 PartOfSpeech=NN Lemma=translation]\n",
      "[Text=of CharacterOffsetBegin=71 CharacterOffsetEnd=73 PartOfSpeech=IN Lemma=of]\n",
      "[Text=more CharacterOffsetBegin=74 CharacterOffsetEnd=78 PartOfSpeech=JJR Lemma=more]\n",
      "[Text=than CharacterOffsetBegin=79 CharacterOffsetEnd=83 PartOfSpeech=IN Lemma=than]\n",
      "[Text=sixty CharacterOffsetBegin=84 CharacterOffsetEnd=89 PartOfSpeech=CD Lemma=sixty]\n",
      "[Text=Russian CharacterOffsetBegin=90 CharacterOffsetEnd=97 PartOfSpeech=JJ Lemma=russian]\n",
      "[Text=sentences CharacterOffsetBegin=98 CharacterOffsetEnd=107 PartOfSpeech=NNS Lemma=sentence]\n",
      "[Text=into CharacterOffsetBegin=108 CharacterOffsetEnd=112 PartOfSpeech=IN Lemma=into]\n",
      "[Text=English CharacterOffsetBegin=113 CharacterOffsetEnd=120 PartOfSpeech=NNP Lemma=English]\n",
      "[Text=. CharacterOffsetBegin=120 CharacterOffsetEnd=121 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (18 tokens):\n",
      "The authors claimed that within three or five years, machine translation would be a solved problem.\n",
      "\n",
      "Tokens:\n",
      "[Text=The CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=DT Lemma=the]\n",
      "[Text=authors CharacterOffsetBegin=4 CharacterOffsetEnd=11 PartOfSpeech=NNS Lemma=author]\n",
      "[Text=claimed CharacterOffsetBegin=12 CharacterOffsetEnd=19 PartOfSpeech=VBD Lemma=claim]\n",
      "[Text=that CharacterOffsetBegin=20 CharacterOffsetEnd=24 PartOfSpeech=IN Lemma=that]\n",
      "[Text=within CharacterOffsetBegin=25 CharacterOffsetEnd=31 PartOfSpeech=IN Lemma=within]\n",
      "[Text=three CharacterOffsetBegin=32 CharacterOffsetEnd=37 PartOfSpeech=CD Lemma=three]\n",
      "[Text=or CharacterOffsetBegin=38 CharacterOffsetEnd=40 PartOfSpeech=CC Lemma=or]\n",
      "[Text=five CharacterOffsetBegin=41 CharacterOffsetEnd=45 PartOfSpeech=CD Lemma=five]\n",
      "[Text=years CharacterOffsetBegin=46 CharacterOffsetEnd=51 PartOfSpeech=NNS Lemma=year]\n",
      "[Text=, CharacterOffsetBegin=51 CharacterOffsetEnd=52 PartOfSpeech=, Lemma=,]\n",
      "[Text=machine CharacterOffsetBegin=53 CharacterOffsetEnd=60 PartOfSpeech=NN Lemma=machine]\n",
      "[Text=translation CharacterOffsetBegin=61 CharacterOffsetEnd=72 PartOfSpeech=NN Lemma=translation]\n",
      "[Text=would CharacterOffsetBegin=73 CharacterOffsetEnd=78 PartOfSpeech=MD Lemma=would]\n",
      "[Text=be CharacterOffsetBegin=79 CharacterOffsetEnd=81 PartOfSpeech=VB Lemma=be]\n",
      "[Text=a CharacterOffsetBegin=82 CharacterOffsetEnd=83 PartOfSpeech=DT Lemma=a]\n",
      "[Text=solved CharacterOffsetBegin=84 CharacterOffsetEnd=90 PartOfSpeech=VBN Lemma=solve]\n",
      "[Text=problem CharacterOffsetBegin=91 CharacterOffsetEnd=98 PartOfSpeech=NN Lemma=problem]\n",
      "[Text=. CharacterOffsetBegin=98 CharacterOffsetEnd=99 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (38 tokens):\n",
      "However, real progress was much slower, and after the ALPAC report in 1966, which found that ten year long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced.\n",
      "\n",
      "Tokens:\n",
      "[Text=However CharacterOffsetBegin=0 CharacterOffsetEnd=7 PartOfSpeech=RB Lemma=however]\n",
      "[Text=, CharacterOffsetBegin=7 CharacterOffsetEnd=8 PartOfSpeech=, Lemma=,]\n",
      "[Text=real CharacterOffsetBegin=9 CharacterOffsetEnd=13 PartOfSpeech=JJ Lemma=real]\n",
      "[Text=progress CharacterOffsetBegin=14 CharacterOffsetEnd=22 PartOfSpeech=NN Lemma=progress]\n",
      "[Text=was CharacterOffsetBegin=23 CharacterOffsetEnd=26 PartOfSpeech=VBD Lemma=be]\n",
      "[Text=much CharacterOffsetBegin=27 CharacterOffsetEnd=31 PartOfSpeech=RB Lemma=much]\n",
      "[Text=slower CharacterOffsetBegin=32 CharacterOffsetEnd=38 PartOfSpeech=JJR Lemma=slower]\n",
      "[Text=, CharacterOffsetBegin=38 CharacterOffsetEnd=39 PartOfSpeech=, Lemma=,]\n",
      "[Text=and CharacterOffsetBegin=40 CharacterOffsetEnd=43 PartOfSpeech=CC Lemma=and]\n",
      "[Text=after CharacterOffsetBegin=44 CharacterOffsetEnd=49 PartOfSpeech=IN Lemma=after]\n",
      "[Text=the CharacterOffsetBegin=50 CharacterOffsetEnd=53 PartOfSpeech=DT Lemma=the]\n",
      "[Text=ALPAC CharacterOffsetBegin=54 CharacterOffsetEnd=59 PartOfSpeech=NNP Lemma=ALPAC]\n",
      "[Text=report CharacterOffsetBegin=60 CharacterOffsetEnd=66 PartOfSpeech=NN Lemma=report]\n",
      "[Text=in CharacterOffsetBegin=67 CharacterOffsetEnd=69 PartOfSpeech=IN Lemma=in]\n",
      "[Text=1966 CharacterOffsetBegin=70 CharacterOffsetEnd=74 PartOfSpeech=CD Lemma=1966]\n",
      "[Text=, CharacterOffsetBegin=74 CharacterOffsetEnd=75 PartOfSpeech=, Lemma=,]\n",
      "[Text=which CharacterOffsetBegin=76 CharacterOffsetEnd=81 PartOfSpeech=WDT Lemma=which]\n",
      "[Text=found CharacterOffsetBegin=82 CharacterOffsetEnd=87 PartOfSpeech=VBD Lemma=find]\n",
      "[Text=that CharacterOffsetBegin=88 CharacterOffsetEnd=92 PartOfSpeech=IN Lemma=that]\n",
      "[Text=ten CharacterOffsetBegin=93 CharacterOffsetEnd=96 PartOfSpeech=CD Lemma=ten]\n",
      "[Text=year CharacterOffsetBegin=97 CharacterOffsetEnd=101 PartOfSpeech=NN Lemma=year]\n",
      "[Text=long CharacterOffsetBegin=102 CharacterOffsetEnd=106 PartOfSpeech=RB Lemma=long]\n",
      "[Text=research CharacterOffsetBegin=107 CharacterOffsetEnd=115 PartOfSpeech=NN Lemma=research]\n",
      "[Text=had CharacterOffsetBegin=116 CharacterOffsetEnd=119 PartOfSpeech=VBD Lemma=have]\n",
      "[Text=failed CharacterOffsetBegin=120 CharacterOffsetEnd=126 PartOfSpeech=VBN Lemma=fail]\n",
      "[Text=to CharacterOffsetBegin=127 CharacterOffsetEnd=129 PartOfSpeech=TO Lemma=to]\n",
      "[Text=fulfill CharacterOffsetBegin=130 CharacterOffsetEnd=137 PartOfSpeech=VB Lemma=fulfill]\n",
      "[Text=the CharacterOffsetBegin=138 CharacterOffsetEnd=141 PartOfSpeech=DT Lemma=the]\n",
      "[Text=expectations CharacterOffsetBegin=142 CharacterOffsetEnd=154 PartOfSpeech=NNS Lemma=expectation]\n",
      "[Text=, CharacterOffsetBegin=154 CharacterOffsetEnd=155 PartOfSpeech=, Lemma=,]\n",
      "[Text=funding CharacterOffsetBegin=156 CharacterOffsetEnd=163 PartOfSpeech=VBG Lemma=fund]\n",
      "[Text=for CharacterOffsetBegin=164 CharacterOffsetEnd=167 PartOfSpeech=IN Lemma=for]\n",
      "[Text=machine CharacterOffsetBegin=168 CharacterOffsetEnd=175 PartOfSpeech=NN Lemma=machine]\n",
      "[Text=translation CharacterOffsetBegin=176 CharacterOffsetEnd=187 PartOfSpeech=NN Lemma=translation]\n",
      "[Text=was CharacterOffsetBegin=188 CharacterOffsetEnd=191 PartOfSpeech=VBD Lemma=be]\n",
      "[Text=dramatically CharacterOffsetBegin=192 CharacterOffsetEnd=204 PartOfSpeech=RB Lemma=dramatically]\n",
      "[Text=reduced CharacterOffsetBegin=205 CharacterOffsetEnd=212 PartOfSpeech=VBN Lemma=reduce]\n",
      "[Text=. CharacterOffsetBegin=212 CharacterOffsetEnd=213 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (23 tokens):\n",
      "Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.\n",
      "\n",
      "Tokens:\n",
      "[Text=Little CharacterOffsetBegin=0 CharacterOffsetEnd=6 PartOfSpeech=JJ Lemma=little]\n",
      "[Text=further CharacterOffsetBegin=7 CharacterOffsetEnd=14 PartOfSpeech=JJ Lemma=further]\n",
      "[Text=research CharacterOffsetBegin=15 CharacterOffsetEnd=23 PartOfSpeech=NN Lemma=research]\n",
      "[Text=in CharacterOffsetBegin=24 CharacterOffsetEnd=26 PartOfSpeech=IN Lemma=in]\n",
      "[Text=machine CharacterOffsetBegin=27 CharacterOffsetEnd=34 PartOfSpeech=NN Lemma=machine]\n",
      "[Text=translation CharacterOffsetBegin=35 CharacterOffsetEnd=46 PartOfSpeech=NN Lemma=translation]\n",
      "[Text=was CharacterOffsetBegin=47 CharacterOffsetEnd=50 PartOfSpeech=VBD Lemma=be]\n",
      "[Text=conducted CharacterOffsetBegin=51 CharacterOffsetEnd=60 PartOfSpeech=VBN Lemma=conduct]\n",
      "[Text=until CharacterOffsetBegin=61 CharacterOffsetEnd=66 PartOfSpeech=IN Lemma=until]\n",
      "[Text=the CharacterOffsetBegin=67 CharacterOffsetEnd=70 PartOfSpeech=DT Lemma=the]\n",
      "[Text=late CharacterOffsetBegin=71 CharacterOffsetEnd=75 PartOfSpeech=JJ Lemma=late]\n",
      "[Text=1980s CharacterOffsetBegin=76 CharacterOffsetEnd=81 PartOfSpeech=NNS Lemma=1980]\n",
      "[Text=, CharacterOffsetBegin=81 CharacterOffsetEnd=82 PartOfSpeech=, Lemma=,]\n",
      "[Text=when CharacterOffsetBegin=83 CharacterOffsetEnd=87 PartOfSpeech=WRB Lemma=when]\n",
      "[Text=the CharacterOffsetBegin=88 CharacterOffsetEnd=91 PartOfSpeech=DT Lemma=the]\n",
      "[Text=first CharacterOffsetBegin=92 CharacterOffsetEnd=97 PartOfSpeech=JJ Lemma=first]\n",
      "[Text=statistical CharacterOffsetBegin=98 CharacterOffsetEnd=109 PartOfSpeech=JJ Lemma=statistical]\n",
      "[Text=machine CharacterOffsetBegin=110 CharacterOffsetEnd=117 PartOfSpeech=NN Lemma=machine]\n",
      "[Text=translation CharacterOffsetBegin=118 CharacterOffsetEnd=129 PartOfSpeech=NN Lemma=translation]\n",
      "[Text=systems CharacterOffsetBegin=130 CharacterOffsetEnd=137 PartOfSpeech=NNS Lemma=system]\n",
      "[Text=were CharacterOffsetBegin=138 CharacterOffsetEnd=142 PartOfSpeech=VBD Lemma=be]\n",
      "[Text=developed CharacterOffsetBegin=143 CharacterOffsetEnd=152 PartOfSpeech=VBN Lemma=develop]\n",
      "[Text=. CharacterOffsetBegin=152 CharacterOffsetEnd=153 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (46 tokens):\n",
      "Some notably successful NLP systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 to 1966.\n",
      "\n",
      "Tokens:\n",
      "[Text=Some CharacterOffsetBegin=0 CharacterOffsetEnd=4 PartOfSpeech=DT Lemma=some]\n",
      "[Text=notably CharacterOffsetBegin=5 CharacterOffsetEnd=12 PartOfSpeech=RB Lemma=notably]\n",
      "[Text=successful CharacterOffsetBegin=13 CharacterOffsetEnd=23 PartOfSpeech=JJ Lemma=successful]\n",
      "[Text=NLP CharacterOffsetBegin=24 CharacterOffsetEnd=27 PartOfSpeech=NN Lemma=nlp]\n",
      "[Text=systems CharacterOffsetBegin=28 CharacterOffsetEnd=35 PartOfSpeech=NNS Lemma=system]\n",
      "[Text=developed CharacterOffsetBegin=36 CharacterOffsetEnd=45 PartOfSpeech=VBN Lemma=develop]\n",
      "[Text=in CharacterOffsetBegin=46 CharacterOffsetEnd=48 PartOfSpeech=IN Lemma=in]\n",
      "[Text=the CharacterOffsetBegin=49 CharacterOffsetEnd=52 PartOfSpeech=DT Lemma=the]\n",
      "[Text=1960s CharacterOffsetBegin=53 CharacterOffsetEnd=58 PartOfSpeech=NNS Lemma=1960]\n",
      "[Text=were CharacterOffsetBegin=59 CharacterOffsetEnd=63 PartOfSpeech=VBD Lemma=be]\n",
      "[Text=SHRDLU CharacterOffsetBegin=64 CharacterOffsetEnd=70 PartOfSpeech=NNP Lemma=SHRDLU]\n",
      "[Text=, CharacterOffsetBegin=70 CharacterOffsetEnd=71 PartOfSpeech=, Lemma=,]\n",
      "[Text=a CharacterOffsetBegin=72 CharacterOffsetEnd=73 PartOfSpeech=DT Lemma=a]\n",
      "[Text=natural CharacterOffsetBegin=74 CharacterOffsetEnd=81 PartOfSpeech=JJ Lemma=natural]\n",
      "[Text=language CharacterOffsetBegin=82 CharacterOffsetEnd=90 PartOfSpeech=NN Lemma=language]\n",
      "[Text=system CharacterOffsetBegin=91 CharacterOffsetEnd=97 PartOfSpeech=NN Lemma=system]\n",
      "[Text=working CharacterOffsetBegin=98 CharacterOffsetEnd=105 PartOfSpeech=VBG Lemma=work]\n",
      "[Text=in CharacterOffsetBegin=106 CharacterOffsetEnd=108 PartOfSpeech=IN Lemma=in]\n",
      "[Text=restricted CharacterOffsetBegin=109 CharacterOffsetEnd=119 PartOfSpeech=JJ Lemma=restricted]\n",
      "[Text=`` CharacterOffsetBegin=120 CharacterOffsetEnd=121 PartOfSpeech=`` Lemma=``]\n",
      "[Text=blocks CharacterOffsetBegin=121 CharacterOffsetEnd=127 PartOfSpeech=NNS Lemma=block]\n",
      "[Text=worlds CharacterOffsetBegin=128 CharacterOffsetEnd=134 PartOfSpeech=NNS Lemma=world]\n",
      "[Text='' CharacterOffsetBegin=134 CharacterOffsetEnd=135 PartOfSpeech='' Lemma='']\n",
      "[Text=with CharacterOffsetBegin=136 CharacterOffsetEnd=140 PartOfSpeech=IN Lemma=with]\n",
      "[Text=restricted CharacterOffsetBegin=141 CharacterOffsetEnd=151 PartOfSpeech=JJ Lemma=restricted]\n",
      "[Text=vocabularies CharacterOffsetBegin=152 CharacterOffsetEnd=164 PartOfSpeech=NNS Lemma=vocabulary]\n",
      "[Text=, CharacterOffsetBegin=164 CharacterOffsetEnd=165 PartOfSpeech=, Lemma=,]\n",
      "[Text=and CharacterOffsetBegin=166 CharacterOffsetEnd=169 PartOfSpeech=CC Lemma=and]\n",
      "[Text=ELIZA CharacterOffsetBegin=170 CharacterOffsetEnd=175 PartOfSpeech=NNP Lemma=ELIZA]\n",
      "[Text=, CharacterOffsetBegin=175 CharacterOffsetEnd=176 PartOfSpeech=, Lemma=,]\n",
      "[Text=a CharacterOffsetBegin=177 CharacterOffsetEnd=178 PartOfSpeech=DT Lemma=a]\n",
      "[Text=simulation CharacterOffsetBegin=179 CharacterOffsetEnd=189 PartOfSpeech=NN Lemma=simulation]\n",
      "[Text=of CharacterOffsetBegin=190 CharacterOffsetEnd=192 PartOfSpeech=IN Lemma=of]\n",
      "[Text=a CharacterOffsetBegin=193 CharacterOffsetEnd=194 PartOfSpeech=DT Lemma=a]\n",
      "[Text=Rogerian CharacterOffsetBegin=195 CharacterOffsetEnd=203 PartOfSpeech=JJ Lemma=rogerian]\n",
      "[Text=psychotherapist CharacterOffsetBegin=204 CharacterOffsetEnd=219 PartOfSpeech=NN Lemma=psychotherapist]\n",
      "[Text=, CharacterOffsetBegin=219 CharacterOffsetEnd=220 PartOfSpeech=, Lemma=,]\n",
      "[Text=written CharacterOffsetBegin=221 CharacterOffsetEnd=228 PartOfSpeech=VBN Lemma=write]\n",
      "[Text=by CharacterOffsetBegin=229 CharacterOffsetEnd=231 PartOfSpeech=IN Lemma=by]\n",
      "[Text=Joseph CharacterOffsetBegin=232 CharacterOffsetEnd=238 PartOfSpeech=NNP Lemma=Joseph]\n",
      "[Text=Weizenbaum CharacterOffsetBegin=239 CharacterOffsetEnd=249 PartOfSpeech=NNP Lemma=Weizenbaum]\n",
      "[Text=between CharacterOffsetBegin=250 CharacterOffsetEnd=257 PartOfSpeech=IN Lemma=between]\n",
      "[Text=1964 CharacterOffsetBegin=258 CharacterOffsetEnd=262 PartOfSpeech=CD Lemma=1964]\n",
      "[Text=to CharacterOffsetBegin=263 CharacterOffsetEnd=265 PartOfSpeech=TO Lemma=to]\n",
      "[Text=1966 CharacterOffsetBegin=266 CharacterOffsetEnd=270 PartOfSpeech=CD Lemma=1966]\n",
      "[Text=. CharacterOffsetBegin=270 CharacterOffsetEnd=271 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (18 tokens):\n",
      "Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction.\n",
      "\n",
      "Tokens:\n",
      "[Text=Using CharacterOffsetBegin=0 CharacterOffsetEnd=5 PartOfSpeech=VBG Lemma=use]\n",
      "[Text=almost CharacterOffsetBegin=6 CharacterOffsetEnd=12 PartOfSpeech=RB Lemma=almost]\n",
      "[Text=no CharacterOffsetBegin=13 CharacterOffsetEnd=15 PartOfSpeech=DT Lemma=no]\n",
      "[Text=information CharacterOffsetBegin=16 CharacterOffsetEnd=27 PartOfSpeech=NN Lemma=information]\n",
      "[Text=about CharacterOffsetBegin=28 CharacterOffsetEnd=33 PartOfSpeech=IN Lemma=about]\n",
      "[Text=human CharacterOffsetBegin=34 CharacterOffsetEnd=39 PartOfSpeech=JJ Lemma=human]\n",
      "[Text=thought CharacterOffsetBegin=40 CharacterOffsetEnd=47 PartOfSpeech=NN Lemma=thought]\n",
      "[Text=or CharacterOffsetBegin=48 CharacterOffsetEnd=50 PartOfSpeech=CC Lemma=or]\n",
      "[Text=emotion CharacterOffsetBegin=51 CharacterOffsetEnd=58 PartOfSpeech=NN Lemma=emotion]\n",
      "[Text=, CharacterOffsetBegin=58 CharacterOffsetEnd=59 PartOfSpeech=, Lemma=,]\n",
      "[Text=ELIZA CharacterOffsetBegin=60 CharacterOffsetEnd=65 PartOfSpeech=NN Lemma=eliza]\n",
      "[Text=sometimes CharacterOffsetBegin=66 CharacterOffsetEnd=75 PartOfSpeech=RB Lemma=sometimes]\n",
      "[Text=provided CharacterOffsetBegin=76 CharacterOffsetEnd=84 PartOfSpeech=VBD Lemma=provide]\n",
      "[Text=a CharacterOffsetBegin=85 CharacterOffsetEnd=86 PartOfSpeech=DT Lemma=a]\n",
      "[Text=startlingly CharacterOffsetBegin=87 CharacterOffsetEnd=98 PartOfSpeech=RB Lemma=startlingly]\n",
      "[Text=human-like CharacterOffsetBegin=99 CharacterOffsetEnd=109 PartOfSpeech=JJ Lemma=human-like]\n",
      "[Text=interaction CharacterOffsetBegin=110 CharacterOffsetEnd=121 PartOfSpeech=NN Lemma=interaction]\n",
      "[Text=. CharacterOffsetBegin=121 CharacterOffsetEnd=122 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (40 tokens):\n",
      "When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\"\n",
      "\n",
      "Tokens:\n",
      "[Text=When CharacterOffsetBegin=0 CharacterOffsetEnd=4 PartOfSpeech=WRB Lemma=when]\n",
      "[Text=the CharacterOffsetBegin=5 CharacterOffsetEnd=8 PartOfSpeech=DT Lemma=the]\n",
      "[Text=`` CharacterOffsetBegin=9 CharacterOffsetEnd=10 PartOfSpeech=`` Lemma=``]\n",
      "[Text=patient CharacterOffsetBegin=10 CharacterOffsetEnd=17 PartOfSpeech=NN Lemma=patient]\n",
      "[Text='' CharacterOffsetBegin=17 CharacterOffsetEnd=18 PartOfSpeech='' Lemma='']\n",
      "[Text=exceeded CharacterOffsetBegin=19 CharacterOffsetEnd=27 PartOfSpeech=VBD Lemma=exceed]\n",
      "[Text=the CharacterOffsetBegin=28 CharacterOffsetEnd=31 PartOfSpeech=DT Lemma=the]\n",
      "[Text=very CharacterOffsetBegin=32 CharacterOffsetEnd=36 PartOfSpeech=RB Lemma=very]\n",
      "[Text=small CharacterOffsetBegin=37 CharacterOffsetEnd=42 PartOfSpeech=JJ Lemma=small]\n",
      "[Text=knowledge CharacterOffsetBegin=43 CharacterOffsetEnd=52 PartOfSpeech=NN Lemma=knowledge]\n",
      "[Text=base CharacterOffsetBegin=53 CharacterOffsetEnd=57 PartOfSpeech=NN Lemma=base]\n",
      "[Text=, CharacterOffsetBegin=57 CharacterOffsetEnd=58 PartOfSpeech=, Lemma=,]\n",
      "[Text=ELIZA CharacterOffsetBegin=59 CharacterOffsetEnd=64 PartOfSpeech=NNP Lemma=ELIZA]\n",
      "[Text=might CharacterOffsetBegin=65 CharacterOffsetEnd=70 PartOfSpeech=MD Lemma=might]\n",
      "[Text=provide CharacterOffsetBegin=71 CharacterOffsetEnd=78 PartOfSpeech=VB Lemma=provide]\n",
      "[Text=a CharacterOffsetBegin=79 CharacterOffsetEnd=80 PartOfSpeech=DT Lemma=a]\n",
      "[Text=generic CharacterOffsetBegin=81 CharacterOffsetEnd=88 PartOfSpeech=JJ Lemma=generic]\n",
      "[Text=response CharacterOffsetBegin=89 CharacterOffsetEnd=97 PartOfSpeech=NN Lemma=response]\n",
      "[Text=, CharacterOffsetBegin=97 CharacterOffsetEnd=98 PartOfSpeech=, Lemma=,]\n",
      "[Text=for CharacterOffsetBegin=99 CharacterOffsetEnd=102 PartOfSpeech=IN Lemma=for]\n",
      "[Text=example CharacterOffsetBegin=103 CharacterOffsetEnd=110 PartOfSpeech=NN Lemma=example]\n",
      "[Text=, CharacterOffsetBegin=110 CharacterOffsetEnd=111 PartOfSpeech=, Lemma=,]\n",
      "[Text=responding CharacterOffsetBegin=112 CharacterOffsetEnd=122 PartOfSpeech=VBG Lemma=respond]\n",
      "[Text=to CharacterOffsetBegin=123 CharacterOffsetEnd=125 PartOfSpeech=TO Lemma=to]\n",
      "[Text=`` CharacterOffsetBegin=126 CharacterOffsetEnd=127 PartOfSpeech=`` Lemma=``]\n",
      "[Text=My CharacterOffsetBegin=127 CharacterOffsetEnd=129 PartOfSpeech=PRP$ Lemma=my]\n",
      "[Text=head CharacterOffsetBegin=130 CharacterOffsetEnd=134 PartOfSpeech=NN Lemma=head]\n",
      "[Text=hurts CharacterOffsetBegin=135 CharacterOffsetEnd=140 PartOfSpeech=VBZ Lemma=hurt]\n",
      "[Text='' CharacterOffsetBegin=140 CharacterOffsetEnd=141 PartOfSpeech='' Lemma='']\n",
      "[Text=with CharacterOffsetBegin=142 CharacterOffsetEnd=146 PartOfSpeech=IN Lemma=with]\n",
      "[Text=`` CharacterOffsetBegin=147 CharacterOffsetEnd=148 PartOfSpeech=`` Lemma=``]\n",
      "[Text=Why CharacterOffsetBegin=148 CharacterOffsetEnd=151 PartOfSpeech=WRB Lemma=why]\n",
      "[Text=do CharacterOffsetBegin=152 CharacterOffsetEnd=154 PartOfSpeech=VBP Lemma=do]\n",
      "[Text=you CharacterOffsetBegin=155 CharacterOffsetEnd=158 PartOfSpeech=PRP Lemma=you]\n",
      "[Text=say CharacterOffsetBegin=159 CharacterOffsetEnd=162 PartOfSpeech=VB Lemma=say]\n",
      "[Text=your CharacterOffsetBegin=163 CharacterOffsetEnd=167 PartOfSpeech=PRP$ Lemma=you]\n",
      "[Text=head CharacterOffsetBegin=168 CharacterOffsetEnd=172 PartOfSpeech=NN Lemma=head]\n",
      "[Text=hurts CharacterOffsetBegin=173 CharacterOffsetEnd=178 PartOfSpeech=VBZ Lemma=hurt]\n",
      "[Text=? CharacterOffsetBegin=178 CharacterOffsetEnd=179 PartOfSpeech=. Lemma=?]\n",
      "[Text='' CharacterOffsetBegin=179 CharacterOffsetEnd=180 PartOfSpeech='' Lemma='']\n",
      "\n",
      "Sentence #2 (1 tokens):\n",
      ".\n",
      "\n",
      "Tokens:\n",
      "[Text=. CharacterOffsetBegin=180 CharacterOffsetEnd=181 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (21 tokens):\n",
      "During the 1970s many programmers began to write 'conceptual ontologies', which structured real-world information into computer-understandable data.\n",
      "\n",
      "Tokens:\n",
      "[Text=During CharacterOffsetBegin=0 CharacterOffsetEnd=6 PartOfSpeech=IN Lemma=during]\n",
      "[Text=the CharacterOffsetBegin=7 CharacterOffsetEnd=10 PartOfSpeech=DT Lemma=the]\n",
      "[Text=1970s CharacterOffsetBegin=11 CharacterOffsetEnd=16 PartOfSpeech=CD Lemma=1970s]\n",
      "[Text=many CharacterOffsetBegin=17 CharacterOffsetEnd=21 PartOfSpeech=JJ Lemma=many]\n",
      "[Text=programmers CharacterOffsetBegin=22 CharacterOffsetEnd=33 PartOfSpeech=NNS Lemma=programmer]\n",
      "[Text=began CharacterOffsetBegin=34 CharacterOffsetEnd=39 PartOfSpeech=VBD Lemma=begin]\n",
      "[Text=to CharacterOffsetBegin=40 CharacterOffsetEnd=42 PartOfSpeech=TO Lemma=to]\n",
      "[Text=write CharacterOffsetBegin=43 CharacterOffsetEnd=48 PartOfSpeech=VB Lemma=write]\n",
      "[Text=` CharacterOffsetBegin=49 CharacterOffsetEnd=50 PartOfSpeech=`` Lemma=`]\n",
      "[Text=conceptual CharacterOffsetBegin=50 CharacterOffsetEnd=60 PartOfSpeech=JJ Lemma=conceptual]\n",
      "[Text=ontologies CharacterOffsetBegin=61 CharacterOffsetEnd=71 PartOfSpeech=NNS Lemma=ontology]\n",
      "[Text=' CharacterOffsetBegin=71 CharacterOffsetEnd=72 PartOfSpeech=POS Lemma=']\n",
      "[Text=, CharacterOffsetBegin=72 CharacterOffsetEnd=73 PartOfSpeech=, Lemma=,]\n",
      "[Text=which CharacterOffsetBegin=74 CharacterOffsetEnd=79 PartOfSpeech=WDT Lemma=which]\n",
      "[Text=structured CharacterOffsetBegin=80 CharacterOffsetEnd=90 PartOfSpeech=VBD Lemma=structure]\n",
      "[Text=real-world CharacterOffsetBegin=91 CharacterOffsetEnd=101 PartOfSpeech=JJ Lemma=real-world]\n",
      "[Text=information CharacterOffsetBegin=102 CharacterOffsetEnd=113 PartOfSpeech=NN Lemma=information]\n",
      "[Text=into CharacterOffsetBegin=114 CharacterOffsetEnd=118 PartOfSpeech=IN Lemma=into]\n",
      "[Text=computer-understandable CharacterOffsetBegin=119 CharacterOffsetEnd=142 PartOfSpeech=JJ Lemma=computer-understandable]\n",
      "[Text=data CharacterOffsetBegin=143 CharacterOffsetEnd=147 PartOfSpeech=NNS Lemma=datum]\n",
      "[Text=. CharacterOffsetBegin=147 CharacterOffsetEnd=148 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (52 tokens):\n",
      "Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981).\n",
      "\n",
      "Tokens:\n",
      "[Text=Examples CharacterOffsetBegin=0 CharacterOffsetEnd=8 PartOfSpeech=NNS Lemma=example]\n",
      "[Text=are CharacterOffsetBegin=9 CharacterOffsetEnd=12 PartOfSpeech=VBP Lemma=be]\n",
      "[Text=MARGIE CharacterOffsetBegin=13 CharacterOffsetEnd=19 PartOfSpeech=NNP Lemma=MARGIE]\n",
      "[Text=-LRB- CharacterOffsetBegin=20 CharacterOffsetEnd=21 PartOfSpeech=-LRB- Lemma=-lrb-]\n",
      "[Text=Schank CharacterOffsetBegin=21 CharacterOffsetEnd=27 PartOfSpeech=NNP Lemma=Schank]\n",
      "[Text=, CharacterOffsetBegin=27 CharacterOffsetEnd=28 PartOfSpeech=, Lemma=,]\n",
      "[Text=1975 CharacterOffsetBegin=29 CharacterOffsetEnd=33 PartOfSpeech=CD Lemma=1975]\n",
      "[Text=-RRB- CharacterOffsetBegin=33 CharacterOffsetEnd=34 PartOfSpeech=-RRB- Lemma=-rrb-]\n",
      "[Text=, CharacterOffsetBegin=34 CharacterOffsetEnd=35 PartOfSpeech=, Lemma=,]\n",
      "[Text=SAM CharacterOffsetBegin=36 CharacterOffsetEnd=39 PartOfSpeech=NNP Lemma=SAM]\n",
      "[Text=-LRB- CharacterOffsetBegin=40 CharacterOffsetEnd=41 PartOfSpeech=-LRB- Lemma=-lrb-]\n",
      "[Text=Cullingford CharacterOffsetBegin=41 CharacterOffsetEnd=52 PartOfSpeech=NNP Lemma=Cullingford]\n",
      "[Text=, CharacterOffsetBegin=52 CharacterOffsetEnd=53 PartOfSpeech=, Lemma=,]\n",
      "[Text=1978 CharacterOffsetBegin=54 CharacterOffsetEnd=58 PartOfSpeech=CD Lemma=1978]\n",
      "[Text=-RRB- CharacterOffsetBegin=58 CharacterOffsetEnd=59 PartOfSpeech=-RRB- Lemma=-rrb-]\n",
      "[Text=, CharacterOffsetBegin=59 CharacterOffsetEnd=60 PartOfSpeech=, Lemma=,]\n",
      "[Text=PAM CharacterOffsetBegin=61 CharacterOffsetEnd=64 PartOfSpeech=NNS Lemma=pam]\n",
      "[Text=-LRB- CharacterOffsetBegin=65 CharacterOffsetEnd=66 PartOfSpeech=-LRB- Lemma=-lrb-]\n",
      "[Text=Wilensky CharacterOffsetBegin=66 CharacterOffsetEnd=74 PartOfSpeech=NNP Lemma=Wilensky]\n",
      "[Text=, CharacterOffsetBegin=74 CharacterOffsetEnd=75 PartOfSpeech=, Lemma=,]\n",
      "[Text=1978 CharacterOffsetBegin=76 CharacterOffsetEnd=80 PartOfSpeech=CD Lemma=1978]\n",
      "[Text=-RRB- CharacterOffsetBegin=80 CharacterOffsetEnd=81 PartOfSpeech=-RRB- Lemma=-rrb-]\n",
      "[Text=, CharacterOffsetBegin=81 CharacterOffsetEnd=82 PartOfSpeech=, Lemma=,]\n",
      "[Text=TaleSpin CharacterOffsetBegin=83 CharacterOffsetEnd=91 PartOfSpeech=NNP Lemma=TaleSpin]\n",
      "[Text=-LRB- CharacterOffsetBegin=92 CharacterOffsetEnd=93 PartOfSpeech=-LRB- Lemma=-lrb-]\n",
      "[Text=Meehan CharacterOffsetBegin=93 CharacterOffsetEnd=99 PartOfSpeech=NNP Lemma=Meehan]\n",
      "[Text=, CharacterOffsetBegin=99 CharacterOffsetEnd=100 PartOfSpeech=, Lemma=,]\n",
      "[Text=1976 CharacterOffsetBegin=101 CharacterOffsetEnd=105 PartOfSpeech=CD Lemma=1976]\n",
      "[Text=-RRB- CharacterOffsetBegin=105 CharacterOffsetEnd=106 PartOfSpeech=-RRB- Lemma=-rrb-]\n",
      "[Text=, CharacterOffsetBegin=106 CharacterOffsetEnd=107 PartOfSpeech=, Lemma=,]\n",
      "[Text=QUALM CharacterOffsetBegin=108 CharacterOffsetEnd=113 PartOfSpeech=NN Lemma=qualm]\n",
      "[Text=-LRB- CharacterOffsetBegin=114 CharacterOffsetEnd=115 PartOfSpeech=-LRB- Lemma=-lrb-]\n",
      "[Text=Lehnert CharacterOffsetBegin=115 CharacterOffsetEnd=122 PartOfSpeech=NNP Lemma=Lehnert]\n",
      "[Text=, CharacterOffsetBegin=122 CharacterOffsetEnd=123 PartOfSpeech=, Lemma=,]\n",
      "[Text=1977 CharacterOffsetBegin=124 CharacterOffsetEnd=128 PartOfSpeech=CD Lemma=1977]\n",
      "[Text=-RRB- CharacterOffsetBegin=128 CharacterOffsetEnd=129 PartOfSpeech=-RRB- Lemma=-rrb-]\n",
      "[Text=, CharacterOffsetBegin=129 CharacterOffsetEnd=130 PartOfSpeech=, Lemma=,]\n",
      "[Text=Politics CharacterOffsetBegin=131 CharacterOffsetEnd=139 PartOfSpeech=NN Lemma=politics]\n",
      "[Text=-LRB- CharacterOffsetBegin=140 CharacterOffsetEnd=141 PartOfSpeech=-LRB- Lemma=-lrb-]\n",
      "[Text=Carbonell CharacterOffsetBegin=141 CharacterOffsetEnd=150 PartOfSpeech=NNP Lemma=Carbonell]\n",
      "[Text=, CharacterOffsetBegin=150 CharacterOffsetEnd=151 PartOfSpeech=, Lemma=,]\n",
      "[Text=1979 CharacterOffsetBegin=152 CharacterOffsetEnd=156 PartOfSpeech=CD Lemma=1979]\n",
      "[Text=-RRB- CharacterOffsetBegin=156 CharacterOffsetEnd=157 PartOfSpeech=-RRB- Lemma=-rrb-]\n",
      "[Text=, CharacterOffsetBegin=157 CharacterOffsetEnd=158 PartOfSpeech=, Lemma=,]\n",
      "[Text=and CharacterOffsetBegin=159 CharacterOffsetEnd=162 PartOfSpeech=CC Lemma=and]\n",
      "[Text=Plot CharacterOffsetBegin=163 CharacterOffsetEnd=167 PartOfSpeech=NN Lemma=plot]\n",
      "[Text=Units CharacterOffsetBegin=168 CharacterOffsetEnd=173 PartOfSpeech=NNS Lemma=unit]\n",
      "[Text=-LRB- CharacterOffsetBegin=174 CharacterOffsetEnd=175 PartOfSpeech=-LRB- Lemma=-lrb-]\n",
      "[Text=Lehnert CharacterOffsetBegin=175 CharacterOffsetEnd=182 PartOfSpeech=NNP Lemma=Lehnert]\n",
      "[Text=1981 CharacterOffsetBegin=183 CharacterOffsetEnd=187 PartOfSpeech=CD Lemma=1981]\n",
      "[Text=-RRB- CharacterOffsetBegin=187 CharacterOffsetEnd=188 PartOfSpeech=-RRB- Lemma=-rrb-]\n",
      "[Text=. CharacterOffsetBegin=188 CharacterOffsetEnd=189 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (16 tokens):\n",
      "During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.\n",
      "\n",
      "Tokens:\n",
      "[Text=During CharacterOffsetBegin=0 CharacterOffsetEnd=6 PartOfSpeech=IN Lemma=during]\n",
      "[Text=this CharacterOffsetBegin=7 CharacterOffsetEnd=11 PartOfSpeech=DT Lemma=this]\n",
      "[Text=time CharacterOffsetBegin=12 CharacterOffsetEnd=16 PartOfSpeech=NN Lemma=time]\n",
      "[Text=, CharacterOffsetBegin=16 CharacterOffsetEnd=17 PartOfSpeech=, Lemma=,]\n",
      "[Text=many CharacterOffsetBegin=18 CharacterOffsetEnd=22 PartOfSpeech=JJ Lemma=many]\n",
      "[Text=chatterbots CharacterOffsetBegin=23 CharacterOffsetEnd=34 PartOfSpeech=NNS Lemma=chatterbot]\n",
      "[Text=were CharacterOffsetBegin=35 CharacterOffsetEnd=39 PartOfSpeech=VBD Lemma=be]\n",
      "[Text=written CharacterOffsetBegin=40 CharacterOffsetEnd=47 PartOfSpeech=VBN Lemma=write]\n",
      "[Text=including CharacterOffsetBegin=48 CharacterOffsetEnd=57 PartOfSpeech=VBG Lemma=include]\n",
      "[Text=PARRY CharacterOffsetBegin=58 CharacterOffsetEnd=63 PartOfSpeech=NNP Lemma=PARRY]\n",
      "[Text=, CharacterOffsetBegin=63 CharacterOffsetEnd=64 PartOfSpeech=, Lemma=,]\n",
      "[Text=Racter CharacterOffsetBegin=65 CharacterOffsetEnd=71 PartOfSpeech=NNP Lemma=Racter]\n",
      "[Text=, CharacterOffsetBegin=71 CharacterOffsetEnd=72 PartOfSpeech=, Lemma=,]\n",
      "[Text=and CharacterOffsetBegin=73 CharacterOffsetEnd=76 PartOfSpeech=CC Lemma=and]\n",
      "[Text=Jabberwacky CharacterOffsetBegin=77 CharacterOffsetEnd=88 PartOfSpeech=NNP Lemma=Jabberwacky]\n",
      "[Text=. CharacterOffsetBegin=88 CharacterOffsetEnd=89 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (17 tokens):\n",
      "Up to the 1980s, most NLP systems were based on complex sets of hand-written rules.\n",
      "\n",
      "Tokens:\n",
      "[Text=Up CharacterOffsetBegin=0 CharacterOffsetEnd=2 PartOfSpeech=IN Lemma=up]\n",
      "[Text=to CharacterOffsetBegin=3 CharacterOffsetEnd=5 PartOfSpeech=TO Lemma=to]\n",
      "[Text=the CharacterOffsetBegin=6 CharacterOffsetEnd=9 PartOfSpeech=DT Lemma=the]\n",
      "[Text=1980s CharacterOffsetBegin=10 CharacterOffsetEnd=15 PartOfSpeech=NNS Lemma=1980]\n",
      "[Text=, CharacterOffsetBegin=15 CharacterOffsetEnd=16 PartOfSpeech=, Lemma=,]\n",
      "[Text=most CharacterOffsetBegin=17 CharacterOffsetEnd=21 PartOfSpeech=JJS Lemma=most]\n",
      "[Text=NLP CharacterOffsetBegin=22 CharacterOffsetEnd=25 PartOfSpeech=NNS Lemma=nlp]\n",
      "[Text=systems CharacterOffsetBegin=26 CharacterOffsetEnd=33 PartOfSpeech=NNS Lemma=system]\n",
      "[Text=were CharacterOffsetBegin=34 CharacterOffsetEnd=38 PartOfSpeech=VBD Lemma=be]\n",
      "[Text=based CharacterOffsetBegin=39 CharacterOffsetEnd=44 PartOfSpeech=VBN Lemma=base]\n",
      "[Text=on CharacterOffsetBegin=45 CharacterOffsetEnd=47 PartOfSpeech=IN Lemma=on]\n",
      "[Text=complex CharacterOffsetBegin=48 CharacterOffsetEnd=55 PartOfSpeech=JJ Lemma=complex]\n",
      "[Text=sets CharacterOffsetBegin=56 CharacterOffsetEnd=60 PartOfSpeech=NNS Lemma=set]\n",
      "[Text=of CharacterOffsetBegin=61 CharacterOffsetEnd=63 PartOfSpeech=IN Lemma=of]\n",
      "[Text=hand-written CharacterOffsetBegin=64 CharacterOffsetEnd=76 PartOfSpeech=JJ Lemma=hand-written]\n",
      "[Text=rules CharacterOffsetBegin=77 CharacterOffsetEnd=82 PartOfSpeech=NNS Lemma=rule]\n",
      "[Text=. CharacterOffsetBegin=82 CharacterOffsetEnd=83 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (25 tokens):\n",
      "Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing.\n",
      "\n",
      "Tokens:\n",
      "[Text=Starting CharacterOffsetBegin=0 CharacterOffsetEnd=8 PartOfSpeech=VBG Lemma=start]\n",
      "[Text=in CharacterOffsetBegin=9 CharacterOffsetEnd=11 PartOfSpeech=IN Lemma=in]\n",
      "[Text=the CharacterOffsetBegin=12 CharacterOffsetEnd=15 PartOfSpeech=DT Lemma=the]\n",
      "[Text=late CharacterOffsetBegin=16 CharacterOffsetEnd=20 PartOfSpeech=JJ Lemma=late]\n",
      "[Text=1980s CharacterOffsetBegin=21 CharacterOffsetEnd=26 PartOfSpeech=NNS Lemma=1980]\n",
      "[Text=, CharacterOffsetBegin=26 CharacterOffsetEnd=27 PartOfSpeech=, Lemma=,]\n",
      "[Text=however CharacterOffsetBegin=28 CharacterOffsetEnd=35 PartOfSpeech=RB Lemma=however]\n",
      "[Text=, CharacterOffsetBegin=35 CharacterOffsetEnd=36 PartOfSpeech=, Lemma=,]\n",
      "[Text=there CharacterOffsetBegin=37 CharacterOffsetEnd=42 PartOfSpeech=EX Lemma=there]\n",
      "[Text=was CharacterOffsetBegin=43 CharacterOffsetEnd=46 PartOfSpeech=VBD Lemma=be]\n",
      "[Text=a CharacterOffsetBegin=47 CharacterOffsetEnd=48 PartOfSpeech=DT Lemma=a]\n",
      "[Text=revolution CharacterOffsetBegin=49 CharacterOffsetEnd=59 PartOfSpeech=NN Lemma=revolution]\n",
      "[Text=in CharacterOffsetBegin=60 CharacterOffsetEnd=62 PartOfSpeech=IN Lemma=in]\n",
      "[Text=NLP CharacterOffsetBegin=63 CharacterOffsetEnd=66 PartOfSpeech=NN Lemma=nlp]\n",
      "[Text=with CharacterOffsetBegin=67 CharacterOffsetEnd=71 PartOfSpeech=IN Lemma=with]\n",
      "[Text=the CharacterOffsetBegin=72 CharacterOffsetEnd=75 PartOfSpeech=DT Lemma=the]\n",
      "[Text=introduction CharacterOffsetBegin=76 CharacterOffsetEnd=88 PartOfSpeech=NN Lemma=introduction]\n",
      "[Text=of CharacterOffsetBegin=89 CharacterOffsetEnd=91 PartOfSpeech=IN Lemma=of]\n",
      "[Text=machine CharacterOffsetBegin=92 CharacterOffsetEnd=99 PartOfSpeech=NN Lemma=machine]\n",
      "[Text=learning CharacterOffsetBegin=100 CharacterOffsetEnd=108 PartOfSpeech=NN Lemma=learning]\n",
      "[Text=algorithms CharacterOffsetBegin=109 CharacterOffsetEnd=119 PartOfSpeech=NNS Lemma=algorithm]\n",
      "[Text=for CharacterOffsetBegin=120 CharacterOffsetEnd=123 PartOfSpeech=IN Lemma=for]\n",
      "[Text=language CharacterOffsetBegin=124 CharacterOffsetEnd=132 PartOfSpeech=NN Lemma=language]\n",
      "[Text=processing CharacterOffsetBegin=133 CharacterOffsetEnd=143 PartOfSpeech=NN Lemma=processing]\n",
      "[Text=. CharacterOffsetBegin=143 CharacterOffsetEnd=144 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (52 tokens):\n",
      "This was due to both the steady increase in computational power resulting from Moore's Law and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.\n",
      "\n",
      "Tokens:\n",
      "[Text=This CharacterOffsetBegin=0 CharacterOffsetEnd=4 PartOfSpeech=DT Lemma=this]\n",
      "[Text=was CharacterOffsetBegin=5 CharacterOffsetEnd=8 PartOfSpeech=VBD Lemma=be]\n",
      "[Text=due CharacterOffsetBegin=9 CharacterOffsetEnd=12 PartOfSpeech=JJ Lemma=due]\n",
      "[Text=to CharacterOffsetBegin=13 CharacterOffsetEnd=15 PartOfSpeech=TO Lemma=to]\n",
      "[Text=both CharacterOffsetBegin=16 CharacterOffsetEnd=20 PartOfSpeech=CC Lemma=both]\n",
      "[Text=the CharacterOffsetBegin=21 CharacterOffsetEnd=24 PartOfSpeech=DT Lemma=the]\n",
      "[Text=steady CharacterOffsetBegin=25 CharacterOffsetEnd=31 PartOfSpeech=JJ Lemma=steady]\n",
      "[Text=increase CharacterOffsetBegin=32 CharacterOffsetEnd=40 PartOfSpeech=NN Lemma=increase]\n",
      "[Text=in CharacterOffsetBegin=41 CharacterOffsetEnd=43 PartOfSpeech=IN Lemma=in]\n",
      "[Text=computational CharacterOffsetBegin=44 CharacterOffsetEnd=57 PartOfSpeech=JJ Lemma=computational]\n",
      "[Text=power CharacterOffsetBegin=58 CharacterOffsetEnd=63 PartOfSpeech=NN Lemma=power]\n",
      "[Text=resulting CharacterOffsetBegin=64 CharacterOffsetEnd=73 PartOfSpeech=VBG Lemma=result]\n",
      "[Text=from CharacterOffsetBegin=74 CharacterOffsetEnd=78 PartOfSpeech=IN Lemma=from]\n",
      "[Text=Moore CharacterOffsetBegin=79 CharacterOffsetEnd=84 PartOfSpeech=NNP Lemma=Moore]\n",
      "[Text='s CharacterOffsetBegin=84 CharacterOffsetEnd=86 PartOfSpeech=POS Lemma='s]\n",
      "[Text=Law CharacterOffsetBegin=87 CharacterOffsetEnd=90 PartOfSpeech=NN Lemma=law]\n",
      "[Text=and CharacterOffsetBegin=91 CharacterOffsetEnd=94 PartOfSpeech=CC Lemma=and]\n",
      "[Text=the CharacterOffsetBegin=95 CharacterOffsetEnd=98 PartOfSpeech=DT Lemma=the]\n",
      "[Text=gradual CharacterOffsetBegin=99 CharacterOffsetEnd=106 PartOfSpeech=JJ Lemma=gradual]\n",
      "[Text=lessening CharacterOffsetBegin=107 CharacterOffsetEnd=116 PartOfSpeech=VBG Lemma=lessen]\n",
      "[Text=of CharacterOffsetBegin=117 CharacterOffsetEnd=119 PartOfSpeech=IN Lemma=of]\n",
      "[Text=the CharacterOffsetBegin=120 CharacterOffsetEnd=123 PartOfSpeech=DT Lemma=the]\n",
      "[Text=dominance CharacterOffsetBegin=124 CharacterOffsetEnd=133 PartOfSpeech=NN Lemma=dominance]\n",
      "[Text=of CharacterOffsetBegin=134 CharacterOffsetEnd=136 PartOfSpeech=IN Lemma=of]\n",
      "[Text=Chomskyan CharacterOffsetBegin=137 CharacterOffsetEnd=146 PartOfSpeech=NNP Lemma=Chomskyan]\n",
      "[Text=theories CharacterOffsetBegin=147 CharacterOffsetEnd=155 PartOfSpeech=NNS Lemma=theory]\n",
      "[Text=of CharacterOffsetBegin=156 CharacterOffsetEnd=158 PartOfSpeech=IN Lemma=of]\n",
      "[Text=linguistics CharacterOffsetBegin=159 CharacterOffsetEnd=170 PartOfSpeech=NNS Lemma=linguistics]\n",
      "[Text=-LRB- CharacterOffsetBegin=171 CharacterOffsetEnd=172 PartOfSpeech=-LRB- Lemma=-lrb-]\n",
      "[Text=e.g. CharacterOffsetBegin=172 CharacterOffsetEnd=176 PartOfSpeech=FW Lemma=e.g.]\n",
      "[Text=transformational CharacterOffsetBegin=177 CharacterOffsetEnd=193 PartOfSpeech=JJ Lemma=transformational]\n",
      "[Text=grammar CharacterOffsetBegin=194 CharacterOffsetEnd=201 PartOfSpeech=NN Lemma=grammar]\n",
      "[Text=-RRB- CharacterOffsetBegin=201 CharacterOffsetEnd=202 PartOfSpeech=-RRB- Lemma=-rrb-]\n",
      "[Text=, CharacterOffsetBegin=202 CharacterOffsetEnd=203 PartOfSpeech=, Lemma=,]\n",
      "[Text=whose CharacterOffsetBegin=204 CharacterOffsetEnd=209 PartOfSpeech=WP$ Lemma=whose]\n",
      "[Text=theoretical CharacterOffsetBegin=210 CharacterOffsetEnd=221 PartOfSpeech=JJ Lemma=theoretical]\n",
      "[Text=underpinnings CharacterOffsetBegin=222 CharacterOffsetEnd=235 PartOfSpeech=NNS Lemma=underpinning]\n",
      "[Text=discouraged CharacterOffsetBegin=236 CharacterOffsetEnd=247 PartOfSpeech=VBD Lemma=discourage]\n",
      "[Text=the CharacterOffsetBegin=248 CharacterOffsetEnd=251 PartOfSpeech=DT Lemma=the]\n",
      "[Text=sort CharacterOffsetBegin=252 CharacterOffsetEnd=256 PartOfSpeech=NN Lemma=sort]\n",
      "[Text=of CharacterOffsetBegin=257 CharacterOffsetEnd=259 PartOfSpeech=IN Lemma=of]\n",
      "[Text=corpus CharacterOffsetBegin=260 CharacterOffsetEnd=266 PartOfSpeech=NN Lemma=corpus]\n",
      "[Text=linguistics CharacterOffsetBegin=267 CharacterOffsetEnd=278 PartOfSpeech=NNS Lemma=linguistics]\n",
      "[Text=that CharacterOffsetBegin=279 CharacterOffsetEnd=283 PartOfSpeech=WDT Lemma=that]\n",
      "[Text=underlies CharacterOffsetBegin=284 CharacterOffsetEnd=293 PartOfSpeech=VBZ Lemma=underlie]\n",
      "[Text=the CharacterOffsetBegin=294 CharacterOffsetEnd=297 PartOfSpeech=DT Lemma=the]\n",
      "[Text=machine-learning CharacterOffsetBegin=298 CharacterOffsetEnd=314 PartOfSpeech=JJ Lemma=machine-learning]\n",
      "[Text=approach CharacterOffsetBegin=315 CharacterOffsetEnd=323 PartOfSpeech=NN Lemma=approach]\n",
      "[Text=to CharacterOffsetBegin=324 CharacterOffsetEnd=326 PartOfSpeech=TO Lemma=to]\n",
      "[Text=language CharacterOffsetBegin=327 CharacterOffsetEnd=335 PartOfSpeech=NN Lemma=language]\n",
      "[Text=processing CharacterOffsetBegin=336 CharacterOffsetEnd=346 PartOfSpeech=NN Lemma=processing]\n",
      "[Text=. CharacterOffsetBegin=346 CharacterOffsetEnd=347 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (25 tokens):\n",
      "Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules.\n",
      "\n",
      "Tokens:\n",
      "[Text=Some CharacterOffsetBegin=0 CharacterOffsetEnd=4 PartOfSpeech=DT Lemma=some]\n",
      "[Text=of CharacterOffsetBegin=5 CharacterOffsetEnd=7 PartOfSpeech=IN Lemma=of]\n",
      "[Text=the CharacterOffsetBegin=8 CharacterOffsetEnd=11 PartOfSpeech=DT Lemma=the]\n",
      "[Text=earliest-used CharacterOffsetBegin=12 CharacterOffsetEnd=25 PartOfSpeech=JJ Lemma=earliest-used]\n",
      "[Text=machine CharacterOffsetBegin=26 CharacterOffsetEnd=33 PartOfSpeech=NN Lemma=machine]\n",
      "[Text=learning CharacterOffsetBegin=34 CharacterOffsetEnd=42 PartOfSpeech=VBG Lemma=learn]\n",
      "[Text=algorithms CharacterOffsetBegin=43 CharacterOffsetEnd=53 PartOfSpeech=NNS Lemma=algorithm]\n",
      "[Text=, CharacterOffsetBegin=53 CharacterOffsetEnd=54 PartOfSpeech=, Lemma=,]\n",
      "[Text=such CharacterOffsetBegin=55 CharacterOffsetEnd=59 PartOfSpeech=JJ Lemma=such]\n",
      "[Text=as CharacterOffsetBegin=60 CharacterOffsetEnd=62 PartOfSpeech=IN Lemma=as]\n",
      "[Text=decision CharacterOffsetBegin=63 CharacterOffsetEnd=71 PartOfSpeech=NN Lemma=decision]\n",
      "[Text=trees CharacterOffsetBegin=72 CharacterOffsetEnd=77 PartOfSpeech=NNS Lemma=tree]\n",
      "[Text=, CharacterOffsetBegin=77 CharacterOffsetEnd=78 PartOfSpeech=, Lemma=,]\n",
      "[Text=produced CharacterOffsetBegin=79 CharacterOffsetEnd=87 PartOfSpeech=VBD Lemma=produce]\n",
      "[Text=systems CharacterOffsetBegin=88 CharacterOffsetEnd=95 PartOfSpeech=NNS Lemma=system]\n",
      "[Text=of CharacterOffsetBegin=96 CharacterOffsetEnd=98 PartOfSpeech=IN Lemma=of]\n",
      "[Text=hard CharacterOffsetBegin=99 CharacterOffsetEnd=103 PartOfSpeech=JJ Lemma=hard]\n",
      "[Text=if-then CharacterOffsetBegin=104 CharacterOffsetEnd=111 PartOfSpeech=JJ Lemma=if-then]\n",
      "[Text=rules CharacterOffsetBegin=112 CharacterOffsetEnd=117 PartOfSpeech=NNS Lemma=rule]\n",
      "[Text=similar CharacterOffsetBegin=118 CharacterOffsetEnd=125 PartOfSpeech=JJ Lemma=similar]\n",
      "[Text=to CharacterOffsetBegin=126 CharacterOffsetEnd=128 PartOfSpeech=TO Lemma=to]\n",
      "[Text=existing CharacterOffsetBegin=129 CharacterOffsetEnd=137 PartOfSpeech=VBG Lemma=exist]\n",
      "[Text=hand-written CharacterOffsetBegin=138 CharacterOffsetEnd=150 PartOfSpeech=JJ Lemma=hand-written]\n",
      "[Text=rules CharacterOffsetBegin=151 CharacterOffsetEnd=156 PartOfSpeech=NNS Lemma=rule]\n",
      "[Text=. CharacterOffsetBegin=156 CharacterOffsetEnd=157 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (46 tokens):\n",
      "However, Part of speech tagging introduced the use of Hidden Markov Models to NLP, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data.\n",
      "\n",
      "Tokens:\n",
      "[Text=However CharacterOffsetBegin=0 CharacterOffsetEnd=7 PartOfSpeech=RB Lemma=however]\n",
      "[Text=, CharacterOffsetBegin=7 CharacterOffsetEnd=8 PartOfSpeech=, Lemma=,]\n",
      "[Text=Part CharacterOffsetBegin=9 CharacterOffsetEnd=13 PartOfSpeech=NN Lemma=part]\n",
      "[Text=of CharacterOffsetBegin=14 CharacterOffsetEnd=16 PartOfSpeech=IN Lemma=of]\n",
      "[Text=speech CharacterOffsetBegin=17 CharacterOffsetEnd=23 PartOfSpeech=NN Lemma=speech]\n",
      "[Text=tagging CharacterOffsetBegin=24 CharacterOffsetEnd=31 PartOfSpeech=VBG Lemma=tag]\n",
      "[Text=introduced CharacterOffsetBegin=32 CharacterOffsetEnd=42 PartOfSpeech=VBN Lemma=introduce]\n",
      "[Text=the CharacterOffsetBegin=43 CharacterOffsetEnd=46 PartOfSpeech=DT Lemma=the]\n",
      "[Text=use CharacterOffsetBegin=47 CharacterOffsetEnd=50 PartOfSpeech=NN Lemma=use]\n",
      "[Text=of CharacterOffsetBegin=51 CharacterOffsetEnd=53 PartOfSpeech=IN Lemma=of]\n",
      "[Text=Hidden CharacterOffsetBegin=54 CharacterOffsetEnd=60 PartOfSpeech=NNP Lemma=Hidden]\n",
      "[Text=Markov CharacterOffsetBegin=61 CharacterOffsetEnd=67 PartOfSpeech=NNP Lemma=Markov]\n",
      "[Text=Models CharacterOffsetBegin=68 CharacterOffsetEnd=74 PartOfSpeech=NNS Lemma=model]\n",
      "[Text=to CharacterOffsetBegin=75 CharacterOffsetEnd=77 PartOfSpeech=TO Lemma=to]\n",
      "[Text=NLP CharacterOffsetBegin=78 CharacterOffsetEnd=81 PartOfSpeech=NNP Lemma=NLP]\n",
      "[Text=, CharacterOffsetBegin=81 CharacterOffsetEnd=82 PartOfSpeech=, Lemma=,]\n",
      "[Text=and CharacterOffsetBegin=83 CharacterOffsetEnd=86 PartOfSpeech=CC Lemma=and]\n",
      "[Text=increasingly CharacterOffsetBegin=87 CharacterOffsetEnd=99 PartOfSpeech=RB Lemma=increasingly]\n",
      "[Text=, CharacterOffsetBegin=99 CharacterOffsetEnd=100 PartOfSpeech=, Lemma=,]\n",
      "[Text=research CharacterOffsetBegin=101 CharacterOffsetEnd=109 PartOfSpeech=NN Lemma=research]\n",
      "[Text=has CharacterOffsetBegin=110 CharacterOffsetEnd=113 PartOfSpeech=VBZ Lemma=have]\n",
      "[Text=focused CharacterOffsetBegin=114 CharacterOffsetEnd=121 PartOfSpeech=VBN Lemma=focus]\n",
      "[Text=on CharacterOffsetBegin=122 CharacterOffsetEnd=124 PartOfSpeech=IN Lemma=on]\n",
      "[Text=statistical CharacterOffsetBegin=125 CharacterOffsetEnd=136 PartOfSpeech=JJ Lemma=statistical]\n",
      "[Text=models CharacterOffsetBegin=137 CharacterOffsetEnd=143 PartOfSpeech=NNS Lemma=model]\n",
      "[Text=, CharacterOffsetBegin=143 CharacterOffsetEnd=144 PartOfSpeech=, Lemma=,]\n",
      "[Text=which CharacterOffsetBegin=145 CharacterOffsetEnd=150 PartOfSpeech=WDT Lemma=which]\n",
      "[Text=make CharacterOffsetBegin=151 CharacterOffsetEnd=155 PartOfSpeech=VBP Lemma=make]\n",
      "[Text=soft CharacterOffsetBegin=156 CharacterOffsetEnd=160 PartOfSpeech=JJ Lemma=soft]\n",
      "[Text=, CharacterOffsetBegin=160 CharacterOffsetEnd=161 PartOfSpeech=, Lemma=,]\n",
      "[Text=probabilistic CharacterOffsetBegin=162 CharacterOffsetEnd=175 PartOfSpeech=JJ Lemma=probabilistic]\n",
      "[Text=decisions CharacterOffsetBegin=176 CharacterOffsetEnd=185 PartOfSpeech=NNS Lemma=decision]\n",
      "[Text=based CharacterOffsetBegin=186 CharacterOffsetEnd=191 PartOfSpeech=VBN Lemma=base]\n",
      "[Text=on CharacterOffsetBegin=192 CharacterOffsetEnd=194 PartOfSpeech=IN Lemma=on]\n",
      "[Text=attaching CharacterOffsetBegin=195 CharacterOffsetEnd=204 PartOfSpeech=VBG Lemma=attach]\n",
      "[Text=real-valued CharacterOffsetBegin=205 CharacterOffsetEnd=216 PartOfSpeech=JJ Lemma=real-valued]\n",
      "[Text=weights CharacterOffsetBegin=217 CharacterOffsetEnd=224 PartOfSpeech=NNS Lemma=weight]\n",
      "[Text=to CharacterOffsetBegin=225 CharacterOffsetEnd=227 PartOfSpeech=TO Lemma=to]\n",
      "[Text=the CharacterOffsetBegin=228 CharacterOffsetEnd=231 PartOfSpeech=DT Lemma=the]\n",
      "[Text=features CharacterOffsetBegin=232 CharacterOffsetEnd=240 PartOfSpeech=NNS Lemma=feature]\n",
      "[Text=making CharacterOffsetBegin=241 CharacterOffsetEnd=247 PartOfSpeech=VBG Lemma=make]\n",
      "[Text=up CharacterOffsetBegin=248 CharacterOffsetEnd=250 PartOfSpeech=RP Lemma=up]\n",
      "[Text=the CharacterOffsetBegin=251 CharacterOffsetEnd=254 PartOfSpeech=DT Lemma=the]\n",
      "[Text=input CharacterOffsetBegin=255 CharacterOffsetEnd=260 PartOfSpeech=NN Lemma=input]\n",
      "[Text=data CharacterOffsetBegin=261 CharacterOffsetEnd=265 PartOfSpeech=NNS Lemma=datum]\n",
      "[Text=. CharacterOffsetBegin=265 CharacterOffsetEnd=266 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (19 tokens):\n",
      "The cache language models upon which many speech recognition systems now rely are examples of such statistical models.\n",
      "\n",
      "Tokens:\n",
      "[Text=The CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=DT Lemma=the]\n",
      "[Text=cache CharacterOffsetBegin=4 CharacterOffsetEnd=9 PartOfSpeech=NN Lemma=cache]\n",
      "[Text=language CharacterOffsetBegin=10 CharacterOffsetEnd=18 PartOfSpeech=NN Lemma=language]\n",
      "[Text=models CharacterOffsetBegin=19 CharacterOffsetEnd=25 PartOfSpeech=NNS Lemma=model]\n",
      "[Text=upon CharacterOffsetBegin=26 CharacterOffsetEnd=30 PartOfSpeech=IN Lemma=upon]\n",
      "[Text=which CharacterOffsetBegin=31 CharacterOffsetEnd=36 PartOfSpeech=WDT Lemma=which]\n",
      "[Text=many CharacterOffsetBegin=37 CharacterOffsetEnd=41 PartOfSpeech=JJ Lemma=many]\n",
      "[Text=speech CharacterOffsetBegin=42 CharacterOffsetEnd=48 PartOfSpeech=NN Lemma=speech]\n",
      "[Text=recognition CharacterOffsetBegin=49 CharacterOffsetEnd=60 PartOfSpeech=NN Lemma=recognition]\n",
      "[Text=systems CharacterOffsetBegin=61 CharacterOffsetEnd=68 PartOfSpeech=NNS Lemma=system]\n",
      "[Text=now CharacterOffsetBegin=69 CharacterOffsetEnd=72 PartOfSpeech=RB Lemma=now]\n",
      "[Text=rely CharacterOffsetBegin=73 CharacterOffsetEnd=77 PartOfSpeech=VBP Lemma=rely]\n",
      "[Text=are CharacterOffsetBegin=78 CharacterOffsetEnd=81 PartOfSpeech=VBP Lemma=be]\n",
      "[Text=examples CharacterOffsetBegin=82 CharacterOffsetEnd=90 PartOfSpeech=NNS Lemma=example]\n",
      "[Text=of CharacterOffsetBegin=91 CharacterOffsetEnd=93 PartOfSpeech=IN Lemma=of]\n",
      "[Text=such CharacterOffsetBegin=94 CharacterOffsetEnd=98 PartOfSpeech=JJ Lemma=such]\n",
      "[Text=statistical CharacterOffsetBegin=99 CharacterOffsetEnd=110 PartOfSpeech=JJ Lemma=statistical]\n",
      "[Text=models CharacterOffsetBegin=111 CharacterOffsetEnd=117 PartOfSpeech=NNS Lemma=model]\n",
      "[Text=. CharacterOffsetBegin=117 CharacterOffsetEnd=118 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (41 tokens):\n",
      "Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.\n",
      "\n",
      "Tokens:\n",
      "[Text=Such CharacterOffsetBegin=0 CharacterOffsetEnd=4 PartOfSpeech=JJ Lemma=such]\n",
      "[Text=models CharacterOffsetBegin=5 CharacterOffsetEnd=11 PartOfSpeech=NNS Lemma=model]\n",
      "[Text=are CharacterOffsetBegin=12 CharacterOffsetEnd=15 PartOfSpeech=VBP Lemma=be]\n",
      "[Text=generally CharacterOffsetBegin=16 CharacterOffsetEnd=25 PartOfSpeech=RB Lemma=generally]\n",
      "[Text=more CharacterOffsetBegin=26 CharacterOffsetEnd=30 PartOfSpeech=RBR Lemma=more]\n",
      "[Text=robust CharacterOffsetBegin=31 CharacterOffsetEnd=37 PartOfSpeech=JJ Lemma=robust]\n",
      "[Text=when CharacterOffsetBegin=38 CharacterOffsetEnd=42 PartOfSpeech=WRB Lemma=when]\n",
      "[Text=given CharacterOffsetBegin=43 CharacterOffsetEnd=48 PartOfSpeech=VBN Lemma=give]\n",
      "[Text=unfamiliar CharacterOffsetBegin=49 CharacterOffsetEnd=59 PartOfSpeech=JJ Lemma=unfamiliar]\n",
      "[Text=input CharacterOffsetBegin=60 CharacterOffsetEnd=65 PartOfSpeech=NN Lemma=input]\n",
      "[Text=, CharacterOffsetBegin=65 CharacterOffsetEnd=66 PartOfSpeech=, Lemma=,]\n",
      "[Text=especially CharacterOffsetBegin=67 CharacterOffsetEnd=77 PartOfSpeech=RB Lemma=especially]\n",
      "[Text=input CharacterOffsetBegin=78 CharacterOffsetEnd=83 PartOfSpeech=NN Lemma=input]\n",
      "[Text=that CharacterOffsetBegin=84 CharacterOffsetEnd=88 PartOfSpeech=WDT Lemma=that]\n",
      "[Text=contains CharacterOffsetBegin=89 CharacterOffsetEnd=97 PartOfSpeech=VBZ Lemma=contain]\n",
      "[Text=errors CharacterOffsetBegin=98 CharacterOffsetEnd=104 PartOfSpeech=NNS Lemma=error]\n",
      "[Text=-LRB- CharacterOffsetBegin=105 CharacterOffsetEnd=106 PartOfSpeech=-LRB- Lemma=-lrb-]\n",
      "[Text=as CharacterOffsetBegin=106 CharacterOffsetEnd=108 PartOfSpeech=RB Lemma=as]\n",
      "[Text=is CharacterOffsetBegin=109 CharacterOffsetEnd=111 PartOfSpeech=VBZ Lemma=be]\n",
      "[Text=very CharacterOffsetBegin=112 CharacterOffsetEnd=116 PartOfSpeech=RB Lemma=very]\n",
      "[Text=common CharacterOffsetBegin=117 CharacterOffsetEnd=123 PartOfSpeech=JJ Lemma=common]\n",
      "[Text=for CharacterOffsetBegin=124 CharacterOffsetEnd=127 PartOfSpeech=IN Lemma=for]\n",
      "[Text=real-world CharacterOffsetBegin=128 CharacterOffsetEnd=138 PartOfSpeech=JJ Lemma=real-world]\n",
      "[Text=data CharacterOffsetBegin=139 CharacterOffsetEnd=143 PartOfSpeech=NNS Lemma=datum]\n",
      "[Text=-RRB- CharacterOffsetBegin=143 CharacterOffsetEnd=144 PartOfSpeech=-RRB- Lemma=-rrb-]\n",
      "[Text=, CharacterOffsetBegin=144 CharacterOffsetEnd=145 PartOfSpeech=, Lemma=,]\n",
      "[Text=and CharacterOffsetBegin=146 CharacterOffsetEnd=149 PartOfSpeech=CC Lemma=and]\n",
      "[Text=produce CharacterOffsetBegin=150 CharacterOffsetEnd=157 PartOfSpeech=VB Lemma=produce]\n",
      "[Text=more CharacterOffsetBegin=158 CharacterOffsetEnd=162 PartOfSpeech=JJR Lemma=more]\n",
      "[Text=reliable CharacterOffsetBegin=163 CharacterOffsetEnd=171 PartOfSpeech=JJ Lemma=reliable]\n",
      "[Text=results CharacterOffsetBegin=172 CharacterOffsetEnd=179 PartOfSpeech=NNS Lemma=result]\n",
      "[Text=when CharacterOffsetBegin=180 CharacterOffsetEnd=184 PartOfSpeech=WRB Lemma=when]\n",
      "[Text=integrated CharacterOffsetBegin=185 CharacterOffsetEnd=195 PartOfSpeech=VBN Lemma=integrate]\n",
      "[Text=into CharacterOffsetBegin=196 CharacterOffsetEnd=200 PartOfSpeech=IN Lemma=into]\n",
      "[Text=a CharacterOffsetBegin=201 CharacterOffsetEnd=202 PartOfSpeech=DT Lemma=a]\n",
      "[Text=larger CharacterOffsetBegin=203 CharacterOffsetEnd=209 PartOfSpeech=JJR Lemma=larger]\n",
      "[Text=system CharacterOffsetBegin=210 CharacterOffsetEnd=216 PartOfSpeech=NN Lemma=system]\n",
      "[Text=comprising CharacterOffsetBegin=217 CharacterOffsetEnd=227 PartOfSpeech=VBG Lemma=comprise]\n",
      "[Text=multiple CharacterOffsetBegin=228 CharacterOffsetEnd=236 PartOfSpeech=JJ Lemma=multiple]\n",
      "[Text=subtasks CharacterOffsetBegin=237 CharacterOffsetEnd=245 PartOfSpeech=NNS Lemma=subtask]\n",
      "[Text=. CharacterOffsetBegin=245 CharacterOffsetEnd=246 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (31 tokens):\n",
      "Many of the notable early successes occurred in the field of machine translation, due especially to work at IBM Research, where successively more complicated statistical models were developed.\n",
      "\n",
      "Tokens:\n",
      "[Text=Many CharacterOffsetBegin=0 CharacterOffsetEnd=4 PartOfSpeech=JJ Lemma=many]\n",
      "[Text=of CharacterOffsetBegin=5 CharacterOffsetEnd=7 PartOfSpeech=IN Lemma=of]\n",
      "[Text=the CharacterOffsetBegin=8 CharacterOffsetEnd=11 PartOfSpeech=DT Lemma=the]\n",
      "[Text=notable CharacterOffsetBegin=12 CharacterOffsetEnd=19 PartOfSpeech=JJ Lemma=notable]\n",
      "[Text=early CharacterOffsetBegin=20 CharacterOffsetEnd=25 PartOfSpeech=JJ Lemma=early]\n",
      "[Text=successes CharacterOffsetBegin=26 CharacterOffsetEnd=35 PartOfSpeech=NNS Lemma=success]\n",
      "[Text=occurred CharacterOffsetBegin=36 CharacterOffsetEnd=44 PartOfSpeech=VBD Lemma=occur]\n",
      "[Text=in CharacterOffsetBegin=45 CharacterOffsetEnd=47 PartOfSpeech=IN Lemma=in]\n",
      "[Text=the CharacterOffsetBegin=48 CharacterOffsetEnd=51 PartOfSpeech=DT Lemma=the]\n",
      "[Text=field CharacterOffsetBegin=52 CharacterOffsetEnd=57 PartOfSpeech=NN Lemma=field]\n",
      "[Text=of CharacterOffsetBegin=58 CharacterOffsetEnd=60 PartOfSpeech=IN Lemma=of]\n",
      "[Text=machine CharacterOffsetBegin=61 CharacterOffsetEnd=68 PartOfSpeech=NN Lemma=machine]\n",
      "[Text=translation CharacterOffsetBegin=69 CharacterOffsetEnd=80 PartOfSpeech=NN Lemma=translation]\n",
      "[Text=, CharacterOffsetBegin=80 CharacterOffsetEnd=81 PartOfSpeech=, Lemma=,]\n",
      "[Text=due CharacterOffsetBegin=82 CharacterOffsetEnd=85 PartOfSpeech=JJ Lemma=due]\n",
      "[Text=especially CharacterOffsetBegin=86 CharacterOffsetEnd=96 PartOfSpeech=RB Lemma=especially]\n",
      "[Text=to CharacterOffsetBegin=97 CharacterOffsetEnd=99 PartOfSpeech=TO Lemma=to]\n",
      "[Text=work CharacterOffsetBegin=100 CharacterOffsetEnd=104 PartOfSpeech=VB Lemma=work]\n",
      "[Text=at CharacterOffsetBegin=105 CharacterOffsetEnd=107 PartOfSpeech=IN Lemma=at]\n",
      "[Text=IBM CharacterOffsetBegin=108 CharacterOffsetEnd=111 PartOfSpeech=NNP Lemma=IBM]\n",
      "[Text=Research CharacterOffsetBegin=112 CharacterOffsetEnd=120 PartOfSpeech=NNP Lemma=Research]\n",
      "[Text=, CharacterOffsetBegin=120 CharacterOffsetEnd=121 PartOfSpeech=, Lemma=,]\n",
      "[Text=where CharacterOffsetBegin=122 CharacterOffsetEnd=127 PartOfSpeech=WRB Lemma=where]\n",
      "[Text=successively CharacterOffsetBegin=128 CharacterOffsetEnd=140 PartOfSpeech=RB Lemma=successively]\n",
      "[Text=more CharacterOffsetBegin=141 CharacterOffsetEnd=145 PartOfSpeech=RBR Lemma=more]\n",
      "[Text=complicated CharacterOffsetBegin=146 CharacterOffsetEnd=157 PartOfSpeech=JJ Lemma=complicated]\n",
      "[Text=statistical CharacterOffsetBegin=158 CharacterOffsetEnd=169 PartOfSpeech=JJ Lemma=statistical]\n",
      "[Text=models CharacterOffsetBegin=170 CharacterOffsetEnd=176 PartOfSpeech=NNS Lemma=model]\n",
      "[Text=were CharacterOffsetBegin=177 CharacterOffsetEnd=181 PartOfSpeech=VBD Lemma=be]\n",
      "[Text=developed CharacterOffsetBegin=182 CharacterOffsetEnd=191 PartOfSpeech=VBN Lemma=develop]\n",
      "[Text=. CharacterOffsetBegin=191 CharacterOffsetEnd=192 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (49 tokens):\n",
      "These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government.\n",
      "\n",
      "Tokens:\n",
      "[Text=These CharacterOffsetBegin=0 CharacterOffsetEnd=5 PartOfSpeech=DT Lemma=these]\n",
      "[Text=systems CharacterOffsetBegin=6 CharacterOffsetEnd=13 PartOfSpeech=NNS Lemma=system]\n",
      "[Text=were CharacterOffsetBegin=14 CharacterOffsetEnd=18 PartOfSpeech=VBD Lemma=be]\n",
      "[Text=able CharacterOffsetBegin=19 CharacterOffsetEnd=23 PartOfSpeech=JJ Lemma=able]\n",
      "[Text=to CharacterOffsetBegin=24 CharacterOffsetEnd=26 PartOfSpeech=TO Lemma=to]\n",
      "[Text=take CharacterOffsetBegin=27 CharacterOffsetEnd=31 PartOfSpeech=VB Lemma=take]\n",
      "[Text=advantage CharacterOffsetBegin=32 CharacterOffsetEnd=41 PartOfSpeech=NN Lemma=advantage]\n",
      "[Text=of CharacterOffsetBegin=42 CharacterOffsetEnd=44 PartOfSpeech=IN Lemma=of]\n",
      "[Text=existing CharacterOffsetBegin=45 CharacterOffsetEnd=53 PartOfSpeech=VBG Lemma=exist]\n",
      "[Text=multilingual CharacterOffsetBegin=54 CharacterOffsetEnd=66 PartOfSpeech=JJ Lemma=multilingual]\n",
      "[Text=textual CharacterOffsetBegin=67 CharacterOffsetEnd=74 PartOfSpeech=JJ Lemma=textual]\n",
      "[Text=corpora CharacterOffsetBegin=75 CharacterOffsetEnd=82 PartOfSpeech=NN Lemma=corpora]\n",
      "[Text=that CharacterOffsetBegin=83 CharacterOffsetEnd=87 PartOfSpeech=WDT Lemma=that]\n",
      "[Text=had CharacterOffsetBegin=88 CharacterOffsetEnd=91 PartOfSpeech=VBD Lemma=have]\n",
      "[Text=been CharacterOffsetBegin=92 CharacterOffsetEnd=96 PartOfSpeech=VBN Lemma=be]\n",
      "[Text=produced CharacterOffsetBegin=97 CharacterOffsetEnd=105 PartOfSpeech=VBN Lemma=produce]\n",
      "[Text=by CharacterOffsetBegin=106 CharacterOffsetEnd=108 PartOfSpeech=IN Lemma=by]\n",
      "[Text=the CharacterOffsetBegin=109 CharacterOffsetEnd=112 PartOfSpeech=DT Lemma=the]\n",
      "[Text=Parliament CharacterOffsetBegin=113 CharacterOffsetEnd=123 PartOfSpeech=NNP Lemma=Parliament]\n",
      "[Text=of CharacterOffsetBegin=124 CharacterOffsetEnd=126 PartOfSpeech=IN Lemma=of]\n",
      "[Text=Canada CharacterOffsetBegin=127 CharacterOffsetEnd=133 PartOfSpeech=NNP Lemma=Canada]\n",
      "[Text=and CharacterOffsetBegin=134 CharacterOffsetEnd=137 PartOfSpeech=CC Lemma=and]\n",
      "[Text=the CharacterOffsetBegin=138 CharacterOffsetEnd=141 PartOfSpeech=DT Lemma=the]\n",
      "[Text=European CharacterOffsetBegin=142 CharacterOffsetEnd=150 PartOfSpeech=NNP Lemma=European]\n",
      "[Text=Union CharacterOffsetBegin=151 CharacterOffsetEnd=156 PartOfSpeech=NNP Lemma=Union]\n",
      "[Text=as CharacterOffsetBegin=157 CharacterOffsetEnd=159 PartOfSpeech=IN Lemma=as]\n",
      "[Text=a CharacterOffsetBegin=160 CharacterOffsetEnd=161 PartOfSpeech=DT Lemma=a]\n",
      "[Text=result CharacterOffsetBegin=162 CharacterOffsetEnd=168 PartOfSpeech=NN Lemma=result]\n",
      "[Text=of CharacterOffsetBegin=169 CharacterOffsetEnd=171 PartOfSpeech=IN Lemma=of]\n",
      "[Text=laws CharacterOffsetBegin=172 CharacterOffsetEnd=176 PartOfSpeech=NNS Lemma=law]\n",
      "[Text=calling CharacterOffsetBegin=177 CharacterOffsetEnd=184 PartOfSpeech=VBG Lemma=call]\n",
      "[Text=for CharacterOffsetBegin=185 CharacterOffsetEnd=188 PartOfSpeech=IN Lemma=for]\n",
      "[Text=the CharacterOffsetBegin=189 CharacterOffsetEnd=192 PartOfSpeech=DT Lemma=the]\n",
      "[Text=translation CharacterOffsetBegin=193 CharacterOffsetEnd=204 PartOfSpeech=NN Lemma=translation]\n",
      "[Text=of CharacterOffsetBegin=205 CharacterOffsetEnd=207 PartOfSpeech=IN Lemma=of]\n",
      "[Text=all CharacterOffsetBegin=208 CharacterOffsetEnd=211 PartOfSpeech=DT Lemma=all]\n",
      "[Text=governmental CharacterOffsetBegin=212 CharacterOffsetEnd=224 PartOfSpeech=JJ Lemma=governmental]\n",
      "[Text=proceedings CharacterOffsetBegin=225 CharacterOffsetEnd=236 PartOfSpeech=NNS Lemma=proceedings]\n",
      "[Text=into CharacterOffsetBegin=237 CharacterOffsetEnd=241 PartOfSpeech=IN Lemma=into]\n",
      "[Text=all CharacterOffsetBegin=242 CharacterOffsetEnd=245 PartOfSpeech=DT Lemma=all]\n",
      "[Text=official CharacterOffsetBegin=246 CharacterOffsetEnd=254 PartOfSpeech=JJ Lemma=official]\n",
      "[Text=languages CharacterOffsetBegin=255 CharacterOffsetEnd=264 PartOfSpeech=NNS Lemma=language]\n",
      "[Text=of CharacterOffsetBegin=265 CharacterOffsetEnd=267 PartOfSpeech=IN Lemma=of]\n",
      "[Text=the CharacterOffsetBegin=268 CharacterOffsetEnd=271 PartOfSpeech=DT Lemma=the]\n",
      "[Text=corresponding CharacterOffsetBegin=272 CharacterOffsetEnd=285 PartOfSpeech=JJ Lemma=corresponding]\n",
      "[Text=systems CharacterOffsetBegin=286 CharacterOffsetEnd=293 PartOfSpeech=NNS Lemma=system]\n",
      "[Text=of CharacterOffsetBegin=294 CharacterOffsetEnd=296 PartOfSpeech=IN Lemma=of]\n",
      "[Text=government CharacterOffsetBegin=297 CharacterOffsetEnd=307 PartOfSpeech=NN Lemma=government]\n",
      "[Text=. CharacterOffsetBegin=307 CharacterOffsetEnd=308 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (37 tokens):\n",
      "However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems.\n",
      "\n",
      "Tokens:\n",
      "[Text=However CharacterOffsetBegin=0 CharacterOffsetEnd=7 PartOfSpeech=RB Lemma=however]\n",
      "[Text=, CharacterOffsetBegin=7 CharacterOffsetEnd=8 PartOfSpeech=, Lemma=,]\n",
      "[Text=most CharacterOffsetBegin=9 CharacterOffsetEnd=13 PartOfSpeech=RBS Lemma=most]\n",
      "[Text=other CharacterOffsetBegin=14 CharacterOffsetEnd=19 PartOfSpeech=JJ Lemma=other]\n",
      "[Text=systems CharacterOffsetBegin=20 CharacterOffsetEnd=27 PartOfSpeech=NNS Lemma=system]\n",
      "[Text=depended CharacterOffsetBegin=28 CharacterOffsetEnd=36 PartOfSpeech=VBD Lemma=depend]\n",
      "[Text=on CharacterOffsetBegin=37 CharacterOffsetEnd=39 PartOfSpeech=IN Lemma=on]\n",
      "[Text=corpora CharacterOffsetBegin=40 CharacterOffsetEnd=47 PartOfSpeech=NN Lemma=corpora]\n",
      "[Text=specifically CharacterOffsetBegin=48 CharacterOffsetEnd=60 PartOfSpeech=RB Lemma=specifically]\n",
      "[Text=developed CharacterOffsetBegin=61 CharacterOffsetEnd=70 PartOfSpeech=VBD Lemma=develop]\n",
      "[Text=for CharacterOffsetBegin=71 CharacterOffsetEnd=74 PartOfSpeech=IN Lemma=for]\n",
      "[Text=the CharacterOffsetBegin=75 CharacterOffsetEnd=78 PartOfSpeech=DT Lemma=the]\n",
      "[Text=tasks CharacterOffsetBegin=79 CharacterOffsetEnd=84 PartOfSpeech=NNS Lemma=task]\n",
      "[Text=implemented CharacterOffsetBegin=85 CharacterOffsetEnd=96 PartOfSpeech=VBN Lemma=implement]\n",
      "[Text=by CharacterOffsetBegin=97 CharacterOffsetEnd=99 PartOfSpeech=IN Lemma=by]\n",
      "[Text=these CharacterOffsetBegin=100 CharacterOffsetEnd=105 PartOfSpeech=DT Lemma=these]\n",
      "[Text=systems CharacterOffsetBegin=106 CharacterOffsetEnd=113 PartOfSpeech=NNS Lemma=system]\n",
      "[Text=, CharacterOffsetBegin=113 CharacterOffsetEnd=114 PartOfSpeech=, Lemma=,]\n",
      "[Text=which CharacterOffsetBegin=115 CharacterOffsetEnd=120 PartOfSpeech=WDT Lemma=which]\n",
      "[Text=was CharacterOffsetBegin=121 CharacterOffsetEnd=124 PartOfSpeech=VBD Lemma=be]\n",
      "[Text=-LRB- CharacterOffsetBegin=125 CharacterOffsetEnd=126 PartOfSpeech=-LRB- Lemma=-lrb-]\n",
      "[Text=and CharacterOffsetBegin=126 CharacterOffsetEnd=129 PartOfSpeech=CC Lemma=and]\n",
      "[Text=often CharacterOffsetBegin=130 CharacterOffsetEnd=135 PartOfSpeech=RB Lemma=often]\n",
      "[Text=continues CharacterOffsetBegin=136 CharacterOffsetEnd=145 PartOfSpeech=VBZ Lemma=continue]\n",
      "[Text=to CharacterOffsetBegin=146 CharacterOffsetEnd=148 PartOfSpeech=TO Lemma=to]\n",
      "[Text=be CharacterOffsetBegin=149 CharacterOffsetEnd=151 PartOfSpeech=VB Lemma=be]\n",
      "[Text=-RRB- CharacterOffsetBegin=151 CharacterOffsetEnd=152 PartOfSpeech=-RRB- Lemma=-rrb-]\n",
      "[Text=a CharacterOffsetBegin=153 CharacterOffsetEnd=154 PartOfSpeech=DT Lemma=a]\n",
      "[Text=major CharacterOffsetBegin=155 CharacterOffsetEnd=160 PartOfSpeech=JJ Lemma=major]\n",
      "[Text=limitation CharacterOffsetBegin=161 CharacterOffsetEnd=171 PartOfSpeech=NN Lemma=limitation]\n",
      "[Text=in CharacterOffsetBegin=172 CharacterOffsetEnd=174 PartOfSpeech=IN Lemma=in]\n",
      "[Text=the CharacterOffsetBegin=175 CharacterOffsetEnd=178 PartOfSpeech=DT Lemma=the]\n",
      "[Text=success CharacterOffsetBegin=179 CharacterOffsetEnd=186 PartOfSpeech=NN Lemma=success]\n",
      "[Text=of CharacterOffsetBegin=187 CharacterOffsetEnd=189 PartOfSpeech=IN Lemma=of]\n",
      "[Text=these CharacterOffsetBegin=190 CharacterOffsetEnd=195 PartOfSpeech=DT Lemma=these]\n",
      "[Text=systems CharacterOffsetBegin=196 CharacterOffsetEnd=203 PartOfSpeech=NNS Lemma=system]\n",
      "[Text=. CharacterOffsetBegin=203 CharacterOffsetEnd=204 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (23 tokens):\n",
      "As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.\n",
      "\n",
      "Tokens:\n",
      "[Text=As CharacterOffsetBegin=0 CharacterOffsetEnd=2 PartOfSpeech=IN Lemma=as]\n",
      "[Text=a CharacterOffsetBegin=3 CharacterOffsetEnd=4 PartOfSpeech=DT Lemma=a]\n",
      "[Text=result CharacterOffsetBegin=5 CharacterOffsetEnd=11 PartOfSpeech=NN Lemma=result]\n",
      "[Text=, CharacterOffsetBegin=11 CharacterOffsetEnd=12 PartOfSpeech=, Lemma=,]\n",
      "[Text=a CharacterOffsetBegin=13 CharacterOffsetEnd=14 PartOfSpeech=DT Lemma=a]\n",
      "[Text=great CharacterOffsetBegin=15 CharacterOffsetEnd=20 PartOfSpeech=JJ Lemma=great]\n",
      "[Text=deal CharacterOffsetBegin=21 CharacterOffsetEnd=25 PartOfSpeech=NN Lemma=deal]\n",
      "[Text=of CharacterOffsetBegin=26 CharacterOffsetEnd=28 PartOfSpeech=IN Lemma=of]\n",
      "[Text=research CharacterOffsetBegin=29 CharacterOffsetEnd=37 PartOfSpeech=NN Lemma=research]\n",
      "[Text=has CharacterOffsetBegin=38 CharacterOffsetEnd=41 PartOfSpeech=VBZ Lemma=have]\n",
      "[Text=gone CharacterOffsetBegin=42 CharacterOffsetEnd=46 PartOfSpeech=VBN Lemma=go]\n",
      "[Text=into CharacterOffsetBegin=47 CharacterOffsetEnd=51 PartOfSpeech=IN Lemma=into]\n",
      "[Text=methods CharacterOffsetBegin=52 CharacterOffsetEnd=59 PartOfSpeech=NNS Lemma=method]\n",
      "[Text=of CharacterOffsetBegin=60 CharacterOffsetEnd=62 PartOfSpeech=IN Lemma=of]\n",
      "[Text=more CharacterOffsetBegin=63 CharacterOffsetEnd=67 PartOfSpeech=JJR Lemma=more]\n",
      "[Text=effectively CharacterOffsetBegin=68 CharacterOffsetEnd=79 PartOfSpeech=RB Lemma=effectively]\n",
      "[Text=learning CharacterOffsetBegin=80 CharacterOffsetEnd=88 PartOfSpeech=VBG Lemma=learn]\n",
      "[Text=from CharacterOffsetBegin=89 CharacterOffsetEnd=93 PartOfSpeech=IN Lemma=from]\n",
      "[Text=limited CharacterOffsetBegin=94 CharacterOffsetEnd=101 PartOfSpeech=JJ Lemma=limited]\n",
      "[Text=amounts CharacterOffsetBegin=102 CharacterOffsetEnd=109 PartOfSpeech=NNS Lemma=amount]\n",
      "[Text=of CharacterOffsetBegin=110 CharacterOffsetEnd=112 PartOfSpeech=IN Lemma=of]\n",
      "[Text=data CharacterOffsetBegin=113 CharacterOffsetEnd=117 PartOfSpeech=NNS Lemma=datum]\n",
      "[Text=. CharacterOffsetBegin=117 CharacterOffsetEnd=118 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (12 tokens):\n",
      "Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms.\n",
      "\n",
      "Tokens:\n",
      "[Text=Recent CharacterOffsetBegin=0 CharacterOffsetEnd=6 PartOfSpeech=JJ Lemma=recent]\n",
      "[Text=research CharacterOffsetBegin=7 CharacterOffsetEnd=15 PartOfSpeech=NN Lemma=research]\n",
      "[Text=has CharacterOffsetBegin=16 CharacterOffsetEnd=19 PartOfSpeech=VBZ Lemma=have]\n",
      "[Text=increasingly CharacterOffsetBegin=20 CharacterOffsetEnd=32 PartOfSpeech=RB Lemma=increasingly]\n",
      "[Text=focused CharacterOffsetBegin=33 CharacterOffsetEnd=40 PartOfSpeech=VBN Lemma=focus]\n",
      "[Text=on CharacterOffsetBegin=41 CharacterOffsetEnd=43 PartOfSpeech=IN Lemma=on]\n",
      "[Text=unsupervised CharacterOffsetBegin=44 CharacterOffsetEnd=56 PartOfSpeech=JJ Lemma=unsupervised]\n",
      "[Text=and CharacterOffsetBegin=57 CharacterOffsetEnd=60 PartOfSpeech=CC Lemma=and]\n",
      "[Text=semi-supervised CharacterOffsetBegin=61 CharacterOffsetEnd=76 PartOfSpeech=JJ Lemma=semi-supervised]\n",
      "[Text=learning CharacterOffsetBegin=77 CharacterOffsetEnd=85 PartOfSpeech=NN Lemma=learning]\n",
      "[Text=algorithms CharacterOffsetBegin=86 CharacterOffsetEnd=96 PartOfSpeech=NNS Lemma=algorithm]\n",
      "[Text=. CharacterOffsetBegin=96 CharacterOffsetEnd=97 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (28 tokens):\n",
      "Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data.\n",
      "\n",
      "Tokens:\n",
      "[Text=Such CharacterOffsetBegin=0 CharacterOffsetEnd=4 PartOfSpeech=JJ Lemma=such]\n",
      "[Text=algorithms CharacterOffsetBegin=5 CharacterOffsetEnd=15 PartOfSpeech=NNS Lemma=algorithm]\n",
      "[Text=are CharacterOffsetBegin=16 CharacterOffsetEnd=19 PartOfSpeech=VBP Lemma=be]\n",
      "[Text=able CharacterOffsetBegin=20 CharacterOffsetEnd=24 PartOfSpeech=JJ Lemma=able]\n",
      "[Text=to CharacterOffsetBegin=25 CharacterOffsetEnd=27 PartOfSpeech=TO Lemma=to]\n",
      "[Text=learn CharacterOffsetBegin=28 CharacterOffsetEnd=33 PartOfSpeech=VB Lemma=learn]\n",
      "[Text=from CharacterOffsetBegin=34 CharacterOffsetEnd=38 PartOfSpeech=IN Lemma=from]\n",
      "[Text=data CharacterOffsetBegin=39 CharacterOffsetEnd=43 PartOfSpeech=NNS Lemma=datum]\n",
      "[Text=that CharacterOffsetBegin=44 CharacterOffsetEnd=48 PartOfSpeech=WDT Lemma=that]\n",
      "[Text=has CharacterOffsetBegin=49 CharacterOffsetEnd=52 PartOfSpeech=VBZ Lemma=have]\n",
      "[Text=not CharacterOffsetBegin=53 CharacterOffsetEnd=56 PartOfSpeech=RB Lemma=not]\n",
      "[Text=been CharacterOffsetBegin=57 CharacterOffsetEnd=61 PartOfSpeech=VBN Lemma=be]\n",
      "[Text=hand-annotated CharacterOffsetBegin=62 CharacterOffsetEnd=76 PartOfSpeech=VBN Lemma=hand-annotate]\n",
      "[Text=with CharacterOffsetBegin=77 CharacterOffsetEnd=81 PartOfSpeech=IN Lemma=with]\n",
      "[Text=the CharacterOffsetBegin=82 CharacterOffsetEnd=85 PartOfSpeech=DT Lemma=the]\n",
      "[Text=desired CharacterOffsetBegin=86 CharacterOffsetEnd=93 PartOfSpeech=VBN Lemma=desire]\n",
      "[Text=answers CharacterOffsetBegin=94 CharacterOffsetEnd=101 PartOfSpeech=NNS Lemma=answer]\n",
      "[Text=, CharacterOffsetBegin=101 CharacterOffsetEnd=102 PartOfSpeech=, Lemma=,]\n",
      "[Text=or CharacterOffsetBegin=103 CharacterOffsetEnd=105 PartOfSpeech=CC Lemma=or]\n",
      "[Text=using CharacterOffsetBegin=106 CharacterOffsetEnd=111 PartOfSpeech=VBG Lemma=use]\n",
      "[Text=a CharacterOffsetBegin=112 CharacterOffsetEnd=113 PartOfSpeech=DT Lemma=a]\n",
      "[Text=combination CharacterOffsetBegin=114 CharacterOffsetEnd=125 PartOfSpeech=NN Lemma=combination]\n",
      "[Text=of CharacterOffsetBegin=126 CharacterOffsetEnd=128 PartOfSpeech=IN Lemma=of]\n",
      "[Text=annotated CharacterOffsetBegin=129 CharacterOffsetEnd=138 PartOfSpeech=JJ Lemma=annotated]\n",
      "[Text=and CharacterOffsetBegin=139 CharacterOffsetEnd=142 PartOfSpeech=CC Lemma=and]\n",
      "[Text=non-annotated CharacterOffsetBegin=143 CharacterOffsetEnd=156 PartOfSpeech=JJ Lemma=non-annotated]\n",
      "[Text=data CharacterOffsetBegin=157 CharacterOffsetEnd=161 PartOfSpeech=NNS Lemma=datum]\n",
      "[Text=. CharacterOffsetBegin=161 CharacterOffsetEnd=162 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (26 tokens):\n",
      "Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data.\n",
      "\n",
      "Tokens:\n",
      "[Text=Generally CharacterOffsetBegin=0 CharacterOffsetEnd=9 PartOfSpeech=RB Lemma=generally]\n",
      "[Text=, CharacterOffsetBegin=9 CharacterOffsetEnd=10 PartOfSpeech=, Lemma=,]\n",
      "[Text=this CharacterOffsetBegin=11 CharacterOffsetEnd=15 PartOfSpeech=DT Lemma=this]\n",
      "[Text=task CharacterOffsetBegin=16 CharacterOffsetEnd=20 PartOfSpeech=NN Lemma=task]\n",
      "[Text=is CharacterOffsetBegin=21 CharacterOffsetEnd=23 PartOfSpeech=VBZ Lemma=be]\n",
      "[Text=much CharacterOffsetBegin=24 CharacterOffsetEnd=28 PartOfSpeech=RB Lemma=much]\n",
      "[Text=more CharacterOffsetBegin=29 CharacterOffsetEnd=33 PartOfSpeech=RBR Lemma=more]\n",
      "[Text=difficult CharacterOffsetBegin=34 CharacterOffsetEnd=43 PartOfSpeech=JJ Lemma=difficult]\n",
      "[Text=than CharacterOffsetBegin=44 CharacterOffsetEnd=48 PartOfSpeech=IN Lemma=than]\n",
      "[Text=supervised CharacterOffsetBegin=49 CharacterOffsetEnd=59 PartOfSpeech=JJ Lemma=supervised]\n",
      "[Text=learning CharacterOffsetBegin=60 CharacterOffsetEnd=68 PartOfSpeech=NN Lemma=learning]\n",
      "[Text=, CharacterOffsetBegin=68 CharacterOffsetEnd=69 PartOfSpeech=, Lemma=,]\n",
      "[Text=and CharacterOffsetBegin=70 CharacterOffsetEnd=73 PartOfSpeech=CC Lemma=and]\n",
      "[Text=typically CharacterOffsetBegin=74 CharacterOffsetEnd=83 PartOfSpeech=RB Lemma=typically]\n",
      "[Text=produces CharacterOffsetBegin=84 CharacterOffsetEnd=92 PartOfSpeech=VBZ Lemma=produce]\n",
      "[Text=less CharacterOffsetBegin=93 CharacterOffsetEnd=97 PartOfSpeech=JJR Lemma=less]\n",
      "[Text=accurate CharacterOffsetBegin=98 CharacterOffsetEnd=106 PartOfSpeech=JJ Lemma=accurate]\n",
      "[Text=results CharacterOffsetBegin=107 CharacterOffsetEnd=114 PartOfSpeech=NNS Lemma=result]\n",
      "[Text=for CharacterOffsetBegin=115 CharacterOffsetEnd=118 PartOfSpeech=IN Lemma=for]\n",
      "[Text=a CharacterOffsetBegin=119 CharacterOffsetEnd=120 PartOfSpeech=DT Lemma=a]\n",
      "[Text=given CharacterOffsetBegin=121 CharacterOffsetEnd=126 PartOfSpeech=VBN Lemma=give]\n",
      "[Text=amount CharacterOffsetBegin=127 CharacterOffsetEnd=133 PartOfSpeech=NN Lemma=amount]\n",
      "[Text=of CharacterOffsetBegin=134 CharacterOffsetEnd=136 PartOfSpeech=IN Lemma=of]\n",
      "[Text=input CharacterOffsetBegin=137 CharacterOffsetEnd=142 PartOfSpeech=NN Lemma=input]\n",
      "[Text=data CharacterOffsetBegin=143 CharacterOffsetEnd=147 PartOfSpeech=NNS Lemma=datum]\n",
      "[Text=. CharacterOffsetBegin=147 CharacterOffsetEnd=148 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (38 tokens):\n",
      "However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results.\n",
      "\n",
      "Tokens:\n",
      "[Text=However CharacterOffsetBegin=0 CharacterOffsetEnd=7 PartOfSpeech=RB Lemma=however]\n",
      "[Text=, CharacterOffsetBegin=7 CharacterOffsetEnd=8 PartOfSpeech=, Lemma=,]\n",
      "[Text=there CharacterOffsetBegin=9 CharacterOffsetEnd=14 PartOfSpeech=EX Lemma=there]\n",
      "[Text=is CharacterOffsetBegin=15 CharacterOffsetEnd=17 PartOfSpeech=VBZ Lemma=be]\n",
      "[Text=an CharacterOffsetBegin=18 CharacterOffsetEnd=20 PartOfSpeech=DT Lemma=a]\n",
      "[Text=enormous CharacterOffsetBegin=21 CharacterOffsetEnd=29 PartOfSpeech=JJ Lemma=enormous]\n",
      "[Text=amount CharacterOffsetBegin=30 CharacterOffsetEnd=36 PartOfSpeech=NN Lemma=amount]\n",
      "[Text=of CharacterOffsetBegin=37 CharacterOffsetEnd=39 PartOfSpeech=IN Lemma=of]\n",
      "[Text=non-annotated CharacterOffsetBegin=40 CharacterOffsetEnd=53 PartOfSpeech=JJ Lemma=non-annotated]\n",
      "[Text=data CharacterOffsetBegin=54 CharacterOffsetEnd=58 PartOfSpeech=NNS Lemma=datum]\n",
      "[Text=available CharacterOffsetBegin=59 CharacterOffsetEnd=68 PartOfSpeech=JJ Lemma=available]\n",
      "[Text=-LRB- CharacterOffsetBegin=69 CharacterOffsetEnd=70 PartOfSpeech=-LRB- Lemma=-lrb-]\n",
      "[Text=including CharacterOffsetBegin=70 CharacterOffsetEnd=79 PartOfSpeech=VBG Lemma=include]\n",
      "[Text=, CharacterOffsetBegin=79 CharacterOffsetEnd=80 PartOfSpeech=, Lemma=,]\n",
      "[Text=among CharacterOffsetBegin=81 CharacterOffsetEnd=86 PartOfSpeech=IN Lemma=among]\n",
      "[Text=other CharacterOffsetBegin=87 CharacterOffsetEnd=92 PartOfSpeech=JJ Lemma=other]\n",
      "[Text=things CharacterOffsetBegin=93 CharacterOffsetEnd=99 PartOfSpeech=NNS Lemma=thing]\n",
      "[Text=, CharacterOffsetBegin=99 CharacterOffsetEnd=100 PartOfSpeech=, Lemma=,]\n",
      "[Text=the CharacterOffsetBegin=101 CharacterOffsetEnd=104 PartOfSpeech=DT Lemma=the]\n",
      "[Text=entire CharacterOffsetBegin=105 CharacterOffsetEnd=111 PartOfSpeech=JJ Lemma=entire]\n",
      "[Text=content CharacterOffsetBegin=112 CharacterOffsetEnd=119 PartOfSpeech=NN Lemma=content]\n",
      "[Text=of CharacterOffsetBegin=120 CharacterOffsetEnd=122 PartOfSpeech=IN Lemma=of]\n",
      "[Text=the CharacterOffsetBegin=123 CharacterOffsetEnd=126 PartOfSpeech=DT Lemma=the]\n",
      "[Text=World CharacterOffsetBegin=127 CharacterOffsetEnd=132 PartOfSpeech=NNP Lemma=World]\n",
      "[Text=Wide CharacterOffsetBegin=133 CharacterOffsetEnd=137 PartOfSpeech=NN Lemma=wide]\n",
      "[Text=Web CharacterOffsetBegin=138 CharacterOffsetEnd=141 PartOfSpeech=NN Lemma=web]\n",
      "[Text=-RRB- CharacterOffsetBegin=141 CharacterOffsetEnd=142 PartOfSpeech=-RRB- Lemma=-rrb-]\n",
      "[Text=, CharacterOffsetBegin=142 CharacterOffsetEnd=143 PartOfSpeech=, Lemma=,]\n",
      "[Text=which CharacterOffsetBegin=144 CharacterOffsetEnd=149 PartOfSpeech=WDT Lemma=which]\n",
      "[Text=can CharacterOffsetBegin=150 CharacterOffsetEnd=153 PartOfSpeech=MD Lemma=can]\n",
      "[Text=often CharacterOffsetBegin=154 CharacterOffsetEnd=159 PartOfSpeech=RB Lemma=often]\n",
      "[Text=make CharacterOffsetBegin=160 CharacterOffsetEnd=164 PartOfSpeech=VB Lemma=make]\n",
      "[Text=up CharacterOffsetBegin=165 CharacterOffsetEnd=167 PartOfSpeech=RP Lemma=up]\n",
      "[Text=for CharacterOffsetBegin=168 CharacterOffsetEnd=171 PartOfSpeech=IN Lemma=for]\n",
      "[Text=the CharacterOffsetBegin=172 CharacterOffsetEnd=175 PartOfSpeech=DT Lemma=the]\n",
      "[Text=inferior CharacterOffsetBegin=176 CharacterOffsetEnd=184 PartOfSpeech=JJ Lemma=inferior]\n",
      "[Text=results CharacterOffsetBegin=185 CharacterOffsetEnd=192 PartOfSpeech=NNS Lemma=result]\n",
      "[Text=. CharacterOffsetBegin=192 CharacterOffsetEnd=193 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (4 tokens):\n",
      "NLP using machine learning\n",
      "\n",
      "Tokens:\n",
      "[Text=NLP CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=NN Lemma=nlp]\n",
      "[Text=using CharacterOffsetBegin=4 CharacterOffsetEnd=9 PartOfSpeech=VBG Lemma=use]\n",
      "[Text=machine CharacterOffsetBegin=10 CharacterOffsetEnd=17 PartOfSpeech=NN Lemma=machine]\n",
      "[Text=learning CharacterOffsetBegin=18 CharacterOffsetEnd=26 PartOfSpeech=NN Lemma=learning]\n",
      "\n",
      "\n",
      "Sentence #1 (14 tokens):\n",
      "Modern NLP algorithms are based on machine learning, especially statistical machine learning.\n",
      "\n",
      "Tokens:\n",
      "[Text=Modern CharacterOffsetBegin=0 CharacterOffsetEnd=6 PartOfSpeech=NNP Lemma=Modern]\n",
      "[Text=NLP CharacterOffsetBegin=7 CharacterOffsetEnd=10 PartOfSpeech=NNP Lemma=NLP]\n",
      "[Text=algorithms CharacterOffsetBegin=11 CharacterOffsetEnd=21 PartOfSpeech=NNS Lemma=algorithm]\n",
      "[Text=are CharacterOffsetBegin=22 CharacterOffsetEnd=25 PartOfSpeech=VBP Lemma=be]\n",
      "[Text=based CharacterOffsetBegin=26 CharacterOffsetEnd=31 PartOfSpeech=VBN Lemma=base]\n",
      "[Text=on CharacterOffsetBegin=32 CharacterOffsetEnd=34 PartOfSpeech=IN Lemma=on]\n",
      "[Text=machine CharacterOffsetBegin=35 CharacterOffsetEnd=42 PartOfSpeech=NN Lemma=machine]\n",
      "[Text=learning CharacterOffsetBegin=43 CharacterOffsetEnd=51 PartOfSpeech=NN Lemma=learning]\n",
      "[Text=, CharacterOffsetBegin=51 CharacterOffsetEnd=52 PartOfSpeech=, Lemma=,]\n",
      "[Text=especially CharacterOffsetBegin=53 CharacterOffsetEnd=63 PartOfSpeech=RB Lemma=especially]\n",
      "[Text=statistical CharacterOffsetBegin=64 CharacterOffsetEnd=75 PartOfSpeech=JJ Lemma=statistical]\n",
      "[Text=machine CharacterOffsetBegin=76 CharacterOffsetEnd=83 PartOfSpeech=NN Lemma=machine]\n",
      "[Text=learning CharacterOffsetBegin=84 CharacterOffsetEnd=92 PartOfSpeech=NN Lemma=learning]\n",
      "[Text=. CharacterOffsetBegin=92 CharacterOffsetEnd=93 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (17 tokens):\n",
      "The paradigm of machine learning is different from that of most prior attempts at language processing.\n",
      "\n",
      "Tokens:\n",
      "[Text=The CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=DT Lemma=the]\n",
      "[Text=paradigm CharacterOffsetBegin=4 CharacterOffsetEnd=12 PartOfSpeech=NN Lemma=paradigm]\n",
      "[Text=of CharacterOffsetBegin=13 CharacterOffsetEnd=15 PartOfSpeech=IN Lemma=of]\n",
      "[Text=machine CharacterOffsetBegin=16 CharacterOffsetEnd=23 PartOfSpeech=NN Lemma=machine]\n",
      "[Text=learning CharacterOffsetBegin=24 CharacterOffsetEnd=32 PartOfSpeech=NN Lemma=learning]\n",
      "[Text=is CharacterOffsetBegin=33 CharacterOffsetEnd=35 PartOfSpeech=VBZ Lemma=be]\n",
      "[Text=different CharacterOffsetBegin=36 CharacterOffsetEnd=45 PartOfSpeech=JJ Lemma=different]\n",
      "[Text=from CharacterOffsetBegin=46 CharacterOffsetEnd=50 PartOfSpeech=IN Lemma=from]\n",
      "[Text=that CharacterOffsetBegin=51 CharacterOffsetEnd=55 PartOfSpeech=DT Lemma=that]\n",
      "[Text=of CharacterOffsetBegin=56 CharacterOffsetEnd=58 PartOfSpeech=IN Lemma=of]\n",
      "[Text=most CharacterOffsetBegin=59 CharacterOffsetEnd=63 PartOfSpeech=RBS Lemma=most]\n",
      "[Text=prior CharacterOffsetBegin=64 CharacterOffsetEnd=69 PartOfSpeech=JJ Lemma=prior]\n",
      "[Text=attempts CharacterOffsetBegin=70 CharacterOffsetEnd=78 PartOfSpeech=NNS Lemma=attempt]\n",
      "[Text=at CharacterOffsetBegin=79 CharacterOffsetEnd=81 PartOfSpeech=IN Lemma=at]\n",
      "[Text=language CharacterOffsetBegin=82 CharacterOffsetEnd=90 PartOfSpeech=NN Lemma=language]\n",
      "[Text=processing CharacterOffsetBegin=91 CharacterOffsetEnd=101 PartOfSpeech=NN Lemma=processing]\n",
      "[Text=. CharacterOffsetBegin=101 CharacterOffsetEnd=102 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (17 tokens):\n",
      "Prior implementations of language-processing tasks typically involved the direct hand coding of large sets of rules.\n",
      "\n",
      "Tokens:\n",
      "[Text=Prior CharacterOffsetBegin=0 CharacterOffsetEnd=5 PartOfSpeech=RB Lemma=prior]\n",
      "[Text=implementations CharacterOffsetBegin=6 CharacterOffsetEnd=21 PartOfSpeech=NNS Lemma=implementation]\n",
      "[Text=of CharacterOffsetBegin=22 CharacterOffsetEnd=24 PartOfSpeech=IN Lemma=of]\n",
      "[Text=language-processing CharacterOffsetBegin=25 CharacterOffsetEnd=44 PartOfSpeech=JJ Lemma=language-processing]\n",
      "[Text=tasks CharacterOffsetBegin=45 CharacterOffsetEnd=50 PartOfSpeech=NNS Lemma=task]\n",
      "[Text=typically CharacterOffsetBegin=51 CharacterOffsetEnd=60 PartOfSpeech=RB Lemma=typically]\n",
      "[Text=involved CharacterOffsetBegin=61 CharacterOffsetEnd=69 PartOfSpeech=VBD Lemma=involve]\n",
      "[Text=the CharacterOffsetBegin=70 CharacterOffsetEnd=73 PartOfSpeech=DT Lemma=the]\n",
      "[Text=direct CharacterOffsetBegin=74 CharacterOffsetEnd=80 PartOfSpeech=JJ Lemma=direct]\n",
      "[Text=hand CharacterOffsetBegin=81 CharacterOffsetEnd=85 PartOfSpeech=NN Lemma=hand]\n",
      "[Text=coding CharacterOffsetBegin=86 CharacterOffsetEnd=92 PartOfSpeech=NN Lemma=coding]\n",
      "[Text=of CharacterOffsetBegin=93 CharacterOffsetEnd=95 PartOfSpeech=IN Lemma=of]\n",
      "[Text=large CharacterOffsetBegin=96 CharacterOffsetEnd=101 PartOfSpeech=JJ Lemma=large]\n",
      "[Text=sets CharacterOffsetBegin=102 CharacterOffsetEnd=106 PartOfSpeech=NNS Lemma=set]\n",
      "[Text=of CharacterOffsetBegin=107 CharacterOffsetEnd=109 PartOfSpeech=IN Lemma=of]\n",
      "[Text=rules CharacterOffsetBegin=110 CharacterOffsetEnd=115 PartOfSpeech=NNS Lemma=rule]\n",
      "[Text=. CharacterOffsetBegin=115 CharacterOffsetEnd=116 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (38 tokens):\n",
      "The machine-learning paradigm calls instead for using general learning algorithms - often, although not always, grounded in statistical inference - to automatically learn such rules through the analysis of large corpora of typical real-world examples.\n",
      "\n",
      "Tokens:\n",
      "[Text=The CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=DT Lemma=the]\n",
      "[Text=machine-learning CharacterOffsetBegin=4 CharacterOffsetEnd=20 PartOfSpeech=JJ Lemma=machine-learning]\n",
      "[Text=paradigm CharacterOffsetBegin=21 CharacterOffsetEnd=29 PartOfSpeech=NN Lemma=paradigm]\n",
      "[Text=calls CharacterOffsetBegin=30 CharacterOffsetEnd=35 PartOfSpeech=VBZ Lemma=call]\n",
      "[Text=instead CharacterOffsetBegin=36 CharacterOffsetEnd=43 PartOfSpeech=RB Lemma=instead]\n",
      "[Text=for CharacterOffsetBegin=44 CharacterOffsetEnd=47 PartOfSpeech=IN Lemma=for]\n",
      "[Text=using CharacterOffsetBegin=48 CharacterOffsetEnd=53 PartOfSpeech=VBG Lemma=use]\n",
      "[Text=general CharacterOffsetBegin=54 CharacterOffsetEnd=61 PartOfSpeech=JJ Lemma=general]\n",
      "[Text=learning CharacterOffsetBegin=62 CharacterOffsetEnd=70 PartOfSpeech=NN Lemma=learning]\n",
      "[Text=algorithms CharacterOffsetBegin=71 CharacterOffsetEnd=81 PartOfSpeech=NNS Lemma=algorithm]\n",
      "[Text=- CharacterOffsetBegin=82 CharacterOffsetEnd=83 PartOfSpeech=: Lemma=-]\n",
      "[Text=often CharacterOffsetBegin=84 CharacterOffsetEnd=89 PartOfSpeech=RB Lemma=often]\n",
      "[Text=, CharacterOffsetBegin=89 CharacterOffsetEnd=90 PartOfSpeech=, Lemma=,]\n",
      "[Text=although CharacterOffsetBegin=91 CharacterOffsetEnd=99 PartOfSpeech=IN Lemma=although]\n",
      "[Text=not CharacterOffsetBegin=100 CharacterOffsetEnd=103 PartOfSpeech=RB Lemma=not]\n",
      "[Text=always CharacterOffsetBegin=104 CharacterOffsetEnd=110 PartOfSpeech=RB Lemma=always]\n",
      "[Text=, CharacterOffsetBegin=110 CharacterOffsetEnd=111 PartOfSpeech=, Lemma=,]\n",
      "[Text=grounded CharacterOffsetBegin=112 CharacterOffsetEnd=120 PartOfSpeech=VBN Lemma=ground]\n",
      "[Text=in CharacterOffsetBegin=121 CharacterOffsetEnd=123 PartOfSpeech=IN Lemma=in]\n",
      "[Text=statistical CharacterOffsetBegin=124 CharacterOffsetEnd=135 PartOfSpeech=JJ Lemma=statistical]\n",
      "[Text=inference CharacterOffsetBegin=136 CharacterOffsetEnd=145 PartOfSpeech=NN Lemma=inference]\n",
      "[Text=- CharacterOffsetBegin=146 CharacterOffsetEnd=147 PartOfSpeech=: Lemma=-]\n",
      "[Text=to CharacterOffsetBegin=148 CharacterOffsetEnd=150 PartOfSpeech=TO Lemma=to]\n",
      "[Text=automatically CharacterOffsetBegin=151 CharacterOffsetEnd=164 PartOfSpeech=RB Lemma=automatically]\n",
      "[Text=learn CharacterOffsetBegin=165 CharacterOffsetEnd=170 PartOfSpeech=VB Lemma=learn]\n",
      "[Text=such CharacterOffsetBegin=171 CharacterOffsetEnd=175 PartOfSpeech=JJ Lemma=such]\n",
      "[Text=rules CharacterOffsetBegin=176 CharacterOffsetEnd=181 PartOfSpeech=NNS Lemma=rule]\n",
      "[Text=through CharacterOffsetBegin=182 CharacterOffsetEnd=189 PartOfSpeech=IN Lemma=through]\n",
      "[Text=the CharacterOffsetBegin=190 CharacterOffsetEnd=193 PartOfSpeech=DT Lemma=the]\n",
      "[Text=analysis CharacterOffsetBegin=194 CharacterOffsetEnd=202 PartOfSpeech=NN Lemma=analysis]\n",
      "[Text=of CharacterOffsetBegin=203 CharacterOffsetEnd=205 PartOfSpeech=IN Lemma=of]\n",
      "[Text=large CharacterOffsetBegin=206 CharacterOffsetEnd=211 PartOfSpeech=JJ Lemma=large]\n",
      "[Text=corpora CharacterOffsetBegin=212 CharacterOffsetEnd=219 PartOfSpeech=NN Lemma=corpora]\n",
      "[Text=of CharacterOffsetBegin=220 CharacterOffsetEnd=222 PartOfSpeech=IN Lemma=of]\n",
      "[Text=typical CharacterOffsetBegin=223 CharacterOffsetEnd=230 PartOfSpeech=JJ Lemma=typical]\n",
      "[Text=real-world CharacterOffsetBegin=231 CharacterOffsetEnd=241 PartOfSpeech=JJ Lemma=real-world]\n",
      "[Text=examples CharacterOffsetBegin=242 CharacterOffsetEnd=250 PartOfSpeech=NNS Lemma=example]\n",
      "[Text=. CharacterOffsetBegin=250 CharacterOffsetEnd=251 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (33 tokens):\n",
      "A corpus (plural, \"corpora\") is a set of documents (or sometimes, individual sentences) that have been hand-annotated with the correct values to be learned.\n",
      "\n",
      "Tokens:\n",
      "[Text=A CharacterOffsetBegin=0 CharacterOffsetEnd=1 PartOfSpeech=DT Lemma=a]\n",
      "[Text=corpus CharacterOffsetBegin=2 CharacterOffsetEnd=8 PartOfSpeech=NN Lemma=corpus]\n",
      "[Text=-LRB- CharacterOffsetBegin=9 CharacterOffsetEnd=10 PartOfSpeech=-LRB- Lemma=-lrb-]\n",
      "[Text=plural CharacterOffsetBegin=10 CharacterOffsetEnd=16 PartOfSpeech=NN Lemma=plural]\n",
      "[Text=, CharacterOffsetBegin=16 CharacterOffsetEnd=17 PartOfSpeech=, Lemma=,]\n",
      "[Text=`` CharacterOffsetBegin=18 CharacterOffsetEnd=19 PartOfSpeech=`` Lemma=``]\n",
      "[Text=corpora CharacterOffsetBegin=19 CharacterOffsetEnd=26 PartOfSpeech=NN Lemma=corpora]\n",
      "[Text='' CharacterOffsetBegin=26 CharacterOffsetEnd=27 PartOfSpeech='' Lemma='']\n",
      "[Text=-RRB- CharacterOffsetBegin=27 CharacterOffsetEnd=28 PartOfSpeech=-RRB- Lemma=-rrb-]\n",
      "[Text=is CharacterOffsetBegin=29 CharacterOffsetEnd=31 PartOfSpeech=VBZ Lemma=be]\n",
      "[Text=a CharacterOffsetBegin=32 CharacterOffsetEnd=33 PartOfSpeech=DT Lemma=a]\n",
      "[Text=set CharacterOffsetBegin=34 CharacterOffsetEnd=37 PartOfSpeech=NN Lemma=set]\n",
      "[Text=of CharacterOffsetBegin=38 CharacterOffsetEnd=40 PartOfSpeech=IN Lemma=of]\n",
      "[Text=documents CharacterOffsetBegin=41 CharacterOffsetEnd=50 PartOfSpeech=NNS Lemma=document]\n",
      "[Text=-LRB- CharacterOffsetBegin=51 CharacterOffsetEnd=52 PartOfSpeech=-LRB- Lemma=-lrb-]\n",
      "[Text=or CharacterOffsetBegin=52 CharacterOffsetEnd=54 PartOfSpeech=CC Lemma=or]\n",
      "[Text=sometimes CharacterOffsetBegin=55 CharacterOffsetEnd=64 PartOfSpeech=RB Lemma=sometimes]\n",
      "[Text=, CharacterOffsetBegin=64 CharacterOffsetEnd=65 PartOfSpeech=, Lemma=,]\n",
      "[Text=individual CharacterOffsetBegin=66 CharacterOffsetEnd=76 PartOfSpeech=JJ Lemma=individual]\n",
      "[Text=sentences CharacterOffsetBegin=77 CharacterOffsetEnd=86 PartOfSpeech=NNS Lemma=sentence]\n",
      "[Text=-RRB- CharacterOffsetBegin=86 CharacterOffsetEnd=87 PartOfSpeech=-RRB- Lemma=-rrb-]\n",
      "[Text=that CharacterOffsetBegin=88 CharacterOffsetEnd=92 PartOfSpeech=WDT Lemma=that]\n",
      "[Text=have CharacterOffsetBegin=93 CharacterOffsetEnd=97 PartOfSpeech=VBP Lemma=have]\n",
      "[Text=been CharacterOffsetBegin=98 CharacterOffsetEnd=102 PartOfSpeech=VBN Lemma=be]\n",
      "[Text=hand-annotated CharacterOffsetBegin=103 CharacterOffsetEnd=117 PartOfSpeech=JJ Lemma=hand-annotated]\n",
      "[Text=with CharacterOffsetBegin=118 CharacterOffsetEnd=122 PartOfSpeech=IN Lemma=with]\n",
      "[Text=the CharacterOffsetBegin=123 CharacterOffsetEnd=126 PartOfSpeech=DT Lemma=the]\n",
      "[Text=correct CharacterOffsetBegin=127 CharacterOffsetEnd=134 PartOfSpeech=JJ Lemma=correct]\n",
      "[Text=values CharacterOffsetBegin=135 CharacterOffsetEnd=141 PartOfSpeech=NNS Lemma=value]\n",
      "[Text=to CharacterOffsetBegin=142 CharacterOffsetEnd=144 PartOfSpeech=TO Lemma=to]\n",
      "[Text=be CharacterOffsetBegin=145 CharacterOffsetEnd=147 PartOfSpeech=VB Lemma=be]\n",
      "[Text=learned CharacterOffsetBegin=148 CharacterOffsetEnd=155 PartOfSpeech=VBN Lemma=learn]\n",
      "[Text=. CharacterOffsetBegin=155 CharacterOffsetEnd=156 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (14 tokens):\n",
      "Many different classes of machine learning algorithms have been applied to NLP tasks.\n",
      "\n",
      "Tokens:\n",
      "[Text=Many CharacterOffsetBegin=0 CharacterOffsetEnd=4 PartOfSpeech=JJ Lemma=many]\n",
      "[Text=different CharacterOffsetBegin=5 CharacterOffsetEnd=14 PartOfSpeech=JJ Lemma=different]\n",
      "[Text=classes CharacterOffsetBegin=15 CharacterOffsetEnd=22 PartOfSpeech=NNS Lemma=class]\n",
      "[Text=of CharacterOffsetBegin=23 CharacterOffsetEnd=25 PartOfSpeech=IN Lemma=of]\n",
      "[Text=machine CharacterOffsetBegin=26 CharacterOffsetEnd=33 PartOfSpeech=NN Lemma=machine]\n",
      "[Text=learning CharacterOffsetBegin=34 CharacterOffsetEnd=42 PartOfSpeech=NN Lemma=learning]\n",
      "[Text=algorithms CharacterOffsetBegin=43 CharacterOffsetEnd=53 PartOfSpeech=NNS Lemma=algorithm]\n",
      "[Text=have CharacterOffsetBegin=54 CharacterOffsetEnd=58 PartOfSpeech=VBP Lemma=have]\n",
      "[Text=been CharacterOffsetBegin=59 CharacterOffsetEnd=63 PartOfSpeech=VBN Lemma=be]\n",
      "[Text=applied CharacterOffsetBegin=64 CharacterOffsetEnd=71 PartOfSpeech=VBN Lemma=apply]\n",
      "[Text=to CharacterOffsetBegin=72 CharacterOffsetEnd=74 PartOfSpeech=TO Lemma=to]\n",
      "[Text=NLP CharacterOffsetBegin=75 CharacterOffsetEnd=78 PartOfSpeech=NN Lemma=nlp]\n",
      "[Text=tasks CharacterOffsetBegin=79 CharacterOffsetEnd=84 PartOfSpeech=NNS Lemma=task]\n",
      "[Text=. CharacterOffsetBegin=84 CharacterOffsetEnd=85 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (20 tokens):\n",
      "These algorithms take as input a large set of \"features\" that are generated from the input data.\n",
      "\n",
      "Tokens:\n",
      "[Text=These CharacterOffsetBegin=0 CharacterOffsetEnd=5 PartOfSpeech=DT Lemma=these]\n",
      "[Text=algorithms CharacterOffsetBegin=6 CharacterOffsetEnd=16 PartOfSpeech=NNS Lemma=algorithm]\n",
      "[Text=take CharacterOffsetBegin=17 CharacterOffsetEnd=21 PartOfSpeech=VBP Lemma=take]\n",
      "[Text=as CharacterOffsetBegin=22 CharacterOffsetEnd=24 PartOfSpeech=RB Lemma=as]\n",
      "[Text=input CharacterOffsetBegin=25 CharacterOffsetEnd=30 PartOfSpeech=NN Lemma=input]\n",
      "[Text=a CharacterOffsetBegin=31 CharacterOffsetEnd=32 PartOfSpeech=DT Lemma=a]\n",
      "[Text=large CharacterOffsetBegin=33 CharacterOffsetEnd=38 PartOfSpeech=JJ Lemma=large]\n",
      "[Text=set CharacterOffsetBegin=39 CharacterOffsetEnd=42 PartOfSpeech=NN Lemma=set]\n",
      "[Text=of CharacterOffsetBegin=43 CharacterOffsetEnd=45 PartOfSpeech=IN Lemma=of]\n",
      "[Text=`` CharacterOffsetBegin=46 CharacterOffsetEnd=47 PartOfSpeech=`` Lemma=``]\n",
      "[Text=features CharacterOffsetBegin=47 CharacterOffsetEnd=55 PartOfSpeech=NNS Lemma=feature]\n",
      "[Text='' CharacterOffsetBegin=55 CharacterOffsetEnd=56 PartOfSpeech='' Lemma='']\n",
      "[Text=that CharacterOffsetBegin=57 CharacterOffsetEnd=61 PartOfSpeech=WDT Lemma=that]\n",
      "[Text=are CharacterOffsetBegin=62 CharacterOffsetEnd=65 PartOfSpeech=VBP Lemma=be]\n",
      "[Text=generated CharacterOffsetBegin=66 CharacterOffsetEnd=75 PartOfSpeech=VBN Lemma=generate]\n",
      "[Text=from CharacterOffsetBegin=76 CharacterOffsetEnd=80 PartOfSpeech=IN Lemma=from]\n",
      "[Text=the CharacterOffsetBegin=81 CharacterOffsetEnd=84 PartOfSpeech=DT Lemma=the]\n",
      "[Text=input CharacterOffsetBegin=85 CharacterOffsetEnd=90 PartOfSpeech=NN Lemma=input]\n",
      "[Text=data CharacterOffsetBegin=91 CharacterOffsetEnd=95 PartOfSpeech=NNS Lemma=datum]\n",
      "[Text=. CharacterOffsetBegin=95 CharacterOffsetEnd=96 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (29 tokens):\n",
      "Some of the earliest-used algorithms, such as decision trees, produced systems of hard if-then rules similar to the systems of hand-written rules that were then common.\n",
      "\n",
      "Tokens:\n",
      "[Text=Some CharacterOffsetBegin=0 CharacterOffsetEnd=4 PartOfSpeech=DT Lemma=some]\n",
      "[Text=of CharacterOffsetBegin=5 CharacterOffsetEnd=7 PartOfSpeech=IN Lemma=of]\n",
      "[Text=the CharacterOffsetBegin=8 CharacterOffsetEnd=11 PartOfSpeech=DT Lemma=the]\n",
      "[Text=earliest-used CharacterOffsetBegin=12 CharacterOffsetEnd=25 PartOfSpeech=JJ Lemma=earliest-used]\n",
      "[Text=algorithms CharacterOffsetBegin=26 CharacterOffsetEnd=36 PartOfSpeech=NNS Lemma=algorithm]\n",
      "[Text=, CharacterOffsetBegin=36 CharacterOffsetEnd=37 PartOfSpeech=, Lemma=,]\n",
      "[Text=such CharacterOffsetBegin=38 CharacterOffsetEnd=42 PartOfSpeech=JJ Lemma=such]\n",
      "[Text=as CharacterOffsetBegin=43 CharacterOffsetEnd=45 PartOfSpeech=IN Lemma=as]\n",
      "[Text=decision CharacterOffsetBegin=46 CharacterOffsetEnd=54 PartOfSpeech=NN Lemma=decision]\n",
      "[Text=trees CharacterOffsetBegin=55 CharacterOffsetEnd=60 PartOfSpeech=NNS Lemma=tree]\n",
      "[Text=, CharacterOffsetBegin=60 CharacterOffsetEnd=61 PartOfSpeech=, Lemma=,]\n",
      "[Text=produced CharacterOffsetBegin=62 CharacterOffsetEnd=70 PartOfSpeech=VBD Lemma=produce]\n",
      "[Text=systems CharacterOffsetBegin=71 CharacterOffsetEnd=78 PartOfSpeech=NNS Lemma=system]\n",
      "[Text=of CharacterOffsetBegin=79 CharacterOffsetEnd=81 PartOfSpeech=IN Lemma=of]\n",
      "[Text=hard CharacterOffsetBegin=82 CharacterOffsetEnd=86 PartOfSpeech=JJ Lemma=hard]\n",
      "[Text=if-then CharacterOffsetBegin=87 CharacterOffsetEnd=94 PartOfSpeech=JJ Lemma=if-then]\n",
      "[Text=rules CharacterOffsetBegin=95 CharacterOffsetEnd=100 PartOfSpeech=NNS Lemma=rule]\n",
      "[Text=similar CharacterOffsetBegin=101 CharacterOffsetEnd=108 PartOfSpeech=JJ Lemma=similar]\n",
      "[Text=to CharacterOffsetBegin=109 CharacterOffsetEnd=111 PartOfSpeech=TO Lemma=to]\n",
      "[Text=the CharacterOffsetBegin=112 CharacterOffsetEnd=115 PartOfSpeech=DT Lemma=the]\n",
      "[Text=systems CharacterOffsetBegin=116 CharacterOffsetEnd=123 PartOfSpeech=NNS Lemma=system]\n",
      "[Text=of CharacterOffsetBegin=124 CharacterOffsetEnd=126 PartOfSpeech=IN Lemma=of]\n",
      "[Text=hand-written CharacterOffsetBegin=127 CharacterOffsetEnd=139 PartOfSpeech=JJ Lemma=hand-written]\n",
      "[Text=rules CharacterOffsetBegin=140 CharacterOffsetEnd=145 PartOfSpeech=NNS Lemma=rule]\n",
      "[Text=that CharacterOffsetBegin=146 CharacterOffsetEnd=150 PartOfSpeech=WDT Lemma=that]\n",
      "[Text=were CharacterOffsetBegin=151 CharacterOffsetEnd=155 PartOfSpeech=VBD Lemma=be]\n",
      "[Text=then CharacterOffsetBegin=156 CharacterOffsetEnd=160 PartOfSpeech=RB Lemma=then]\n",
      "[Text=common CharacterOffsetBegin=161 CharacterOffsetEnd=167 PartOfSpeech=JJ Lemma=common]\n",
      "[Text=. CharacterOffsetBegin=167 CharacterOffsetEnd=168 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (27 tokens):\n",
      "Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature.\n",
      "\n",
      "Tokens:\n",
      "[Text=Increasingly CharacterOffsetBegin=0 CharacterOffsetEnd=12 PartOfSpeech=RB Lemma=increasingly]\n",
      "[Text=, CharacterOffsetBegin=12 CharacterOffsetEnd=13 PartOfSpeech=, Lemma=,]\n",
      "[Text=however CharacterOffsetBegin=14 CharacterOffsetEnd=21 PartOfSpeech=RB Lemma=however]\n",
      "[Text=, CharacterOffsetBegin=21 CharacterOffsetEnd=22 PartOfSpeech=, Lemma=,]\n",
      "[Text=research CharacterOffsetBegin=23 CharacterOffsetEnd=31 PartOfSpeech=NN Lemma=research]\n",
      "[Text=has CharacterOffsetBegin=32 CharacterOffsetEnd=35 PartOfSpeech=VBZ Lemma=have]\n",
      "[Text=focused CharacterOffsetBegin=36 CharacterOffsetEnd=43 PartOfSpeech=VBN Lemma=focus]\n",
      "[Text=on CharacterOffsetBegin=44 CharacterOffsetEnd=46 PartOfSpeech=IN Lemma=on]\n",
      "[Text=statistical CharacterOffsetBegin=47 CharacterOffsetEnd=58 PartOfSpeech=JJ Lemma=statistical]\n",
      "[Text=models CharacterOffsetBegin=59 CharacterOffsetEnd=65 PartOfSpeech=NNS Lemma=model]\n",
      "[Text=, CharacterOffsetBegin=65 CharacterOffsetEnd=66 PartOfSpeech=, Lemma=,]\n",
      "[Text=which CharacterOffsetBegin=67 CharacterOffsetEnd=72 PartOfSpeech=WDT Lemma=which]\n",
      "[Text=make CharacterOffsetBegin=73 CharacterOffsetEnd=77 PartOfSpeech=VBP Lemma=make]\n",
      "[Text=soft CharacterOffsetBegin=78 CharacterOffsetEnd=82 PartOfSpeech=JJ Lemma=soft]\n",
      "[Text=, CharacterOffsetBegin=82 CharacterOffsetEnd=83 PartOfSpeech=, Lemma=,]\n",
      "[Text=probabilistic CharacterOffsetBegin=84 CharacterOffsetEnd=97 PartOfSpeech=JJ Lemma=probabilistic]\n",
      "[Text=decisions CharacterOffsetBegin=98 CharacterOffsetEnd=107 PartOfSpeech=NNS Lemma=decision]\n",
      "[Text=based CharacterOffsetBegin=108 CharacterOffsetEnd=113 PartOfSpeech=VBN Lemma=base]\n",
      "[Text=on CharacterOffsetBegin=114 CharacterOffsetEnd=116 PartOfSpeech=IN Lemma=on]\n",
      "[Text=attaching CharacterOffsetBegin=117 CharacterOffsetEnd=126 PartOfSpeech=VBG Lemma=attach]\n",
      "[Text=real-valued CharacterOffsetBegin=127 CharacterOffsetEnd=138 PartOfSpeech=JJ Lemma=real-valued]\n",
      "[Text=weights CharacterOffsetBegin=139 CharacterOffsetEnd=146 PartOfSpeech=NNS Lemma=weight]\n",
      "[Text=to CharacterOffsetBegin=147 CharacterOffsetEnd=149 PartOfSpeech=TO Lemma=to]\n",
      "[Text=each CharacterOffsetBegin=150 CharacterOffsetEnd=154 PartOfSpeech=DT Lemma=each]\n",
      "[Text=input CharacterOffsetBegin=155 CharacterOffsetEnd=160 PartOfSpeech=NN Lemma=input]\n",
      "[Text=feature CharacterOffsetBegin=161 CharacterOffsetEnd=168 PartOfSpeech=NN Lemma=feature]\n",
      "[Text=. CharacterOffsetBegin=168 CharacterOffsetEnd=169 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (40 tokens):\n",
      "Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.\n",
      "\n",
      "Tokens:\n",
      "[Text=Such CharacterOffsetBegin=0 CharacterOffsetEnd=4 PartOfSpeech=JJ Lemma=such]\n",
      "[Text=models CharacterOffsetBegin=5 CharacterOffsetEnd=11 PartOfSpeech=NNS Lemma=model]\n",
      "[Text=have CharacterOffsetBegin=12 CharacterOffsetEnd=16 PartOfSpeech=VBP Lemma=have]\n",
      "[Text=the CharacterOffsetBegin=17 CharacterOffsetEnd=20 PartOfSpeech=DT Lemma=the]\n",
      "[Text=advantage CharacterOffsetBegin=21 CharacterOffsetEnd=30 PartOfSpeech=NN Lemma=advantage]\n",
      "[Text=that CharacterOffsetBegin=31 CharacterOffsetEnd=35 PartOfSpeech=IN Lemma=that]\n",
      "[Text=they CharacterOffsetBegin=36 CharacterOffsetEnd=40 PartOfSpeech=PRP Lemma=they]\n",
      "[Text=can CharacterOffsetBegin=41 CharacterOffsetEnd=44 PartOfSpeech=MD Lemma=can]\n",
      "[Text=express CharacterOffsetBegin=45 CharacterOffsetEnd=52 PartOfSpeech=VB Lemma=express]\n",
      "[Text=the CharacterOffsetBegin=53 CharacterOffsetEnd=56 PartOfSpeech=DT Lemma=the]\n",
      "[Text=relative CharacterOffsetBegin=57 CharacterOffsetEnd=65 PartOfSpeech=JJ Lemma=relative]\n",
      "[Text=certainty CharacterOffsetBegin=66 CharacterOffsetEnd=75 PartOfSpeech=NN Lemma=certainty]\n",
      "[Text=of CharacterOffsetBegin=76 CharacterOffsetEnd=78 PartOfSpeech=IN Lemma=of]\n",
      "[Text=many CharacterOffsetBegin=79 CharacterOffsetEnd=83 PartOfSpeech=JJ Lemma=many]\n",
      "[Text=different CharacterOffsetBegin=84 CharacterOffsetEnd=93 PartOfSpeech=JJ Lemma=different]\n",
      "[Text=possible CharacterOffsetBegin=94 CharacterOffsetEnd=102 PartOfSpeech=JJ Lemma=possible]\n",
      "[Text=answers CharacterOffsetBegin=103 CharacterOffsetEnd=110 PartOfSpeech=NNS Lemma=answer]\n",
      "[Text=rather CharacterOffsetBegin=111 CharacterOffsetEnd=117 PartOfSpeech=RB Lemma=rather]\n",
      "[Text=than CharacterOffsetBegin=118 CharacterOffsetEnd=122 PartOfSpeech=IN Lemma=than]\n",
      "[Text=only CharacterOffsetBegin=123 CharacterOffsetEnd=127 PartOfSpeech=RB Lemma=only]\n",
      "[Text=one CharacterOffsetBegin=128 CharacterOffsetEnd=131 PartOfSpeech=CD Lemma=one]\n",
      "[Text=, CharacterOffsetBegin=131 CharacterOffsetEnd=132 PartOfSpeech=, Lemma=,]\n",
      "[Text=producing CharacterOffsetBegin=133 CharacterOffsetEnd=142 PartOfSpeech=VBG Lemma=produce]\n",
      "[Text=more CharacterOffsetBegin=143 CharacterOffsetEnd=147 PartOfSpeech=JJR Lemma=more]\n",
      "[Text=reliable CharacterOffsetBegin=148 CharacterOffsetEnd=156 PartOfSpeech=JJ Lemma=reliable]\n",
      "[Text=results CharacterOffsetBegin=157 CharacterOffsetEnd=164 PartOfSpeech=NNS Lemma=result]\n",
      "[Text=when CharacterOffsetBegin=165 CharacterOffsetEnd=169 PartOfSpeech=WRB Lemma=when]\n",
      "[Text=such CharacterOffsetBegin=170 CharacterOffsetEnd=174 PartOfSpeech=PDT Lemma=such]\n",
      "[Text=a CharacterOffsetBegin=175 CharacterOffsetEnd=176 PartOfSpeech=DT Lemma=a]\n",
      "[Text=model CharacterOffsetBegin=177 CharacterOffsetEnd=182 PartOfSpeech=NN Lemma=model]\n",
      "[Text=is CharacterOffsetBegin=183 CharacterOffsetEnd=185 PartOfSpeech=VBZ Lemma=be]\n",
      "[Text=included CharacterOffsetBegin=186 CharacterOffsetEnd=194 PartOfSpeech=VBN Lemma=include]\n",
      "[Text=as CharacterOffsetBegin=195 CharacterOffsetEnd=197 PartOfSpeech=IN Lemma=as]\n",
      "[Text=a CharacterOffsetBegin=198 CharacterOffsetEnd=199 PartOfSpeech=DT Lemma=a]\n",
      "[Text=component CharacterOffsetBegin=200 CharacterOffsetEnd=209 PartOfSpeech=NN Lemma=component]\n",
      "[Text=of CharacterOffsetBegin=210 CharacterOffsetEnd=212 PartOfSpeech=IN Lemma=of]\n",
      "[Text=a CharacterOffsetBegin=213 CharacterOffsetEnd=214 PartOfSpeech=DT Lemma=a]\n",
      "[Text=larger CharacterOffsetBegin=215 CharacterOffsetEnd=221 PartOfSpeech=JJR Lemma=larger]\n",
      "[Text=system CharacterOffsetBegin=222 CharacterOffsetEnd=228 PartOfSpeech=NN Lemma=system]\n",
      "[Text=. CharacterOffsetBegin=228 CharacterOffsetEnd=229 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (12 tokens):\n",
      "Systems based on machine-learning algorithms have many advantages over hand-produced rules:\n",
      "\n",
      "Tokens:\n",
      "[Text=Systems CharacterOffsetBegin=0 CharacterOffsetEnd=7 PartOfSpeech=NNPS Lemma=Systems]\n",
      "[Text=based CharacterOffsetBegin=8 CharacterOffsetEnd=13 PartOfSpeech=VBN Lemma=base]\n",
      "[Text=on CharacterOffsetBegin=14 CharacterOffsetEnd=16 PartOfSpeech=IN Lemma=on]\n",
      "[Text=machine-learning CharacterOffsetBegin=17 CharacterOffsetEnd=33 PartOfSpeech=JJ Lemma=machine-learning]\n",
      "[Text=algorithms CharacterOffsetBegin=34 CharacterOffsetEnd=44 PartOfSpeech=NNS Lemma=algorithm]\n",
      "[Text=have CharacterOffsetBegin=45 CharacterOffsetEnd=49 PartOfSpeech=VBP Lemma=have]\n",
      "[Text=many CharacterOffsetBegin=50 CharacterOffsetEnd=54 PartOfSpeech=JJ Lemma=many]\n",
      "[Text=advantages CharacterOffsetBegin=55 CharacterOffsetEnd=65 PartOfSpeech=NNS Lemma=advantage]\n",
      "[Text=over CharacterOffsetBegin=66 CharacterOffsetEnd=70 PartOfSpeech=IN Lemma=over]\n",
      "[Text=hand-produced CharacterOffsetBegin=71 CharacterOffsetEnd=84 PartOfSpeech=JJ Lemma=hand-produced]\n",
      "[Text=rules CharacterOffsetBegin=85 CharacterOffsetEnd=90 PartOfSpeech=NNS Lemma=rule]\n",
      "[Text=: CharacterOffsetBegin=90 CharacterOffsetEnd=91 PartOfSpeech=: Lemma=:]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence #1 (35 tokens):\n",
      "The learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not obvious at all where the effort should be directed.\n",
      "\n",
      "Tokens:\n",
      "[Text=The CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=DT Lemma=the]\n",
      "[Text=learning CharacterOffsetBegin=4 CharacterOffsetEnd=12 PartOfSpeech=NN Lemma=learning]\n",
      "[Text=procedures CharacterOffsetBegin=13 CharacterOffsetEnd=23 PartOfSpeech=NNS Lemma=procedure]\n",
      "[Text=used CharacterOffsetBegin=24 CharacterOffsetEnd=28 PartOfSpeech=VBN Lemma=use]\n",
      "[Text=during CharacterOffsetBegin=29 CharacterOffsetEnd=35 PartOfSpeech=IN Lemma=during]\n",
      "[Text=machine CharacterOffsetBegin=36 CharacterOffsetEnd=43 PartOfSpeech=NN Lemma=machine]\n",
      "[Text=learning CharacterOffsetBegin=44 CharacterOffsetEnd=52 PartOfSpeech=NN Lemma=learning]\n",
      "[Text=automatically CharacterOffsetBegin=53 CharacterOffsetEnd=66 PartOfSpeech=RB Lemma=automatically]\n",
      "[Text=focus CharacterOffsetBegin=67 CharacterOffsetEnd=72 PartOfSpeech=VB Lemma=focus]\n",
      "[Text=on CharacterOffsetBegin=73 CharacterOffsetEnd=75 PartOfSpeech=IN Lemma=on]\n",
      "[Text=the CharacterOffsetBegin=76 CharacterOffsetEnd=79 PartOfSpeech=DT Lemma=the]\n",
      "[Text=most CharacterOffsetBegin=80 CharacterOffsetEnd=84 PartOfSpeech=RBS Lemma=most]\n",
      "[Text=common CharacterOffsetBegin=85 CharacterOffsetEnd=91 PartOfSpeech=JJ Lemma=common]\n",
      "[Text=cases CharacterOffsetBegin=92 CharacterOffsetEnd=97 PartOfSpeech=NNS Lemma=case]\n",
      "[Text=, CharacterOffsetBegin=97 CharacterOffsetEnd=98 PartOfSpeech=, Lemma=,]\n",
      "[Text=whereas CharacterOffsetBegin=99 CharacterOffsetEnd=106 PartOfSpeech=IN Lemma=whereas]\n",
      "[Text=when CharacterOffsetBegin=107 CharacterOffsetEnd=111 PartOfSpeech=WRB Lemma=when]\n",
      "[Text=writing CharacterOffsetBegin=112 CharacterOffsetEnd=119 PartOfSpeech=VBG Lemma=write]\n",
      "[Text=rules CharacterOffsetBegin=120 CharacterOffsetEnd=125 PartOfSpeech=NNS Lemma=rule]\n",
      "[Text=by CharacterOffsetBegin=126 CharacterOffsetEnd=128 PartOfSpeech=IN Lemma=by]\n",
      "[Text=hand CharacterOffsetBegin=129 CharacterOffsetEnd=133 PartOfSpeech=NN Lemma=hand]\n",
      "[Text=it CharacterOffsetBegin=134 CharacterOffsetEnd=136 PartOfSpeech=PRP Lemma=it]\n",
      "[Text=is CharacterOffsetBegin=137 CharacterOffsetEnd=139 PartOfSpeech=VBZ Lemma=be]\n",
      "[Text=often CharacterOffsetBegin=140 CharacterOffsetEnd=145 PartOfSpeech=RB Lemma=often]\n",
      "[Text=not CharacterOffsetBegin=146 CharacterOffsetEnd=149 PartOfSpeech=RB Lemma=not]\n",
      "[Text=obvious CharacterOffsetBegin=150 CharacterOffsetEnd=157 PartOfSpeech=JJ Lemma=obvious]\n",
      "[Text=at CharacterOffsetBegin=158 CharacterOffsetEnd=160 PartOfSpeech=IN Lemma=at]\n",
      "[Text=all CharacterOffsetBegin=161 CharacterOffsetEnd=164 PartOfSpeech=DT Lemma=all]\n",
      "[Text=where CharacterOffsetBegin=165 CharacterOffsetEnd=170 PartOfSpeech=WRB Lemma=where]\n",
      "[Text=the CharacterOffsetBegin=171 CharacterOffsetEnd=174 PartOfSpeech=DT Lemma=the]\n",
      "[Text=effort CharacterOffsetBegin=175 CharacterOffsetEnd=181 PartOfSpeech=NN Lemma=effort]\n",
      "[Text=should CharacterOffsetBegin=182 CharacterOffsetEnd=188 PartOfSpeech=MD Lemma=should]\n",
      "[Text=be CharacterOffsetBegin=189 CharacterOffsetEnd=191 PartOfSpeech=VB Lemma=be]\n",
      "[Text=directed CharacterOffsetBegin=192 CharacterOffsetEnd=200 PartOfSpeech=VBN Lemma=direct]\n",
      "[Text=. CharacterOffsetBegin=200 CharacterOffsetEnd=201 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (47 tokens):\n",
      "Automatic learning procedures can make use of statistical inference algorithms to produce models that are robust to unfamiliar input (e.g. containing words or structures that have not been seen before) and to erroneous input (e.g. with misspelled words or words accidentally omitted).\n",
      "\n",
      "Tokens:\n",
      "[Text=Automatic CharacterOffsetBegin=0 CharacterOffsetEnd=9 PartOfSpeech=NNP Lemma=Automatic]\n",
      "[Text=learning CharacterOffsetBegin=10 CharacterOffsetEnd=18 PartOfSpeech=VBG Lemma=learn]\n",
      "[Text=procedures CharacterOffsetBegin=19 CharacterOffsetEnd=29 PartOfSpeech=NNS Lemma=procedure]\n",
      "[Text=can CharacterOffsetBegin=30 CharacterOffsetEnd=33 PartOfSpeech=MD Lemma=can]\n",
      "[Text=make CharacterOffsetBegin=34 CharacterOffsetEnd=38 PartOfSpeech=VB Lemma=make]\n",
      "[Text=use CharacterOffsetBegin=39 CharacterOffsetEnd=42 PartOfSpeech=NN Lemma=use]\n",
      "[Text=of CharacterOffsetBegin=43 CharacterOffsetEnd=45 PartOfSpeech=IN Lemma=of]\n",
      "[Text=statistical CharacterOffsetBegin=46 CharacterOffsetEnd=57 PartOfSpeech=JJ Lemma=statistical]\n",
      "[Text=inference CharacterOffsetBegin=58 CharacterOffsetEnd=67 PartOfSpeech=NN Lemma=inference]\n",
      "[Text=algorithms CharacterOffsetBegin=68 CharacterOffsetEnd=78 PartOfSpeech=NNS Lemma=algorithm]\n",
      "[Text=to CharacterOffsetBegin=79 CharacterOffsetEnd=81 PartOfSpeech=TO Lemma=to]\n",
      "[Text=produce CharacterOffsetBegin=82 CharacterOffsetEnd=89 PartOfSpeech=VB Lemma=produce]\n",
      "[Text=models CharacterOffsetBegin=90 CharacterOffsetEnd=96 PartOfSpeech=NNS Lemma=model]\n",
      "[Text=that CharacterOffsetBegin=97 CharacterOffsetEnd=101 PartOfSpeech=WDT Lemma=that]\n",
      "[Text=are CharacterOffsetBegin=102 CharacterOffsetEnd=105 PartOfSpeech=VBP Lemma=be]\n",
      "[Text=robust CharacterOffsetBegin=106 CharacterOffsetEnd=112 PartOfSpeech=JJ Lemma=robust]\n",
      "[Text=to CharacterOffsetBegin=113 CharacterOffsetEnd=115 PartOfSpeech=TO Lemma=to]\n",
      "[Text=unfamiliar CharacterOffsetBegin=116 CharacterOffsetEnd=126 PartOfSpeech=JJ Lemma=unfamiliar]\n",
      "[Text=input CharacterOffsetBegin=127 CharacterOffsetEnd=132 PartOfSpeech=NN Lemma=input]\n",
      "[Text=-LRB- CharacterOffsetBegin=133 CharacterOffsetEnd=134 PartOfSpeech=-LRB- Lemma=-lrb-]\n",
      "[Text=e.g. CharacterOffsetBegin=134 CharacterOffsetEnd=138 PartOfSpeech=FW Lemma=e.g.]\n",
      "[Text=containing CharacterOffsetBegin=139 CharacterOffsetEnd=149 PartOfSpeech=VBG Lemma=contain]\n",
      "[Text=words CharacterOffsetBegin=150 CharacterOffsetEnd=155 PartOfSpeech=NNS Lemma=word]\n",
      "[Text=or CharacterOffsetBegin=156 CharacterOffsetEnd=158 PartOfSpeech=CC Lemma=or]\n",
      "[Text=structures CharacterOffsetBegin=159 CharacterOffsetEnd=169 PartOfSpeech=NNS Lemma=structure]\n",
      "[Text=that CharacterOffsetBegin=170 CharacterOffsetEnd=174 PartOfSpeech=WDT Lemma=that]\n",
      "[Text=have CharacterOffsetBegin=175 CharacterOffsetEnd=179 PartOfSpeech=VBP Lemma=have]\n",
      "[Text=not CharacterOffsetBegin=180 CharacterOffsetEnd=183 PartOfSpeech=RB Lemma=not]\n",
      "[Text=been CharacterOffsetBegin=184 CharacterOffsetEnd=188 PartOfSpeech=VBN Lemma=be]\n",
      "[Text=seen CharacterOffsetBegin=189 CharacterOffsetEnd=193 PartOfSpeech=VBN Lemma=see]\n",
      "[Text=before CharacterOffsetBegin=194 CharacterOffsetEnd=200 PartOfSpeech=IN Lemma=before]\n",
      "[Text=-RRB- CharacterOffsetBegin=200 CharacterOffsetEnd=201 PartOfSpeech=-RRB- Lemma=-rrb-]\n",
      "[Text=and CharacterOffsetBegin=202 CharacterOffsetEnd=205 PartOfSpeech=CC Lemma=and]\n",
      "[Text=to CharacterOffsetBegin=206 CharacterOffsetEnd=208 PartOfSpeech=TO Lemma=to]\n",
      "[Text=erroneous CharacterOffsetBegin=209 CharacterOffsetEnd=218 PartOfSpeech=JJ Lemma=erroneous]\n",
      "[Text=input CharacterOffsetBegin=219 CharacterOffsetEnd=224 PartOfSpeech=NN Lemma=input]\n",
      "[Text=-LRB- CharacterOffsetBegin=225 CharacterOffsetEnd=226 PartOfSpeech=-LRB- Lemma=-lrb-]\n",
      "[Text=e.g. CharacterOffsetBegin=226 CharacterOffsetEnd=230 PartOfSpeech=FW Lemma=e.g.]\n",
      "[Text=with CharacterOffsetBegin=231 CharacterOffsetEnd=235 PartOfSpeech=IN Lemma=with]\n",
      "[Text=misspelled CharacterOffsetBegin=236 CharacterOffsetEnd=246 PartOfSpeech=VBN Lemma=misspell]\n",
      "[Text=words CharacterOffsetBegin=247 CharacterOffsetEnd=252 PartOfSpeech=NNS Lemma=word]\n",
      "[Text=or CharacterOffsetBegin=253 CharacterOffsetEnd=255 PartOfSpeech=CC Lemma=or]\n",
      "[Text=words CharacterOffsetBegin=256 CharacterOffsetEnd=261 PartOfSpeech=NNS Lemma=word]\n",
      "[Text=accidentally CharacterOffsetBegin=262 CharacterOffsetEnd=274 PartOfSpeech=RB Lemma=accidentally]\n",
      "[Text=omitted CharacterOffsetBegin=275 CharacterOffsetEnd=282 PartOfSpeech=VBN Lemma=omit]\n",
      "[Text=-RRB- CharacterOffsetBegin=282 CharacterOffsetEnd=283 PartOfSpeech=-RRB- Lemma=-rrb-]\n",
      "[Text=. CharacterOffsetBegin=283 CharacterOffsetEnd=284 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (31 tokens):\n",
      "Generally, handling such input gracefully with hand-written rules -- or more generally, creating systems of hand-written rules that make soft decisions -- extremely difficult, error-prone and time-consuming.\n",
      "\n",
      "Tokens:\n",
      "[Text=Generally CharacterOffsetBegin=0 CharacterOffsetEnd=9 PartOfSpeech=RB Lemma=generally]\n",
      "[Text=, CharacterOffsetBegin=9 CharacterOffsetEnd=10 PartOfSpeech=, Lemma=,]\n",
      "[Text=handling CharacterOffsetBegin=11 CharacterOffsetEnd=19 PartOfSpeech=VBG Lemma=handle]\n",
      "[Text=such CharacterOffsetBegin=20 CharacterOffsetEnd=24 PartOfSpeech=JJ Lemma=such]\n",
      "[Text=input CharacterOffsetBegin=25 CharacterOffsetEnd=30 PartOfSpeech=NN Lemma=input]\n",
      "[Text=gracefully CharacterOffsetBegin=31 CharacterOffsetEnd=41 PartOfSpeech=RB Lemma=gracefully]\n",
      "[Text=with CharacterOffsetBegin=42 CharacterOffsetEnd=46 PartOfSpeech=IN Lemma=with]\n",
      "[Text=hand-written CharacterOffsetBegin=47 CharacterOffsetEnd=59 PartOfSpeech=JJ Lemma=hand-written]\n",
      "[Text=rules CharacterOffsetBegin=60 CharacterOffsetEnd=65 PartOfSpeech=NNS Lemma=rule]\n",
      "[Text=-- CharacterOffsetBegin=66 CharacterOffsetEnd=68 PartOfSpeech=: Lemma=--]\n",
      "[Text=or CharacterOffsetBegin=69 CharacterOffsetEnd=71 PartOfSpeech=CC Lemma=or]\n",
      "[Text=more CharacterOffsetBegin=72 CharacterOffsetEnd=76 PartOfSpeech=JJR Lemma=more]\n",
      "[Text=generally CharacterOffsetBegin=77 CharacterOffsetEnd=86 PartOfSpeech=RB Lemma=generally]\n",
      "[Text=, CharacterOffsetBegin=86 CharacterOffsetEnd=87 PartOfSpeech=, Lemma=,]\n",
      "[Text=creating CharacterOffsetBegin=88 CharacterOffsetEnd=96 PartOfSpeech=VBG Lemma=create]\n",
      "[Text=systems CharacterOffsetBegin=97 CharacterOffsetEnd=104 PartOfSpeech=NNS Lemma=system]\n",
      "[Text=of CharacterOffsetBegin=105 CharacterOffsetEnd=107 PartOfSpeech=IN Lemma=of]\n",
      "[Text=hand-written CharacterOffsetBegin=108 CharacterOffsetEnd=120 PartOfSpeech=JJ Lemma=hand-written]\n",
      "[Text=rules CharacterOffsetBegin=121 CharacterOffsetEnd=126 PartOfSpeech=NNS Lemma=rule]\n",
      "[Text=that CharacterOffsetBegin=127 CharacterOffsetEnd=131 PartOfSpeech=WDT Lemma=that]\n",
      "[Text=make CharacterOffsetBegin=132 CharacterOffsetEnd=136 PartOfSpeech=VBP Lemma=make]\n",
      "[Text=soft CharacterOffsetBegin=137 CharacterOffsetEnd=141 PartOfSpeech=JJ Lemma=soft]\n",
      "[Text=decisions CharacterOffsetBegin=142 CharacterOffsetEnd=151 PartOfSpeech=NNS Lemma=decision]\n",
      "[Text=-- CharacterOffsetBegin=152 CharacterOffsetEnd=154 PartOfSpeech=: Lemma=--]\n",
      "[Text=extremely CharacterOffsetBegin=155 CharacterOffsetEnd=164 PartOfSpeech=RB Lemma=extremely]\n",
      "[Text=difficult CharacterOffsetBegin=165 CharacterOffsetEnd=174 PartOfSpeech=JJ Lemma=difficult]\n",
      "[Text=, CharacterOffsetBegin=174 CharacterOffsetEnd=175 PartOfSpeech=, Lemma=,]\n",
      "[Text=error-prone CharacterOffsetBegin=176 CharacterOffsetEnd=187 PartOfSpeech=JJ Lemma=error-prone]\n",
      "[Text=and CharacterOffsetBegin=188 CharacterOffsetEnd=191 PartOfSpeech=CC Lemma=and]\n",
      "[Text=time-consuming CharacterOffsetBegin=192 CharacterOffsetEnd=206 PartOfSpeech=JJ Lemma=time-consuming]\n",
      "[Text=. CharacterOffsetBegin=206 CharacterOffsetEnd=207 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (19 tokens):\n",
      "Systems based on automatically learning the rules can be made more accurate simply by supplying more input data.\n",
      "\n",
      "Tokens:\n",
      "[Text=Systems CharacterOffsetBegin=0 CharacterOffsetEnd=7 PartOfSpeech=NNPS Lemma=Systems]\n",
      "[Text=based CharacterOffsetBegin=8 CharacterOffsetEnd=13 PartOfSpeech=VBN Lemma=base]\n",
      "[Text=on CharacterOffsetBegin=14 CharacterOffsetEnd=16 PartOfSpeech=IN Lemma=on]\n",
      "[Text=automatically CharacterOffsetBegin=17 CharacterOffsetEnd=30 PartOfSpeech=RB Lemma=automatically]\n",
      "[Text=learning CharacterOffsetBegin=31 CharacterOffsetEnd=39 PartOfSpeech=VBG Lemma=learn]\n",
      "[Text=the CharacterOffsetBegin=40 CharacterOffsetEnd=43 PartOfSpeech=DT Lemma=the]\n",
      "[Text=rules CharacterOffsetBegin=44 CharacterOffsetEnd=49 PartOfSpeech=NNS Lemma=rule]\n",
      "[Text=can CharacterOffsetBegin=50 CharacterOffsetEnd=53 PartOfSpeech=MD Lemma=can]\n",
      "[Text=be CharacterOffsetBegin=54 CharacterOffsetEnd=56 PartOfSpeech=VB Lemma=be]\n",
      "[Text=made CharacterOffsetBegin=57 CharacterOffsetEnd=61 PartOfSpeech=VBN Lemma=make]\n",
      "[Text=more CharacterOffsetBegin=62 CharacterOffsetEnd=66 PartOfSpeech=RBR Lemma=more]\n",
      "[Text=accurate CharacterOffsetBegin=67 CharacterOffsetEnd=75 PartOfSpeech=JJ Lemma=accurate]\n",
      "[Text=simply CharacterOffsetBegin=76 CharacterOffsetEnd=82 PartOfSpeech=RB Lemma=simply]\n",
      "[Text=by CharacterOffsetBegin=83 CharacterOffsetEnd=85 PartOfSpeech=IN Lemma=by]\n",
      "[Text=supplying CharacterOffsetBegin=86 CharacterOffsetEnd=95 PartOfSpeech=VBG Lemma=supply]\n",
      "[Text=more CharacterOffsetBegin=96 CharacterOffsetEnd=100 PartOfSpeech=JJR Lemma=more]\n",
      "[Text=input CharacterOffsetBegin=101 CharacterOffsetEnd=106 PartOfSpeech=NN Lemma=input]\n",
      "[Text=data CharacterOffsetBegin=107 CharacterOffsetEnd=111 PartOfSpeech=NNS Lemma=datum]\n",
      "[Text=. CharacterOffsetBegin=111 CharacterOffsetEnd=112 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (29 tokens):\n",
      "However, systems based on hand-written rules can only be made more accurate by increasing the complexity of the rules, which is a much more difficult task.\n",
      "\n",
      "Tokens:\n",
      "[Text=However CharacterOffsetBegin=0 CharacterOffsetEnd=7 PartOfSpeech=RB Lemma=however]\n",
      "[Text=, CharacterOffsetBegin=7 CharacterOffsetEnd=8 PartOfSpeech=, Lemma=,]\n",
      "[Text=systems CharacterOffsetBegin=9 CharacterOffsetEnd=16 PartOfSpeech=NNS Lemma=system]\n",
      "[Text=based CharacterOffsetBegin=17 CharacterOffsetEnd=22 PartOfSpeech=VBN Lemma=base]\n",
      "[Text=on CharacterOffsetBegin=23 CharacterOffsetEnd=25 PartOfSpeech=IN Lemma=on]\n",
      "[Text=hand-written CharacterOffsetBegin=26 CharacterOffsetEnd=38 PartOfSpeech=JJ Lemma=hand-written]\n",
      "[Text=rules CharacterOffsetBegin=39 CharacterOffsetEnd=44 PartOfSpeech=NNS Lemma=rule]\n",
      "[Text=can CharacterOffsetBegin=45 CharacterOffsetEnd=48 PartOfSpeech=MD Lemma=can]\n",
      "[Text=only CharacterOffsetBegin=49 CharacterOffsetEnd=53 PartOfSpeech=RB Lemma=only]\n",
      "[Text=be CharacterOffsetBegin=54 CharacterOffsetEnd=56 PartOfSpeech=VB Lemma=be]\n",
      "[Text=made CharacterOffsetBegin=57 CharacterOffsetEnd=61 PartOfSpeech=VBN Lemma=make]\n",
      "[Text=more CharacterOffsetBegin=62 CharacterOffsetEnd=66 PartOfSpeech=RBR Lemma=more]\n",
      "[Text=accurate CharacterOffsetBegin=67 CharacterOffsetEnd=75 PartOfSpeech=JJ Lemma=accurate]\n",
      "[Text=by CharacterOffsetBegin=76 CharacterOffsetEnd=78 PartOfSpeech=IN Lemma=by]\n",
      "[Text=increasing CharacterOffsetBegin=79 CharacterOffsetEnd=89 PartOfSpeech=VBG Lemma=increase]\n",
      "[Text=the CharacterOffsetBegin=90 CharacterOffsetEnd=93 PartOfSpeech=DT Lemma=the]\n",
      "[Text=complexity CharacterOffsetBegin=94 CharacterOffsetEnd=104 PartOfSpeech=NN Lemma=complexity]\n",
      "[Text=of CharacterOffsetBegin=105 CharacterOffsetEnd=107 PartOfSpeech=IN Lemma=of]\n",
      "[Text=the CharacterOffsetBegin=108 CharacterOffsetEnd=111 PartOfSpeech=DT Lemma=the]\n",
      "[Text=rules CharacterOffsetBegin=112 CharacterOffsetEnd=117 PartOfSpeech=NNS Lemma=rule]\n",
      "[Text=, CharacterOffsetBegin=117 CharacterOffsetEnd=118 PartOfSpeech=, Lemma=,]\n",
      "[Text=which CharacterOffsetBegin=119 CharacterOffsetEnd=124 PartOfSpeech=WDT Lemma=which]\n",
      "[Text=is CharacterOffsetBegin=125 CharacterOffsetEnd=127 PartOfSpeech=VBZ Lemma=be]\n",
      "[Text=a CharacterOffsetBegin=128 CharacterOffsetEnd=129 PartOfSpeech=DT Lemma=a]\n",
      "[Text=much CharacterOffsetBegin=130 CharacterOffsetEnd=134 PartOfSpeech=RB Lemma=much]\n",
      "[Text=more CharacterOffsetBegin=135 CharacterOffsetEnd=139 PartOfSpeech=RBR Lemma=more]\n",
      "[Text=difficult CharacterOffsetBegin=140 CharacterOffsetEnd=149 PartOfSpeech=JJ Lemma=difficult]\n",
      "[Text=task CharacterOffsetBegin=150 CharacterOffsetEnd=154 PartOfSpeech=NN Lemma=task]\n",
      "[Text=. CharacterOffsetBegin=154 CharacterOffsetEnd=155 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (27 tokens):\n",
      "In particular, there is a limit to the complexity of systems based on hand-crafted rules, beyond which the systems become more and more unmanageable.\n",
      "\n",
      "Tokens:\n",
      "[Text=In CharacterOffsetBegin=0 CharacterOffsetEnd=2 PartOfSpeech=IN Lemma=in]\n",
      "[Text=particular CharacterOffsetBegin=3 CharacterOffsetEnd=13 PartOfSpeech=JJ Lemma=particular]\n",
      "[Text=, CharacterOffsetBegin=13 CharacterOffsetEnd=14 PartOfSpeech=, Lemma=,]\n",
      "[Text=there CharacterOffsetBegin=15 CharacterOffsetEnd=20 PartOfSpeech=EX Lemma=there]\n",
      "[Text=is CharacterOffsetBegin=21 CharacterOffsetEnd=23 PartOfSpeech=VBZ Lemma=be]\n",
      "[Text=a CharacterOffsetBegin=24 CharacterOffsetEnd=25 PartOfSpeech=DT Lemma=a]\n",
      "[Text=limit CharacterOffsetBegin=26 CharacterOffsetEnd=31 PartOfSpeech=NN Lemma=limit]\n",
      "[Text=to CharacterOffsetBegin=32 CharacterOffsetEnd=34 PartOfSpeech=TO Lemma=to]\n",
      "[Text=the CharacterOffsetBegin=35 CharacterOffsetEnd=38 PartOfSpeech=DT Lemma=the]\n",
      "[Text=complexity CharacterOffsetBegin=39 CharacterOffsetEnd=49 PartOfSpeech=NN Lemma=complexity]\n",
      "[Text=of CharacterOffsetBegin=50 CharacterOffsetEnd=52 PartOfSpeech=IN Lemma=of]\n",
      "[Text=systems CharacterOffsetBegin=53 CharacterOffsetEnd=60 PartOfSpeech=NNS Lemma=system]\n",
      "[Text=based CharacterOffsetBegin=61 CharacterOffsetEnd=66 PartOfSpeech=VBN Lemma=base]\n",
      "[Text=on CharacterOffsetBegin=67 CharacterOffsetEnd=69 PartOfSpeech=IN Lemma=on]\n",
      "[Text=hand-crafted CharacterOffsetBegin=70 CharacterOffsetEnd=82 PartOfSpeech=JJ Lemma=hand-crafted]\n",
      "[Text=rules CharacterOffsetBegin=83 CharacterOffsetEnd=88 PartOfSpeech=NNS Lemma=rule]\n",
      "[Text=, CharacterOffsetBegin=88 CharacterOffsetEnd=89 PartOfSpeech=, Lemma=,]\n",
      "[Text=beyond CharacterOffsetBegin=90 CharacterOffsetEnd=96 PartOfSpeech=IN Lemma=beyond]\n",
      "[Text=which CharacterOffsetBegin=97 CharacterOffsetEnd=102 PartOfSpeech=WDT Lemma=which]\n",
      "[Text=the CharacterOffsetBegin=103 CharacterOffsetEnd=106 PartOfSpeech=DT Lemma=the]\n",
      "[Text=systems CharacterOffsetBegin=107 CharacterOffsetEnd=114 PartOfSpeech=NNS Lemma=system]\n",
      "[Text=become CharacterOffsetBegin=115 CharacterOffsetEnd=121 PartOfSpeech=VBP Lemma=become]\n",
      "[Text=more CharacterOffsetBegin=122 CharacterOffsetEnd=126 PartOfSpeech=RBR Lemma=more]\n",
      "[Text=and CharacterOffsetBegin=127 CharacterOffsetEnd=130 PartOfSpeech=CC Lemma=and]\n",
      "[Text=more CharacterOffsetBegin=131 CharacterOffsetEnd=135 PartOfSpeech=RBR Lemma=more]\n",
      "[Text=unmanageable CharacterOffsetBegin=136 CharacterOffsetEnd=148 PartOfSpeech=JJ Lemma=unmanageable]\n",
      "[Text=. CharacterOffsetBegin=148 CharacterOffsetEnd=149 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (34 tokens):\n",
      "However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process.\n",
      "\n",
      "Tokens:\n",
      "[Text=However CharacterOffsetBegin=0 CharacterOffsetEnd=7 PartOfSpeech=RB Lemma=however]\n",
      "[Text=, CharacterOffsetBegin=7 CharacterOffsetEnd=8 PartOfSpeech=, Lemma=,]\n",
      "[Text=creating CharacterOffsetBegin=9 CharacterOffsetEnd=17 PartOfSpeech=VBG Lemma=create]\n",
      "[Text=more CharacterOffsetBegin=18 CharacterOffsetEnd=22 PartOfSpeech=JJR Lemma=more]\n",
      "[Text=data CharacterOffsetBegin=23 CharacterOffsetEnd=27 PartOfSpeech=NNS Lemma=datum]\n",
      "[Text=to CharacterOffsetBegin=28 CharacterOffsetEnd=30 PartOfSpeech=TO Lemma=to]\n",
      "[Text=input CharacterOffsetBegin=31 CharacterOffsetEnd=36 PartOfSpeech=NN Lemma=input]\n",
      "[Text=to CharacterOffsetBegin=37 CharacterOffsetEnd=39 PartOfSpeech=TO Lemma=to]\n",
      "[Text=machine-learning CharacterOffsetBegin=40 CharacterOffsetEnd=56 PartOfSpeech=JJ Lemma=machine-learning]\n",
      "[Text=systems CharacterOffsetBegin=57 CharacterOffsetEnd=64 PartOfSpeech=NNS Lemma=system]\n",
      "[Text=simply CharacterOffsetBegin=65 CharacterOffsetEnd=71 PartOfSpeech=RB Lemma=simply]\n",
      "[Text=requires CharacterOffsetBegin=72 CharacterOffsetEnd=80 PartOfSpeech=VBZ Lemma=require]\n",
      "[Text=a CharacterOffsetBegin=81 CharacterOffsetEnd=82 PartOfSpeech=DT Lemma=a]\n",
      "[Text=corresponding CharacterOffsetBegin=83 CharacterOffsetEnd=96 PartOfSpeech=JJ Lemma=corresponding]\n",
      "[Text=increase CharacterOffsetBegin=97 CharacterOffsetEnd=105 PartOfSpeech=NN Lemma=increase]\n",
      "[Text=in CharacterOffsetBegin=106 CharacterOffsetEnd=108 PartOfSpeech=IN Lemma=in]\n",
      "[Text=the CharacterOffsetBegin=109 CharacterOffsetEnd=112 PartOfSpeech=DT Lemma=the]\n",
      "[Text=number CharacterOffsetBegin=113 CharacterOffsetEnd=119 PartOfSpeech=NN Lemma=number]\n",
      "[Text=of CharacterOffsetBegin=120 CharacterOffsetEnd=122 PartOfSpeech=IN Lemma=of]\n",
      "[Text=man-hours CharacterOffsetBegin=123 CharacterOffsetEnd=132 PartOfSpeech=NN Lemma=man-hours]\n",
      "[Text=worked CharacterOffsetBegin=133 CharacterOffsetEnd=139 PartOfSpeech=VBD Lemma=work]\n",
      "[Text=, CharacterOffsetBegin=139 CharacterOffsetEnd=140 PartOfSpeech=, Lemma=,]\n",
      "[Text=generally CharacterOffsetBegin=141 CharacterOffsetEnd=150 PartOfSpeech=RB Lemma=generally]\n",
      "[Text=without CharacterOffsetBegin=151 CharacterOffsetEnd=158 PartOfSpeech=IN Lemma=without]\n",
      "[Text=significant CharacterOffsetBegin=159 CharacterOffsetEnd=170 PartOfSpeech=JJ Lemma=significant]\n",
      "[Text=increases CharacterOffsetBegin=171 CharacterOffsetEnd=180 PartOfSpeech=NNS Lemma=increase]\n",
      "[Text=in CharacterOffsetBegin=181 CharacterOffsetEnd=183 PartOfSpeech=IN Lemma=in]\n",
      "[Text=the CharacterOffsetBegin=184 CharacterOffsetEnd=187 PartOfSpeech=DT Lemma=the]\n",
      "[Text=complexity CharacterOffsetBegin=188 CharacterOffsetEnd=198 PartOfSpeech=NN Lemma=complexity]\n",
      "[Text=of CharacterOffsetBegin=199 CharacterOffsetEnd=201 PartOfSpeech=IN Lemma=of]\n",
      "[Text=the CharacterOffsetBegin=202 CharacterOffsetEnd=205 PartOfSpeech=DT Lemma=the]\n",
      "[Text=annotation CharacterOffsetBegin=206 CharacterOffsetEnd=216 PartOfSpeech=NN Lemma=annotation]\n",
      "[Text=process CharacterOffsetBegin=217 CharacterOffsetEnd=224 PartOfSpeech=NN Lemma=process]\n",
      "[Text=. CharacterOffsetBegin=224 CharacterOffsetEnd=225 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (41 tokens):\n",
      "The subfield of NLP devoted to learning approaches is known as Natural Language Learning (NLL) and its conference CoNLL and peak body SIGNLL are sponsored by ACL, recognizing also their links with Computational Linguistics and Language Acquisition.\n",
      "\n",
      "Tokens:\n",
      "[Text=The CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=DT Lemma=the]\n",
      "[Text=subfield CharacterOffsetBegin=4 CharacterOffsetEnd=12 PartOfSpeech=NN Lemma=subfield]\n",
      "[Text=of CharacterOffsetBegin=13 CharacterOffsetEnd=15 PartOfSpeech=IN Lemma=of]\n",
      "[Text=NLP CharacterOffsetBegin=16 CharacterOffsetEnd=19 PartOfSpeech=NNP Lemma=NLP]\n",
      "[Text=devoted CharacterOffsetBegin=20 CharacterOffsetEnd=27 PartOfSpeech=VBN Lemma=devote]\n",
      "[Text=to CharacterOffsetBegin=28 CharacterOffsetEnd=30 PartOfSpeech=IN Lemma=to]\n",
      "[Text=learning CharacterOffsetBegin=31 CharacterOffsetEnd=39 PartOfSpeech=VBG Lemma=learn]\n",
      "[Text=approaches CharacterOffsetBegin=40 CharacterOffsetEnd=50 PartOfSpeech=NNS Lemma=approach]\n",
      "[Text=is CharacterOffsetBegin=51 CharacterOffsetEnd=53 PartOfSpeech=VBZ Lemma=be]\n",
      "[Text=known CharacterOffsetBegin=54 CharacterOffsetEnd=59 PartOfSpeech=VBN Lemma=know]\n",
      "[Text=as CharacterOffsetBegin=60 CharacterOffsetEnd=62 PartOfSpeech=IN Lemma=as]\n",
      "[Text=Natural CharacterOffsetBegin=63 CharacterOffsetEnd=70 PartOfSpeech=JJ Lemma=natural]\n",
      "[Text=Language CharacterOffsetBegin=71 CharacterOffsetEnd=79 PartOfSpeech=NN Lemma=language]\n",
      "[Text=Learning CharacterOffsetBegin=80 CharacterOffsetEnd=88 PartOfSpeech=NNP Lemma=Learning]\n",
      "[Text=-LRB- CharacterOffsetBegin=89 CharacterOffsetEnd=90 PartOfSpeech=-LRB- Lemma=-lrb-]\n",
      "[Text=NLL CharacterOffsetBegin=90 CharacterOffsetEnd=93 PartOfSpeech=NNP Lemma=NLL]\n",
      "[Text=-RRB- CharacterOffsetBegin=93 CharacterOffsetEnd=94 PartOfSpeech=-RRB- Lemma=-rrb-]\n",
      "[Text=and CharacterOffsetBegin=95 CharacterOffsetEnd=98 PartOfSpeech=CC Lemma=and]\n",
      "[Text=its CharacterOffsetBegin=99 CharacterOffsetEnd=102 PartOfSpeech=PRP$ Lemma=its]\n",
      "[Text=conference CharacterOffsetBegin=103 CharacterOffsetEnd=113 PartOfSpeech=NN Lemma=conference]\n",
      "[Text=CoNLL CharacterOffsetBegin=114 CharacterOffsetEnd=119 PartOfSpeech=NN Lemma=conll]\n",
      "[Text=and CharacterOffsetBegin=120 CharacterOffsetEnd=123 PartOfSpeech=CC Lemma=and]\n",
      "[Text=peak CharacterOffsetBegin=124 CharacterOffsetEnd=128 PartOfSpeech=NN Lemma=peak]\n",
      "[Text=body CharacterOffsetBegin=129 CharacterOffsetEnd=133 PartOfSpeech=NN Lemma=body]\n",
      "[Text=SIGNLL CharacterOffsetBegin=134 CharacterOffsetEnd=140 PartOfSpeech=NNP Lemma=SIGNLL]\n",
      "[Text=are CharacterOffsetBegin=141 CharacterOffsetEnd=144 PartOfSpeech=VBP Lemma=be]\n",
      "[Text=sponsored CharacterOffsetBegin=145 CharacterOffsetEnd=154 PartOfSpeech=VBN Lemma=sponsor]\n",
      "[Text=by CharacterOffsetBegin=155 CharacterOffsetEnd=157 PartOfSpeech=IN Lemma=by]\n",
      "[Text=ACL CharacterOffsetBegin=158 CharacterOffsetEnd=161 PartOfSpeech=NN Lemma=acl]\n",
      "[Text=, CharacterOffsetBegin=161 CharacterOffsetEnd=162 PartOfSpeech=, Lemma=,]\n",
      "[Text=recognizing CharacterOffsetBegin=163 CharacterOffsetEnd=174 PartOfSpeech=VBG Lemma=recognize]\n",
      "[Text=also CharacterOffsetBegin=175 CharacterOffsetEnd=179 PartOfSpeech=RB Lemma=also]\n",
      "[Text=their CharacterOffsetBegin=180 CharacterOffsetEnd=185 PartOfSpeech=PRP$ Lemma=they]\n",
      "[Text=links CharacterOffsetBegin=186 CharacterOffsetEnd=191 PartOfSpeech=NNS Lemma=link]\n",
      "[Text=with CharacterOffsetBegin=192 CharacterOffsetEnd=196 PartOfSpeech=IN Lemma=with]\n",
      "[Text=Computational CharacterOffsetBegin=197 CharacterOffsetEnd=210 PartOfSpeech=JJ Lemma=computational]\n",
      "[Text=Linguistics CharacterOffsetBegin=211 CharacterOffsetEnd=222 PartOfSpeech=NNS Lemma=linguistics]\n",
      "[Text=and CharacterOffsetBegin=223 CharacterOffsetEnd=226 PartOfSpeech=CC Lemma=and]\n",
      "[Text=Language CharacterOffsetBegin=227 CharacterOffsetEnd=235 PartOfSpeech=NNP Lemma=Language]\n",
      "[Text=Acquisition CharacterOffsetBegin=236 CharacterOffsetEnd=247 PartOfSpeech=NNP Lemma=Acquisition]\n",
      "[Text=. CharacterOffsetBegin=247 CharacterOffsetEnd=248 PartOfSpeech=. Lemma=.]\n",
      "\n",
      "\n",
      "Sentence #1 (30 tokens):\n",
      "When the aims of computational language learning research is to understand more about human language acquisition, or psycholinguistics, NLL overlaps into the related field of Computational Psycholinguistics.\n",
      "\n",
      "Tokens:\n",
      "[Text=When CharacterOffsetBegin=0 CharacterOffsetEnd=4 PartOfSpeech=WRB Lemma=when]\n",
      "[Text=the CharacterOffsetBegin=5 CharacterOffsetEnd=8 PartOfSpeech=DT Lemma=the]\n",
      "[Text=aims CharacterOffsetBegin=9 CharacterOffsetEnd=13 PartOfSpeech=NNS Lemma=aim]\n",
      "[Text=of CharacterOffsetBegin=14 CharacterOffsetEnd=16 PartOfSpeech=IN Lemma=of]\n",
      "[Text=computational CharacterOffsetBegin=17 CharacterOffsetEnd=30 PartOfSpeech=JJ Lemma=computational]\n",
      "[Text=language CharacterOffsetBegin=31 CharacterOffsetEnd=39 PartOfSpeech=NN Lemma=language]\n",
      "[Text=learning CharacterOffsetBegin=40 CharacterOffsetEnd=48 PartOfSpeech=VBG Lemma=learn]\n",
      "[Text=research CharacterOffsetBegin=49 CharacterOffsetEnd=57 PartOfSpeech=NN Lemma=research]\n",
      "[Text=is CharacterOffsetBegin=58 CharacterOffsetEnd=60 PartOfSpeech=VBZ Lemma=be]\n",
      "[Text=to CharacterOffsetBegin=61 CharacterOffsetEnd=63 PartOfSpeech=TO Lemma=to]\n",
      "[Text=understand CharacterOffsetBegin=64 CharacterOffsetEnd=74 PartOfSpeech=VB Lemma=understand]\n",
      "[Text=more CharacterOffsetBegin=75 CharacterOffsetEnd=79 PartOfSpeech=JJR Lemma=more]\n",
      "[Text=about CharacterOffsetBegin=80 CharacterOffsetEnd=85 PartOfSpeech=IN Lemma=about]\n",
      "[Text=human CharacterOffsetBegin=86 CharacterOffsetEnd=91 PartOfSpeech=JJ Lemma=human]\n",
      "[Text=language CharacterOffsetBegin=92 CharacterOffsetEnd=100 PartOfSpeech=NN Lemma=language]\n",
      "[Text=acquisition CharacterOffsetBegin=101 CharacterOffsetEnd=112 PartOfSpeech=NN Lemma=acquisition]\n",
      "[Text=, CharacterOffsetBegin=112 CharacterOffsetEnd=113 PartOfSpeech=, Lemma=,]\n",
      "[Text=or CharacterOffsetBegin=114 CharacterOffsetEnd=116 PartOfSpeech=CC Lemma=or]\n",
      "[Text=psycholinguistics CharacterOffsetBegin=117 CharacterOffsetEnd=134 PartOfSpeech=NNS Lemma=psycholinguistic]\n",
      "[Text=, CharacterOffsetBegin=134 CharacterOffsetEnd=135 PartOfSpeech=, Lemma=,]\n",
      "[Text=NLL CharacterOffsetBegin=136 CharacterOffsetEnd=139 PartOfSpeech=NN Lemma=nll]\n",
      "[Text=overlaps CharacterOffsetBegin=140 CharacterOffsetEnd=148 PartOfSpeech=VBZ Lemma=overlap]\n",
      "[Text=into CharacterOffsetBegin=149 CharacterOffsetEnd=153 PartOfSpeech=IN Lemma=into]\n",
      "[Text=the CharacterOffsetBegin=154 CharacterOffsetEnd=157 PartOfSpeech=DT Lemma=the]\n",
      "[Text=related CharacterOffsetBegin=158 CharacterOffsetEnd=165 PartOfSpeech=JJ Lemma=related]\n",
      "[Text=field CharacterOffsetBegin=166 CharacterOffsetEnd=171 PartOfSpeech=NN Lemma=field]\n",
      "[Text=of CharacterOffsetBegin=172 CharacterOffsetEnd=174 PartOfSpeech=IN Lemma=of]\n",
      "[Text=Computational CharacterOffsetBegin=175 CharacterOffsetEnd=188 PartOfSpeech=NNP Lemma=Computational]\n",
      "[Text=Psycholinguistics CharacterOffsetBegin=189 CharacterOffsetEnd=206 PartOfSpeech=NNPS Lemma=Psycholinguistics]\n",
      "[Text=. CharacterOffsetBegin=206 CharacterOffsetEnd=207 PartOfSpeech=. Lemma=.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for l in nlp_lines(nlp_txt_path):\n",
    "    print(corenlp.annotate(l , properties = props))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 品詞の記号の説明\n",
    "\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-b6783275a32f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorenlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nlp100data/nlp.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "corenlp.url('nlp100data/nlp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing']\n",
      "['From', 'Wikipedia', ',', 'the', 'free', 'encyclopedia']\n",
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', ',', 'and', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', '.']\n",
      "['As', 'such', ',', 'NLP', 'is', 'related', 'to', 'the', 'area', 'of', 'humani-computer', 'interaction', '.']\n",
      "['Many', 'challenges', 'in', 'NLP', 'involve', 'natural', 'language', 'understanding', ',', 'that', 'is', ',', 'enabling', 'computers', 'to', 'derive', 'meaning', 'from', 'human', 'or', 'natural', 'language', 'input', ',', 'and', 'others', 'involve', 'natural', 'language', 'generation', '.']\n",
      "['History']\n",
      "['The', 'history', 'of', 'NLP', 'generally', 'starts', 'in', 'the', '1950s', ',', 'although', 'work', 'can', 'be', 'found', 'from', 'earlier', 'periods', '.']\n",
      "['In', '1950', ',', 'Alan', 'Turing', 'published', 'an', 'article', 'titled', '\"', 'Computing', 'Machinery', 'and', 'Intelligence', '\"', 'which', 'proposed', 'what', 'is', 'now', 'called', 'the', 'Turing', 'test', 'as', 'a', 'criterion', 'of', 'intelligence', '.']\n",
      "['The', 'Georgetown', 'experiment', 'in', '1954', 'involved', 'fully', 'automatic', 'translation', 'of', 'more', 'than', 'sixty', 'Russian', 'sentences', 'into', 'English', '.']\n",
      "['The', 'authors', 'claimed', 'that', 'within', 'three', 'or', 'five', 'years', ',', 'machine', 'translation', 'would', 'be', 'a', 'solved', 'problem', '.']\n",
      "['However', ',', 'real', 'progress', 'was', 'much', 'slower', ',', 'and', 'after', 'the', 'ALPAC', 'report', 'in', '1966', ',', 'which', 'found', 'that', 'ten', 'year', 'long', 'research', 'had', 'failed', 'to', 'fulfill', 'the', 'expectations', ',', 'funding', 'for', 'machine', 'translation', 'was', 'dramatically', 'reduced', '.']\n",
      "['Little', 'further', 'research', 'in', 'machine', 'translation', 'was', 'conducted', 'until', 'the', 'late', '1980s', ',', 'when', 'the', 'first', 'statistical', 'machine', 'translation', 'systems', 'were', 'developed', '.']\n",
      "['Some', 'notably', 'successful', 'NLP', 'systems', 'developed', 'in', 'the', '1960s', 'were', 'SHRDLU', ',', 'a', 'natural', 'language', 'system', 'working', 'in', 'restricted', '\"', 'blocks', 'worlds', '\"', 'with', 'restricted', 'vocabularies', ',', 'and', 'ELIZA', ',', 'a', 'simulation', 'of', 'a', 'Rogerian', 'psychotherapist', ',', 'written', 'by', 'Joseph', 'Weizenbaum', 'between', '1964', 'to', '1966', '.']\n",
      "['Using', 'almost', 'no', 'information', 'about', 'human', 'thought', 'or', 'emotion', ',', 'ELIZA', 'sometimes', 'provided', 'a', 'startlingly', 'human-like', 'interaction', '.']\n",
      "['When', 'the', '\"', 'patient', '\"', 'exceeded', 'the', 'very', 'small', 'knowledge', 'base', ',', 'ELIZA', 'might', 'provide', 'a', 'generic', 'response', ',', 'for', 'example', ',', 'responding', 'to', '\"', 'My', 'head', 'hurts', '\"', 'with', '\"', 'Why', 'do', 'you', 'say', 'your', 'head', 'hurts', '?', '\"', '.']\n",
      "['During', 'the', '1970s', 'many', 'programmers', 'began', 'to', 'write', \"'\", 'conceptual', 'ontologies', \"'\", ',', 'which', 'structured', 'real-world', 'information', 'into', 'computer-understandable', 'data', '.']\n",
      "['Examples', 'are', 'MARGIE', '(', 'Schank', ',', '1975', ')', ',', 'SAM', '(', 'Cullingford', ',', '1978', ')', ',', 'PAM', '(', 'Wilensky', ',', '1978', ')', ',', 'TaleSpin', '(', 'Meehan', ',', '1976', ')', ',', 'QUALM', '(', 'Lehnert', ',', '1977', ')', ',', 'Politics', '(', 'Carbonell', ',', '1979', ')', ',', 'and', 'Plot', 'Units', '(', 'Lehnert', '1981', ')', '.']\n",
      "['During', 'this', 'time', ',', 'many', 'chatterbots', 'were', 'written', 'including', 'PARRY', ',', 'Racter', ',', 'and', 'Jabberwacky', '.']\n",
      "['Up', 'to', 'the', '1980s', ',', 'most', 'NLP', 'systems', 'were', 'based', 'on', 'complex', 'sets', 'of', 'hand-written', 'rules', '.']\n",
      "['Starting', 'in', 'the', 'late', '1980s', ',', 'however', ',', 'there', 'was', 'a', 'revolution', 'in', 'NLP', 'with', 'the', 'introduction', 'of', 'machine', 'learning', 'algorithms', 'for', 'language', 'processing', '.']\n",
      "['This', 'was', 'due', 'to', 'both', 'the', 'steady', 'increase', 'in', 'computational', 'power', 'resulting', 'from', 'Moore', \"'s\", 'Law', 'and', 'the', 'gradual', 'lessening', 'of', 'the', 'dominance', 'of', 'Chomskyan', 'theories', 'of', 'linguistics', '(', 'e.g.', 'transformational', 'grammar', ')', ',', 'whose', 'theoretical', 'underpinnings', 'discouraged', 'the', 'sort', 'of', 'corpus', 'linguistics', 'that', 'underlies', 'the', 'machine-learning', 'approach', 'to', 'language', 'processing', '.']\n",
      "['Some', 'of', 'the', 'earliest-used', 'machine', 'learning', 'algorithms', ',', 'such', 'as', 'decision', 'trees', ',', 'produced', 'systems', 'of', 'hard', 'if-then', 'rules', 'similar', 'to', 'existing', 'hand-written', 'rules', '.']\n",
      "['However', ',', 'Part', 'of', 'speech', 'tagging', 'introduced', 'the', 'use', 'of', 'Hidden', 'Markov', 'Models', 'to', 'NLP', ',', 'and', 'increasingly', ',', 'research', 'has', 'focused', 'on', 'statistical', 'models', ',', 'which', 'make', 'soft', ',', 'probabilistic', 'decisions', 'based', 'on', 'attaching', 'real-valued', 'weights', 'to', 'the', 'features', 'making', 'up', 'the', 'input', 'data', '.']\n",
      "['The', 'cache', 'language', 'models', 'upon', 'which', 'many', 'speech', 'recognition', 'systems', 'now', 'rely', 'are', 'examples', 'of', 'such', 'statistical', 'models', '.']\n",
      "['Such', 'models', 'are', 'generally', 'more', 'robust', 'when', 'given', 'unfamiliar', 'input', ',', 'especially', 'input', 'that', 'contains', 'errors', '(', 'as', 'is', 'very', 'common', 'for', 'real-world', 'data', ')', ',', 'and', 'produce', 'more', 'reliable', 'results', 'when', 'integrated', 'into', 'a', 'larger', 'system', 'comprising', 'multiple', 'subtasks', '.']\n",
      "['Many', 'of', 'the', 'notable', 'early', 'successes', 'occurred', 'in', 'the', 'field', 'of', 'machine', 'translation', ',', 'due', 'especially', 'to', 'work', 'at', 'IBM', 'Research', ',', 'where', 'successively', 'more', 'complicated', 'statistical', 'models', 'were', 'developed', '.']\n",
      "['These', 'systems', 'were', 'able', 'to', 'take', 'advantage', 'of', 'existing', 'multilingual', 'textual', 'corpora', 'that', 'had', 'been', 'produced', 'by', 'the', 'Parliament', 'of', 'Canada', 'and', 'the', 'European', 'Union', 'as', 'a', 'result', 'of', 'laws', 'calling', 'for', 'the', 'translation', 'of', 'all', 'governmental', 'proceedings', 'into', 'all', 'official', 'languages', 'of', 'the', 'corresponding', 'systems', 'of', 'government', '.']\n",
      "['However', ',', 'most', 'other', 'systems', 'depended', 'on', 'corpora', 'specifically', 'developed', 'for', 'the', 'tasks', 'implemented', 'by', 'these', 'systems', ',', 'which', 'was', '(', 'and', 'often', 'continues', 'to', 'be', ')', 'a', 'major', 'limitation', 'in', 'the', 'success', 'of', 'these', 'systems', '.']\n",
      "['As', 'a', 'result', ',', 'a', 'great', 'deal', 'of', 'research', 'has', 'gone', 'into', 'methods', 'of', 'more', 'effectively', 'learning', 'from', 'limited', 'amounts', 'of', 'data', '.']\n",
      "['Recent', 'research', 'has', 'increasingly', 'focused', 'on', 'unsupervised', 'and', 'semi-supervised', 'learning', 'algorithms', '.']\n",
      "['Such', 'algorithms', 'are', 'able', 'to', 'learn', 'from', 'data', 'that', 'has', 'not', 'been', 'hand-annotated', 'with', 'the', 'desired', 'answers', ',', 'or', 'using', 'a', 'combination', 'of', 'annotated', 'and', 'non-annotated', 'data', '.']\n",
      "['Generally', ',', 'this', 'task', 'is', 'much', 'more', 'difficult', 'than', 'supervised', 'learning', ',', 'and', 'typically', 'produces', 'less', 'accurate', 'results', 'for', 'a', 'given', 'amount', 'of', 'input', 'data', '.']\n",
      "['However', ',', 'there', 'is', 'an', 'enormous', 'amount', 'of', 'non-annotated', 'data', 'available', '(', 'including', ',', 'among', 'other', 'things', ',', 'the', 'entire', 'content', 'of', 'the', 'World', 'Wide', 'Web', ')', ',', 'which', 'can', 'often', 'make', 'up', 'for', 'the', 'inferior', 'results', '.']\n",
      "['NLP', 'using', 'machine', 'learning']\n",
      "['Modern', 'NLP', 'algorithms', 'are', 'based', 'on', 'machine', 'learning', ',', 'especially', 'statistical', 'machine', 'learning', '.']\n",
      "['The', 'paradigm', 'of', 'machine', 'learning', 'is', 'different', 'from', 'that', 'of', 'most', 'prior', 'attempts', 'at', 'language', 'processing', '.']\n",
      "['Prior', 'implementations', 'of', 'language-processing', 'tasks', 'typically', 'involved', 'the', 'direct', 'hand', 'coding', 'of', 'large', 'sets', 'of', 'rules', '.']\n",
      "['The', 'machine-learning', 'paradigm', 'calls', 'instead', 'for', 'using', 'general', 'learning', 'algorithms', '-', 'often', ',', 'although', 'not', 'always', ',', 'grounded', 'in', 'statistical', 'inference', '-', 'to', 'automatically', 'learn', 'such', 'rules', 'through', 'the', 'analysis', 'of', 'large', 'corpora', 'of', 'typical', 'real-world', 'examples', '.']\n",
      "['A', 'corpus', '(', 'plural', ',', '\"', 'corpora', '\"', ')', 'is', 'a', 'set', 'of', 'documents', '(', 'or', 'sometimes', ',', 'individual', 'sentences', ')', 'that', 'have', 'been', 'hand-annotated', 'with', 'the', 'correct', 'values', 'to', 'be', 'learned', '.']\n",
      "['Many', 'different', 'classes', 'of', 'machine', 'learning', 'algorithms', 'have', 'been', 'applied', 'to', 'NLP', 'tasks', '.']\n",
      "['These', 'algorithms', 'take', 'as', 'input', 'a', 'large', 'set', 'of', '\"', 'features', '\"', 'that', 'are', 'generated', 'from', 'the', 'input', 'data', '.']\n",
      "['Some', 'of', 'the', 'earliest-used', 'algorithms', ',', 'such', 'as', 'decision', 'trees', ',', 'produced', 'systems', 'of', 'hard', 'if-then', 'rules', 'similar', 'to', 'the', 'systems', 'of', 'hand-written', 'rules', 'that', 'were', 'then', 'common', '.']\n",
      "['Increasingly', ',', 'however', ',', 'research', 'has', 'focused', 'on', 'statistical', 'models', ',', 'which', 'make', 'soft', ',', 'probabilistic', 'decisions', 'based', 'on', 'attaching', 'real-valued', 'weights', 'to', 'each', 'input', 'feature', '.']\n",
      "['Such', 'models', 'have', 'the', 'advantage', 'that', 'they', 'can', 'express', 'the', 'relative', 'certainty', 'of', 'many', 'different', 'possible', 'answers', 'rather', 'than', 'only', 'one', ',', 'producing', 'more', 'reliable', 'results', 'when', 'such', 'a', 'model', 'is', 'included', 'as', 'a', 'component', 'of', 'a', 'larger', 'system', '.']\n",
      "['Systems', 'based', 'on', 'machine-learning', 'algorithms', 'have', 'many', 'advantages', 'over', 'hand-produced', 'rules', ':']\n",
      "['The', 'learning', 'procedures', 'used', 'during', 'machine', 'learning', 'automatically', 'focus', 'on', 'the', 'most', 'common', 'cases', ',', 'whereas', 'when', 'writing', 'rules', 'by', 'hand', 'it', 'is', 'often', 'not', 'obvious', 'at', 'all', 'where', 'the', 'effort', 'should', 'be', 'directed', '.']\n",
      "['Automatic', 'learning', 'procedures', 'can', 'make', 'use', 'of', 'statistical', 'inference', 'algorithms', 'to', 'produce', 'models', 'that', 'are', 'robust', 'to', 'unfamiliar', 'input', '(', 'e.g.', 'containing', 'words', 'or', 'structures', 'that', 'have', 'not', 'been', 'seen', 'before', ')', 'and', 'to', 'erroneous', 'input', '(', 'e.g.', 'with', 'misspelled', 'words', 'or', 'words', 'accidentally', 'omitted', ')', '.']\n",
      "['Generally', ',', 'handling', 'such', 'input', 'gracefully', 'with', 'hand-written', 'rules', '--', 'or', 'more', 'generally', ',', 'creating', 'systems', 'of', 'hand-written', 'rules', 'that', 'make', 'soft', 'decisions', '--', 'extremely', 'difficult', ',', 'error-prone', 'and', 'time-consuming', '.']\n",
      "['Systems', 'based', 'on', 'automatically', 'learning', 'the', 'rules', 'can', 'be', 'made', 'more', 'accurate', 'simply', 'by', 'supplying', 'more', 'input', 'data', '.']\n",
      "['However', ',', 'systems', 'based', 'on', 'hand-written', 'rules', 'can', 'only', 'be', 'made', 'more', 'accurate', 'by', 'increasing', 'the', 'complexity', 'of', 'the', 'rules', ',', 'which', 'is', 'a', 'much', 'more', 'difficult', 'task', '.']\n",
      "['In', 'particular', ',', 'there', 'is', 'a', 'limit', 'to', 'the', 'complexity', 'of', 'systems', 'based', 'on', 'hand-crafted', 'rules', ',', 'beyond', 'which', 'the', 'systems', 'become', 'more', 'and', 'more', 'unmanageable', '.']\n",
      "['However', ',', 'creating', 'more', 'data', 'to', 'input', 'to', 'machine-learning', 'systems', 'simply', 'requires', 'a', 'corresponding', 'increase', 'in', 'the', 'number', 'of', 'man-hours', 'worked', ',', 'generally', 'without', 'significant', 'increases', 'in', 'the', 'complexity', 'of', 'the', 'annotation', 'process', '.']\n",
      "['The', 'subfield', 'of', 'NLP', 'devoted', 'to', 'learning', 'approaches', 'is', 'known', 'as', 'Natural', 'Language', 'Learning', '(', 'NLL', ')', 'and', 'its', 'conference', 'CoNLL', 'and', 'peak', 'body', 'SIGNLL', 'are', 'sponsored', 'by', 'ACL', ',', 'recognizing', 'also', 'their', 'links', 'with', 'Computational', 'Linguistics', 'and', 'Language', 'Acquisition', '.']\n",
      "['When', 'the', 'aims', 'of', 'computational', 'language', 'learning', 'research', 'is', 'to', 'understand', 'more', 'about', 'human', 'language', 'acquisition', ',', 'or', 'psycholinguistics', ',', 'NLL', 'overlaps', 'into', 'the', 'related', 'field', 'of', 'Computational', 'Psycholinguistics', '.']\n"
     ]
    }
   ],
   "source": [
    "for l in nlp_lines(nlp_txt_path):\n",
    "    print(corenlp.word_tokenize(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [0.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.8 sec].\n",
      "[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "[main] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.\n",
      "[main] INFO edu.stanford.nlp.time.TimeExpressionExtractorImpl - Using following SUTime rules: edu/stanford/nlp/models/sutime/defs.sutime.txt,edu/stanford/nlp/models/sutime/english.sutime.txt,edu/stanford/nlp/models/sutime/english.holidays.sutime.txt\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - TokensRegexNERAnnotator ner.fine.regexner: Read 580641 unique entries out of 581790 from edu/stanford/nlp/models/kbp/regexner_caseless.tab, 0 TokensRegex patterns.\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - TokensRegexNERAnnotator ner.fine.regexner: Read 4857 unique entries out of 4868 from edu/stanford/nlp/models/kbp/regexner_cased.tab, 0 TokensRegex patterns.\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokensRegexNERAnnotator - TokensRegexNERAnnotator ner.fine.regexner: Read 585498 unique entries from 2 files\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.2 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator dcoref\n",
      "Exception in thread \"main\" java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat sun.reflect.GeneratedSerializationConstructorAccessor78.newInstance(Unknown Source)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat java.io.ObjectStreamClass.newInstance(ObjectStreamClass.java:1079)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2051)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)\n",
      "\tat java.util.HashMap.readObject(HashMap.java:1407)\n",
      "\tat sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1158)\n",
      "\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2176)\n",
      "\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2067)\n",
      "\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1571)\n",
      "\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:431)\n",
      "\tat edu.stanford.nlp.io.IOUtils.readObjectFromURLOrClasspathOrFileSystem(IOUtils.java:309)\n",
      "\tat edu.stanford.nlp.dcoref.Dictionaries.loadGenderNumber(Dictionaries.java:393)\n",
      "\tat edu.stanford.nlp.dcoref.Dictionaries.<init>(Dictionaries.java:557)\n",
      "\tat edu.stanford.nlp.dcoref.Dictionaries.<init>(Dictionaries.java:466)\n",
      "\tat edu.stanford.nlp.dcoref.SieveCoreferenceSystem.<init>(SieveCoreferenceSystem.java:280)\n",
      "\tat edu.stanford.nlp.pipeline.DeterministicCorefAnnotator.<init>(DeterministicCorefAnnotator.java:66)\n",
      "\tat edu.stanford.nlp.pipeline.AnnotatorImplementations.dcoref(AnnotatorImplementations.java:203)\n",
      "\tat edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$getNamedAnnotators$52(StanfordCoreNLP.java:554)\n",
      "\tat edu.stanford.nlp.pipeline.StanfordCoreNLP$$Lambda$24/434176574.apply(Unknown Source)\n",
      "\tat edu.stanford.nlp.pipeline.StanfordCoreNLP.lambda$null$69(StanfordCoreNLP.java:625)\n",
      "\tat edu.stanford.nlp.pipeline.StanfordCoreNLP$$Lambda$38/1451043227.get(Unknown Source)\n",
      "\tat edu.stanford.nlp.util.Lazy$3.compute(Lazy.java:126)\n",
      "\tat edu.stanford.nlp.util.Lazy.get(Lazy.java:31)\n",
      "\tat edu.stanford.nlp.pipeline.AnnotatorPool.get(AnnotatorPool.java:149)\n",
      "\tat edu.stanford.nlp.pipeline.StanfordCoreNLP.construct(StanfordCoreNLP.java:495)\n",
      "\tat edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:201)\n",
      "\tat edu.stanford.nlp.pipeline.StanfordCoreNLP.<init>(StanfordCoreNLP.java:194)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#cd ~/Tools/stanford-corenlp-full-2018-02-27/\n",
    "java -cp \"/home/toshinao/Tools/stanford-corenlp-full-2018-02-27/*\" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file nlp100data/nlp.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = [s for i , s in enumerate(nlp_lines(nlp_txt_path)) if i < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence #1 (31 tokens):\n",
      "Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages.\n",
      "\n",
      "Tokens:\n",
      "[Text=Natural CharacterOffsetBegin=0 CharacterOffsetEnd=7 PartOfSpeech=JJ Lemma=natural]\n",
      "[Text=language CharacterOffsetBegin=8 CharacterOffsetEnd=16 PartOfSpeech=NN Lemma=language]\n",
      "[Text=processing CharacterOffsetBegin=17 CharacterOffsetEnd=27 PartOfSpeech=NN Lemma=processing]\n",
      "[Text=-LRB- CharacterOffsetBegin=28 CharacterOffsetEnd=29 PartOfSpeech=-LRB- Lemma=-lrb-]\n",
      "[Text=NLP CharacterOffsetBegin=29 CharacterOffsetEnd=32 PartOfSpeech=NN Lemma=nlp]\n",
      "[Text=-RRB- CharacterOffsetBegin=32 CharacterOffsetEnd=33 PartOfSpeech=-RRB- Lemma=-rrb-]\n",
      "[Text=is CharacterOffsetBegin=34 CharacterOffsetEnd=36 PartOfSpeech=VBZ Lemma=be]\n",
      "[Text=a CharacterOffsetBegin=37 CharacterOffsetEnd=38 PartOfSpeech=DT Lemma=a]\n",
      "[Text=field CharacterOffsetBegin=39 CharacterOffsetEnd=44 PartOfSpeech=NN Lemma=field]\n",
      "[Text=of CharacterOffsetBegin=45 CharacterOffsetEnd=47 PartOfSpeech=IN Lemma=of]\n",
      "[Text=computer CharacterOffsetBegin=48 CharacterOffsetEnd=56 PartOfSpeech=NN Lemma=computer]\n",
      "[Text=science CharacterOffsetBegin=57 CharacterOffsetEnd=64 PartOfSpeech=NN Lemma=science]\n",
      "[Text=, CharacterOffsetBegin=64 CharacterOffsetEnd=65 PartOfSpeech=, Lemma=,]\n",
      "[Text=artificial CharacterOffsetBegin=66 CharacterOffsetEnd=76 PartOfSpeech=JJ Lemma=artificial]\n",
      "[Text=intelligence CharacterOffsetBegin=77 CharacterOffsetEnd=89 PartOfSpeech=NN Lemma=intelligence]\n",
      "[Text=, CharacterOffsetBegin=89 CharacterOffsetEnd=90 PartOfSpeech=, Lemma=,]\n",
      "[Text=and CharacterOffsetBegin=91 CharacterOffsetEnd=94 PartOfSpeech=CC Lemma=and]\n",
      "[Text=linguistics CharacterOffsetBegin=95 CharacterOffsetEnd=106 PartOfSpeech=NNS Lemma=linguistics]\n",
      "[Text=concerned CharacterOffsetBegin=107 CharacterOffsetEnd=116 PartOfSpeech=VBN Lemma=concern]\n",
      "[Text=with CharacterOffsetBegin=117 CharacterOffsetEnd=121 PartOfSpeech=IN Lemma=with]\n",
      "[Text=the CharacterOffsetBegin=122 CharacterOffsetEnd=125 PartOfSpeech=DT Lemma=the]\n",
      "[Text=interactions CharacterOffsetBegin=126 CharacterOffsetEnd=138 PartOfSpeech=NNS Lemma=interaction]\n",
      "[Text=between CharacterOffsetBegin=139 CharacterOffsetEnd=146 PartOfSpeech=IN Lemma=between]\n",
      "[Text=computers CharacterOffsetBegin=147 CharacterOffsetEnd=156 PartOfSpeech=NNS Lemma=computer]\n",
      "[Text=and CharacterOffsetBegin=157 CharacterOffsetEnd=160 PartOfSpeech=CC Lemma=and]\n",
      "[Text=human CharacterOffsetBegin=161 CharacterOffsetEnd=166 PartOfSpeech=JJ Lemma=human]\n",
      "[Text=-LRB- CharacterOffsetBegin=167 CharacterOffsetEnd=168 PartOfSpeech=-LRB- Lemma=-lrb-]\n",
      "[Text=natural CharacterOffsetBegin=168 CharacterOffsetEnd=175 PartOfSpeech=JJ Lemma=natural]\n",
      "[Text=-RRB- CharacterOffsetBegin=175 CharacterOffsetEnd=176 PartOfSpeech=-RRB- Lemma=-rrb-]\n",
      "[Text=languages CharacterOffsetBegin=177 CharacterOffsetEnd=186 PartOfSpeech=NNS Lemma=language]\n",
      "[Text=. CharacterOffsetBegin=186 CharacterOffsetEnd=187 PartOfSpeech=. Lemma=.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(nlp.annotate(tmp[2] , properties = props))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP\n",
      "      (NP (JJ Natural) (NN language) (NN processing))\n",
      "      (PRN (-LRB- -LRB-)\n",
      "        (NP (NN NLP))\n",
      "        (-RRB- -RRB-)))\n",
      "    (VP (VBZ is)\n",
      "      (NP\n",
      "        (NP\n",
      "          (NP (DT a) (NN field))\n",
      "          (PP (IN of)\n",
      "            (NP (NN computer) (NN science))))\n",
      "        (, ,)\n",
      "        (NP (JJ artificial) (NN intelligence))\n",
      "        (, ,)\n",
      "        (CC and)\n",
      "        (NP\n",
      "          (NP (NNS linguistics))\n",
      "          (VP (VBN concerned)\n",
      "            (PP (IN with)\n",
      "              (NP\n",
      "                (NP (DT the) (NNS interactions))\n",
      "                (PP (IN between)\n",
      "                  (NP\n",
      "                    (NP (NNS computers))\n",
      "                    (CC and)\n",
      "                    (NP (JJ human) (-LRB- -LRB-) (JJ natural) (-RRB- -RRB-) (NNS languages))))))))))\n",
      "    (. .)))\n"
     ]
    }
   ],
   "source": [
    "print(nlp.parse(tmp[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: Could not find or load main class edu.stanford.nlp.pipeline.StanfordCoreNLP\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "java -cp \"~/Tools/stanford-corenlp-full-2018-02-27/*\" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file nlp100data/nlp.txt > nlp_parsed.xml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
