{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** データの入手・整形 ****\n",
    "\n",
    "文に関する極性分析の正解データを用い，以下の要領で正解データ（sentiment.txt）を作成せよ．\n",
    "\n",
    "rt-polarity.posの各行の先頭に\"+1 \"という文字列を追加する（極性ラベル\"+1\"とスペースに続けて肯定的な文の内容が続く）\n",
    "rt-polarity.negの各行の先頭に\"-1 \"という文字列を追加する（極性ラベル\"-1\"とスペースに続けて否定的な文の内容が続く）\n",
    "上述1と2の内容を結合（concatenate）し，行をランダムに並び替える\n",
    "\n",
    "sentiment.txtを作成したら，正例（肯定的な文）の数と負例（否定的な文）の数を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import codecs\n",
    "import random\n",
    "\n",
    "fname_pos = 'nlp100data/rt-polaritydata/rt-polarity.pos'\n",
    "fname_neg = 'nlp100data/rt-polaritydata/rt-polarity.neg'\n",
    "fname_smt = 'nlp100data/sentiment.txt'\n",
    "fencoding = 'cp1252'        # Windows-1252らしい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ポジティブデータの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(fname_pos, 'r', fencoding) as file_pos:\n",
    "    result.extend(['+1 {}'.format(line.strip()) for line in file_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of postive sentences is {}\".format(len(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_neg = []\n",
    "with codecs.open(fname_neg, 'r', fencoding) as file_neg:\n",
    "    result_neg.extend(['-1 {}'.format(line.strip()) for line in file_neg])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of negative sentences is {}\".format(len(result_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.extend(result_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total number of sentences is {}\".format(len(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "シャッフル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "書き出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(fname_smt, 'w', fencoding) as file_out:\n",
    "    print(*result, sep='\\n', file=file_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ストップワード\n",
    "\n",
    "英語のストップワードのリスト（ストップリスト）を適当に作成せよ．\n",
    "\n",
    "さらに，引数に与えられた単語（文字列）がストップリストに含まれている場合は真，それ以外は偽を返す関数を実装せよ．\n",
    "\n",
    "さらに，その関数に対するテストを記述せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ストップワードのリスト  http://xpo6.com/list-of-english-stop-words/ のCSV Formatより"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = (\n",
    "    'a,able,about,across,after,all,almost,also,am,among,an,and,any,are,'\n",
    "    'as,at,be,because,been,but,by,can,cannot,could,dear,did,do,does,'\n",
    "    'either,else,ever,every,for,from,get,got,had,has,have,he,her,hers,'\n",
    "    'him,his,how,however,i,if,in,into,is,it,its,just,least,let,like,'\n",
    "    'likely,may,me,might,most,must,my,neither,no,nor,not,of,off,often,'\n",
    "    'on,only,or,other,our,own,rather,said,say,says,she,should,since,so,'\n",
    "    'some,than,that,the,their,them,then,there,these,they,this,tis,to,too,'\n",
    "    'twas,us,wants,was,we,were,what,when,where,which,while,who,whom,why,'\n",
    "    'will,with,would,yet,you,your').lower().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_stopword(str):\n",
    "    '''文字がストップワードかどうかを返す\n",
    "    大小文字は同一視する\n",
    "\n",
    "    戻り値：\n",
    "    ストップワードならTrue、違う場合はFalse\n",
    "    '''\n",
    "    return str.lower() in stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-14282b1f4af6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mis_stopword\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tokyo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert is_stopword(\"Tokyo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_stopword(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 素性抽出\n",
    "\n",
    "極性分析に有用そうな素性を各自で設計し，学習データから素性を抽出せよ．\n",
    "\n",
    "素性としては，レビューからストップワードを除去し，各単語をステミング処理したものが最低限のベースラインとなるであろう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** coreNLPを使ったほうがよさそう。nlp100_50.ipynbの先頭を踏襲すればよい（？）****\n",
    "\n",
    "https://github.com/Lynten/stanford-corenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(fname_smt, 'r', fencoding) as rf:\n",
    "    for i , l in enumerate(rf):\n",
    "        for w in l[3:].split(\" \"):\n",
    "            print(w.strip())\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corenlp = StanfordCoreNLP(r'/home/toshinao/Tools/stanford-corenlp-full-2018-02-27')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "props={'annotators': 'lemma','pipelineLanguage':'en','outputFormat':'xml'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmls = list()\n",
    "with codecs.open(fname_smt, 'r', fencoding) as rf:\n",
    "    for l in rf:\n",
    "        run_xml = corenlp.annotate(l , properties=props)\n",
    "        xmls.append(run_xml)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(xmls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"x\" not in stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counter = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in xmls:\n",
    "    soup = BeautifulSoup(x, 'lxml')\n",
    "    for x in soup.find_all(\"lemma\"):\n",
    "        y = x.get_text()\n",
    "        if y not in stop_words:\n",
    "            word_counter.update([y])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [word for word, count in word_counter.items() if count >= 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of seletcted features is {}\".format(len(features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_features = \"nlp100data/features.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(fname_features, 'w', fencoding) as file_out:\n",
    "    print(*features, sep='\\n', file=file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict_features():\n",
    "    '''features.txtを読み込み、素性をインデックスに変換するための辞書を作成\n",
    "    インデックスの値は1ベースで、features.txtにおける行番号と一致する。\n",
    "\n",
    "    戻り値：\n",
    "    素性をインデックスに変換する辞書\n",
    "    '''\n",
    "    with codecs.open(fname_features, 'r', fencoding) as file_in:\n",
    "        return [l.strip() for l in file_in]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git commit -a -m \"from notebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = set()\n",
    "\n",
    "for x in xmls:\n",
    "    soup = BeautifulSoup(x, 'lxml')\n",
    "    for x in soup.find_all(\"lemma\"):\n",
    "        y = x.get_text()\n",
    "        if y not in stop_words:\n",
    "            features.add(y)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in soup.find_all(\"lemma\"):\n",
    "    print(x.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_xml = corenlp.annotate(l, properties=props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corenlp.parse(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Lynten/stanford-corenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "props={'annotators': 'tokenize,ssplit,pos','pipelineLanguage':'en','outputFormat':'xml'}\n",
    "print(corenlp.annotate(l, properties=props))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学習\n",
    "72で抽出した素性を用いて，ロジスティック回帰モデルを学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer as CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = load_dict_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CV(vocabulary=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x3441 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform(features[0:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(cv.transform([\"shibuya\"]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sum_list = []\n",
    "for x in xmls:\n",
    "    soup = BeautifulSoup(x, 'lxml')\n",
    "    lemmas = list()\n",
    "    for x in soup.find_all(\"lemma\"):\n",
    "        xt = x.get_text()\n",
    "        if xt not in stop_words:\n",
    "            lemmas.append(xt)\n",
    "    token_matrix = cv.transform(lemmas).toarray()\n",
    "    token_sum = token_matrix.sum(axis = 0).astype(float)\n",
    "    token_sum_list.append(token_sum.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-1', 'devoid', 'quality', 'make', 'first', 'film', 'special', '.']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    " token_matrix = cv.transform(lemmas).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token_matrix.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['critic',\n",
       " 'goofy',\n",
       " 'disjointed',\n",
       " 'originality',\n",
       " 'service',\n",
       " 'queen',\n",
       " 'magic',\n",
       " 'boldly',\n",
       " 'convenient',\n",
       " 'safe',\n",
       " 'lump',\n",
       " 'harris',\n",
       " 'inexplicable',\n",
       " '21st',\n",
       " 'hero',\n",
       " 'biopic',\n",
       " 'constantly',\n",
       " 'frame',\n",
       " 'shot',\n",
       " '&',\n",
       " 'chaotic',\n",
       " 'oddly',\n",
       " 'wry',\n",
       " 'swing',\n",
       " 'acting',\n",
       " 'distinctive',\n",
       " 'sever',\n",
       " 'chick',\n",
       " 'replete',\n",
       " 'father',\n",
       " 'elegant',\n",
       " 'kissinger',\n",
       " 'witherspoon',\n",
       " 'hokum',\n",
       " 'total',\n",
       " 'besides',\n",
       " 'ahead',\n",
       " 'jolie',\n",
       " 'woman',\n",
       " 'please',\n",
       " 'demented',\n",
       " 'divert',\n",
       " 'hip-hop',\n",
       " 'penetrate',\n",
       " 'fashion',\n",
       " 'fame',\n",
       " 'stroke',\n",
       " 'renner',\n",
       " 'hugh',\n",
       " 'none',\n",
       " 'birot',\n",
       " 'cleverly',\n",
       " 'handle',\n",
       " 'tap',\n",
       " 'visit',\n",
       " 'justice',\n",
       " 'tadpole',\n",
       " 'honest',\n",
       " 'comedy',\n",
       " 'complicate',\n",
       " 'edit',\n",
       " 'genre',\n",
       " 'grave',\n",
       " 'trust',\n",
       " 'around',\n",
       " 'convey',\n",
       " 'tooth',\n",
       " 'animation',\n",
       " 'compensate',\n",
       " 'disney',\n",
       " 'fart',\n",
       " 'recent',\n",
       " 'illustrate',\n",
       " 'rescue',\n",
       " 'sand',\n",
       " 'hugely',\n",
       " 'must-see',\n",
       " 'itself',\n",
       " 'capture',\n",
       " 'worse',\n",
       " 'portray',\n",
       " 'fast',\n",
       " 'didactic',\n",
       " 'being',\n",
       " 'label',\n",
       " 'b',\n",
       " 'velocity',\n",
       " 'christian',\n",
       " 'sappy',\n",
       " 'x',\n",
       " 'positively',\n",
       " 'classic',\n",
       " 'material',\n",
       " 'formulaic',\n",
       " 'humanity',\n",
       " 'episode',\n",
       " 'deal',\n",
       " 'job',\n",
       " 'punch-drunk',\n",
       " 'grow',\n",
       " 'witch',\n",
       " 'belong',\n",
       " 'successfully',\n",
       " 'faithful',\n",
       " 'tie',\n",
       " 'fast-paced',\n",
       " 'brown',\n",
       " 'quirky',\n",
       " 'possibility',\n",
       " 'sensational',\n",
       " 'killer',\n",
       " 'reading',\n",
       " 'lovingly',\n",
       " 'lane',\n",
       " 'hokey',\n",
       " 'escape',\n",
       " 'self-indulgent',\n",
       " 'image',\n",
       " 'eric',\n",
       " 'engrossing',\n",
       " 'awful',\n",
       " 'silly',\n",
       " 'lively',\n",
       " 'sympathetic',\n",
       " 'encounter',\n",
       " 'gun',\n",
       " 'private',\n",
       " 'bullock',\n",
       " 'creation',\n",
       " 'far',\n",
       " 'passable',\n",
       " 'substance',\n",
       " 'casual',\n",
       " 'later',\n",
       " 'crush',\n",
       " 'interpretation',\n",
       " 'somber',\n",
       " 'paranoia',\n",
       " 'way',\n",
       " 'attack',\n",
       " 'definition',\n",
       " 'piano',\n",
       " 'studio',\n",
       " 'extra',\n",
       " 'many',\n",
       " 'seat',\n",
       " 'first-time',\n",
       " 'card',\n",
       " 'lurid',\n",
       " 'track',\n",
       " 'unnecessary',\n",
       " 'even',\n",
       " 'spectacle',\n",
       " 'interest',\n",
       " 'memory',\n",
       " 'confront',\n",
       " 'local',\n",
       " 'completely',\n",
       " 'mamet',\n",
       " 'overwhelm',\n",
       " 'shout',\n",
       " 'big-screen',\n",
       " 'century',\n",
       " 'lead',\n",
       " 'bore',\n",
       " 'davis',\n",
       " 'bring',\n",
       " 'wilde',\n",
       " 'younger',\n",
       " 'punch',\n",
       " 'charmless',\n",
       " 'bride',\n",
       " 'honor',\n",
       " 'well-acted',\n",
       " 'piece',\n",
       " 'tour',\n",
       " 'kinnear',\n",
       " 'rough',\n",
       " 'intelligent',\n",
       " 'cho',\n",
       " 'few',\n",
       " 'danger',\n",
       " 'top-notch',\n",
       " 'period',\n",
       " 'stretch',\n",
       " 'journey',\n",
       " 'bit',\n",
       " 'shadow',\n",
       " 'forgive',\n",
       " 'lock',\n",
       " 'el',\n",
       " 'funeral',\n",
       " 'overlong',\n",
       " 'unoriginal',\n",
       " 'random',\n",
       " 'week',\n",
       " 'previous',\n",
       " 'worthy',\n",
       " 'seductive',\n",
       " 'disappoint',\n",
       " 'imitation',\n",
       " 'frontal',\n",
       " 'exception',\n",
       " 'horror',\n",
       " 'capacity',\n",
       " 'particular',\n",
       " 'speak',\n",
       " 'exhaust',\n",
       " 'resist',\n",
       " 'term',\n",
       " 'rank',\n",
       " 'confidence',\n",
       " 'alternative',\n",
       " 'unpleasant',\n",
       " 'technically',\n",
       " 'florid',\n",
       " 'timing',\n",
       " 'form',\n",
       " 'risk',\n",
       " 'seriously',\n",
       " 'hilarity',\n",
       " 'belly',\n",
       " 'exploitative',\n",
       " 'superior',\n",
       " 'clooney',\n",
       " 'ironically',\n",
       " 'field',\n",
       " 'pull',\n",
       " 'anderson',\n",
       " 'gorgeous',\n",
       " '30',\n",
       " 'jagger',\n",
       " 'heal',\n",
       " 'mike',\n",
       " 'simple',\n",
       " 'veteran',\n",
       " 'evoke',\n",
       " 'rip-off',\n",
       " 'happily',\n",
       " 'frantic',\n",
       " 'artsy',\n",
       " 'exploit',\n",
       " 'huge',\n",
       " 'nasty',\n",
       " 'worth',\n",
       " 'hoot',\n",
       " 'expense',\n",
       " 'sister',\n",
       " 'account',\n",
       " 'b-movie',\n",
       " 'conviction',\n",
       " 'philip',\n",
       " 'spice',\n",
       " 'sayle',\n",
       " 'commercial',\n",
       " 'five',\n",
       " 'noise',\n",
       " 'serve',\n",
       " 'soulless',\n",
       " 'silent',\n",
       " 'undeniably',\n",
       " 'matthew',\n",
       " 'snapshot',\n",
       " 'moviemaking',\n",
       " 'pace',\n",
       " 'downer',\n",
       " 'stir',\n",
       " 'destine',\n",
       " 'hawke',\n",
       " 'burst',\n",
       " 'canne',\n",
       " 'retread',\n",
       " 'daytime',\n",
       " 'sleep',\n",
       " 'viewer',\n",
       " 'excitement',\n",
       " 'woo',\n",
       " 'bog',\n",
       " 'pair',\n",
       " 'reno',\n",
       " 'dog',\n",
       " 'likable',\n",
       " 'notice',\n",
       " 'liberal',\n",
       " 'achieve',\n",
       " 'hopkin',\n",
       " 'revel',\n",
       " 'screenplay',\n",
       " 'frankly',\n",
       " 'squander',\n",
       " 'anywhere',\n",
       " 'lack',\n",
       " 'person',\n",
       " 'motion',\n",
       " 'appetite',\n",
       " 'producer',\n",
       " 'near',\n",
       " 'lousy',\n",
       " 'involve',\n",
       " 'conventional',\n",
       " 'appreciate',\n",
       " 'collapse',\n",
       " 'inconsequential',\n",
       " 'irish',\n",
       " 'yu',\n",
       " 'heavily',\n",
       " 'entirely',\n",
       " 'coherent',\n",
       " 'predictable',\n",
       " 'marriage',\n",
       " 'deliver',\n",
       " 'heaven',\n",
       " 'expand',\n",
       " 'possibly',\n",
       " 'fatal',\n",
       " 'means',\n",
       " 'conversation',\n",
       " 'trashy',\n",
       " 'french',\n",
       " 'bullet',\n",
       " '!',\n",
       " 'routine',\n",
       " 'elusive',\n",
       " 'spin',\n",
       " 'record',\n",
       " 'chri',\n",
       " 'waydowntown',\n",
       " 'perfect',\n",
       " 'direct',\n",
       " 'nice',\n",
       " 'left',\n",
       " 'repeat',\n",
       " 'eastwood',\n",
       " 'neat',\n",
       " 'push',\n",
       " 'tepid',\n",
       " 'disguise',\n",
       " 'timeless',\n",
       " 'endless',\n",
       " 'hotel',\n",
       " 'huppert',\n",
       " 'linger',\n",
       " 'emerge',\n",
       " 'live',\n",
       " 'send',\n",
       " 'life',\n",
       " 'christmas',\n",
       " 'sure',\n",
       " 'wickedly',\n",
       " 'painful',\n",
       " 'literary',\n",
       " 'disposable',\n",
       " 'cynicism',\n",
       " 'moviegoing',\n",
       " 'surface',\n",
       " 'suspect',\n",
       " 'uma',\n",
       " 'year',\n",
       " 'seek',\n",
       " 'thing',\n",
       " 'anthony',\n",
       " 'hill',\n",
       " 'sentence',\n",
       " 'special',\n",
       " 'reggio',\n",
       " 'suit',\n",
       " 'inner',\n",
       " 'change',\n",
       " 'paint',\n",
       " 'science',\n",
       " 'grab',\n",
       " 'aim',\n",
       " 'novel',\n",
       " 'mire',\n",
       " 'enough',\n",
       " 'cultural',\n",
       " 'necessarily',\n",
       " 'murder',\n",
       " 'vitality',\n",
       " 'beguiling',\n",
       " 'throw',\n",
       " 'greek',\n",
       " 'warning',\n",
       " 'monsoon',\n",
       " 'sit',\n",
       " 'production',\n",
       " 'fiction',\n",
       " 'town',\n",
       " 'quickly',\n",
       " 'pianist',\n",
       " 'obviously',\n",
       " 'chuckle',\n",
       " 'cheer',\n",
       " 'primarily',\n",
       " 'seldahl',\n",
       " 'common',\n",
       " 'loss',\n",
       " 'willis',\n",
       " 'such',\n",
       " 'undercut',\n",
       " 'depiction',\n",
       " 'funny',\n",
       " 'alabama',\n",
       " 'provoke',\n",
       " 'sexy',\n",
       " 'core',\n",
       " 'political',\n",
       " 'fundamental',\n",
       " 'hardly',\n",
       " 'bridge',\n",
       " 'jackson',\n",
       " 'melancholy',\n",
       " 'confuse',\n",
       " 'separate',\n",
       " 'fire',\n",
       " 'shanghai',\n",
       " 'bind',\n",
       " 'kline',\n",
       " 'fantastic',\n",
       " 'describe',\n",
       " 'blend',\n",
       " 'talent',\n",
       " 'incident',\n",
       " 'giant',\n",
       " 'open',\n",
       " 'donovan',\n",
       " 'shock',\n",
       " 'confession',\n",
       " 'argue',\n",
       " 'derivative',\n",
       " 'sordid',\n",
       " 'emphasize',\n",
       " 'lan',\n",
       " 'threaten',\n",
       " 'hide',\n",
       " 'weakness',\n",
       " 'series',\n",
       " 'herself',\n",
       " 'death',\n",
       " 'dream',\n",
       " 'light',\n",
       " 'gender',\n",
       " 'background',\n",
       " 'sens',\n",
       " 'adaptation',\n",
       " 'mindless',\n",
       " 'sluggish',\n",
       " 'potent',\n",
       " 'exotic',\n",
       " 'unexpected',\n",
       " 'incoherent',\n",
       " 'context',\n",
       " 'contain',\n",
       " 'folk',\n",
       " 'decade',\n",
       " 'object',\n",
       " 'prophecy',\n",
       " 'pastiche',\n",
       " 'macdowell',\n",
       " 'aside',\n",
       " 'freedom',\n",
       " 'meat',\n",
       " 'grip',\n",
       " 'fresh',\n",
       " 'brutal',\n",
       " 'simply',\n",
       " 'ballistic',\n",
       " 'chest',\n",
       " 'shortcoming',\n",
       " 'cold',\n",
       " 'celebrity',\n",
       " 'western',\n",
       " 'imagine',\n",
       " 'spike',\n",
       " 'thousand',\n",
       " 'shrewd',\n",
       " 'whose',\n",
       " 'delicious',\n",
       " 'resemble',\n",
       " 'depict',\n",
       " 'fool',\n",
       " 'talky',\n",
       " 'ideal',\n",
       " 'believe',\n",
       " 'isolation',\n",
       " '$',\n",
       " 'idea',\n",
       " 'terribly',\n",
       " 'student',\n",
       " 'key',\n",
       " 'guilty',\n",
       " 'attractive',\n",
       " 'wisdom',\n",
       " 'compose',\n",
       " 'improvise',\n",
       " 'brilliance',\n",
       " 'benefit',\n",
       " 'quite',\n",
       " 'philosophical',\n",
       " 'living',\n",
       " 'struggle',\n",
       " 'colorful',\n",
       " 'pokemon',\n",
       " 'return',\n",
       " 'binoche',\n",
       " 'bourne',\n",
       " 'entry',\n",
       " 'ad',\n",
       " 'coming-of-age',\n",
       " 'gritty',\n",
       " 'nonetheless',\n",
       " 'tom',\n",
       " 'thematic',\n",
       " 'costner',\n",
       " 'keep',\n",
       " 'j',\n",
       " 'vibrant',\n",
       " 'traveler',\n",
       " 'deuce',\n",
       " 'eck',\n",
       " 'sincere',\n",
       " 'exciting',\n",
       " 'detail',\n",
       " 'guess',\n",
       " 'aspiration',\n",
       " 'refusal',\n",
       " 'jealousy',\n",
       " 'aid',\n",
       " 'virtually',\n",
       " 'smooth',\n",
       " 'crowd',\n",
       " 'pink',\n",
       " 'sad',\n",
       " 'pun',\n",
       " 'bill',\n",
       " 'york',\n",
       " 'high',\n",
       " 'suspend',\n",
       " 'directorial',\n",
       " 'pose',\n",
       " 'female',\n",
       " 'mechanic',\n",
       " \"'\",\n",
       " 'op',\n",
       " 'articulate',\n",
       " 'unsentimental',\n",
       " 'brilliantly',\n",
       " 'traffic',\n",
       " 'away',\n",
       " 'achievement',\n",
       " 'strangely',\n",
       " 'examine',\n",
       " 'chilly',\n",
       " 'con',\n",
       " 'affair',\n",
       " 'drivel',\n",
       " 'exhilarating',\n",
       " 'dull',\n",
       " 'spread',\n",
       " 'satisfying',\n",
       " 'unfaithful',\n",
       " 'owe',\n",
       " 'remember',\n",
       " 'artifice',\n",
       " 'tv',\n",
       " 'inconsistent',\n",
       " 'somehow',\n",
       " 'during',\n",
       " 'handsome',\n",
       " 'upon',\n",
       " 'fat',\n",
       " 'easier',\n",
       " 'schmaltzy',\n",
       " 'mystery',\n",
       " 'choose',\n",
       " 'well-crafted',\n",
       " 'seinfeld',\n",
       " 'control',\n",
       " 'anchor',\n",
       " 'cross',\n",
       " 'reward',\n",
       " 'except',\n",
       " 'outcome',\n",
       " 'store',\n",
       " 'care',\n",
       " ',',\n",
       " 'cram',\n",
       " 'hard',\n",
       " 'happy',\n",
       " 'feat',\n",
       " 'mildly',\n",
       " 'ya',\n",
       " 'satire',\n",
       " 'attract',\n",
       " 'distract',\n",
       " 'morvern',\n",
       " 'trademark',\n",
       " 'feeling',\n",
       " 'sisterhood',\n",
       " 'deep',\n",
       " 'ironic',\n",
       " 'foot',\n",
       " 'indulgent',\n",
       " 'showcase',\n",
       " 'crystal',\n",
       " 'camouflage',\n",
       " 'notch',\n",
       " 'face',\n",
       " 'range',\n",
       " 'rival',\n",
       " 'wood',\n",
       " 'television',\n",
       " 'predictably',\n",
       " 'lazy',\n",
       " 'make',\n",
       " 'despite',\n",
       " 'execution',\n",
       " 'taylor',\n",
       " '40',\n",
       " 'promising',\n",
       " 'word',\n",
       " 'observation',\n",
       " 'cruel',\n",
       " 'spooky',\n",
       " 'thought-provoking',\n",
       " 'convincing',\n",
       " 'tone',\n",
       " 'slip',\n",
       " 'hot',\n",
       " 'splendid',\n",
       " 'faster',\n",
       " 'soon',\n",
       " 'quick',\n",
       " 'u',\n",
       " 'crew',\n",
       " 'tune',\n",
       " 'shallow',\n",
       " 'naturalistic',\n",
       " 'feel-good',\n",
       " 'talk',\n",
       " 'flow',\n",
       " 'culture',\n",
       " 'pale',\n",
       " 'super',\n",
       " 'need',\n",
       " 'enjoy',\n",
       " 'similarly',\n",
       " 'infuse',\n",
       " 'satisfy',\n",
       " 'flair',\n",
       " 'badly',\n",
       " 'upper',\n",
       " 'slapstick',\n",
       " 'suspenseful',\n",
       " 'perspective',\n",
       " 'guy',\n",
       " 'stand',\n",
       " 'monotonous',\n",
       " 'deserve',\n",
       " 'initial',\n",
       " 'land',\n",
       " 'pleasure',\n",
       " 'current',\n",
       " 'savvy',\n",
       " 'spirit',\n",
       " 'viewing',\n",
       " 'longing',\n",
       " 'uncanny',\n",
       " '...',\n",
       " 'unfocused',\n",
       " 'frightening',\n",
       " 'favor',\n",
       " 'slick',\n",
       " 'w',\n",
       " 'require',\n",
       " 'visually',\n",
       " 'coppola',\n",
       " 'relationship',\n",
       " 'roman',\n",
       " 'murderous',\n",
       " 'shoot',\n",
       " 'brosnan',\n",
       " 'newcomer',\n",
       " 'ending',\n",
       " 'advice',\n",
       " 'failure',\n",
       " 'addition',\n",
       " 'uneven',\n",
       " 'distasteful',\n",
       " 'merit',\n",
       " 'endeavor',\n",
       " 'unconvincing',\n",
       " 'audacious',\n",
       " 'release',\n",
       " 'stop',\n",
       " 'cut',\n",
       " 'deception',\n",
       " 'office',\n",
       " 'triumph',\n",
       " 'ago',\n",
       " 'byler',\n",
       " 'garbage',\n",
       " 'inoffensive',\n",
       " 'unforced',\n",
       " 'letter',\n",
       " 'ground',\n",
       " 'look',\n",
       " 'together',\n",
       " 'worry',\n",
       " 'team',\n",
       " 'romp',\n",
       " 'reality',\n",
       " 'prove',\n",
       " 'refreshingly',\n",
       " 'ram',\n",
       " 'chateau',\n",
       " 'admit',\n",
       " 'bogus',\n",
       " 'joyless',\n",
       " 'finally',\n",
       " 'trap',\n",
       " 'cute',\n",
       " 'define',\n",
       " 'nevertheless',\n",
       " 'dramatically',\n",
       " 'attraction',\n",
       " 'brim',\n",
       " 'example',\n",
       " 'plight',\n",
       " 'exploration',\n",
       " 'desperately',\n",
       " 'closely',\n",
       " 'picture',\n",
       " 'trailer',\n",
       " 'check',\n",
       " 'transform',\n",
       " 'construct',\n",
       " 'take',\n",
       " 'balance',\n",
       " 'experience',\n",
       " 'beat',\n",
       " 'quit',\n",
       " 'development',\n",
       " 'subtly',\n",
       " 'obnoxious',\n",
       " 'charm',\n",
       " 'uncomfortably',\n",
       " 'transcend',\n",
       " 'concerned',\n",
       " 'frustrating',\n",
       " 'ordinary',\n",
       " 'inventive',\n",
       " 'prospect',\n",
       " 'plodding',\n",
       " 'pretentious',\n",
       " 'stand-up',\n",
       " 'careful',\n",
       " 'nichola',\n",
       " 'table',\n",
       " 'mouth',\n",
       " 'tender',\n",
       " 'eat',\n",
       " 'wit',\n",
       " 'game',\n",
       " 'provocative',\n",
       " 'amuse',\n",
       " 'atmosphere',\n",
       " 'attempt',\n",
       " 'labute',\n",
       " 'instinct',\n",
       " 'jessica',\n",
       " 'different',\n",
       " 'cinta',\n",
       " 'box',\n",
       " 'sensitive',\n",
       " 'pray',\n",
       " 'die',\n",
       " 'extend',\n",
       " 'miss',\n",
       " 'public',\n",
       " 'squarely',\n",
       " 'pratfall',\n",
       " 'forth',\n",
       " 'ruin',\n",
       " 'lopez',\n",
       " 'invention',\n",
       " 'apparent',\n",
       " 'unsatisfying',\n",
       " 'asian',\n",
       " 'constant',\n",
       " '90',\n",
       " 'clunker',\n",
       " 'religious',\n",
       " 'exquisite',\n",
       " 'scene',\n",
       " 'entertaining',\n",
       " 'lifestyle',\n",
       " 'nuanced',\n",
       " 'sign',\n",
       " 'host',\n",
       " 'ask',\n",
       " 'tuxedo',\n",
       " 'reunion',\n",
       " 'season',\n",
       " 'dense',\n",
       " 'droll',\n",
       " 'stage',\n",
       " 'consequence',\n",
       " 'tsaus',\n",
       " 'scary',\n",
       " 'handful',\n",
       " 'tend',\n",
       " 'peculiar',\n",
       " 'fact',\n",
       " 'call',\n",
       " 'run',\n",
       " 'robin',\n",
       " 'inane',\n",
       " 'hannibal',\n",
       " 'so-so',\n",
       " 'vanity',\n",
       " 'twice',\n",
       " 'closer',\n",
       " 'sundance',\n",
       " 'dynamic',\n",
       " 'pedestrian',\n",
       " 'sentimentality',\n",
       " 'undoubtedly',\n",
       " 'fellow',\n",
       " 'afloat',\n",
       " 'schneider',\n",
       " 'chamber',\n",
       " 'device',\n",
       " 'effect',\n",
       " 'hayne',\n",
       " 'chabrol',\n",
       " 's',\n",
       " 'bruckheimer',\n",
       " 'depth',\n",
       " 'lesson',\n",
       " 'distinguish',\n",
       " 'haphazard',\n",
       " 'text',\n",
       " 'lady',\n",
       " 'stay',\n",
       " 'insanely',\n",
       " 'exude',\n",
       " 'partner',\n",
       " 'lie',\n",
       " 'view',\n",
       " 'touch',\n",
       " 'earth',\n",
       " 'smarter',\n",
       " 'plain',\n",
       " 'magical',\n",
       " 'expression',\n",
       " 'russell',\n",
       " 'program',\n",
       " 'sharp',\n",
       " 'true',\n",
       " 'counterpart',\n",
       " 'max',\n",
       " 'present',\n",
       " 'formula',\n",
       " 'count',\n",
       " 'herzog',\n",
       " 'share',\n",
       " 'washington',\n",
       " 'installment',\n",
       " 'una',\n",
       " 'uneasy',\n",
       " 'benjamin',\n",
       " 'credible',\n",
       " 'combination',\n",
       " 'jewish',\n",
       " 'west',\n",
       " 'happiness',\n",
       " 'concern',\n",
       " 'format',\n",
       " 'torture',\n",
       " 'lackluster',\n",
       " 'sacrifice',\n",
       " 'mad',\n",
       " 'spite',\n",
       " 'stunning',\n",
       " 'erotic',\n",
       " 'momentum',\n",
       " 'rock',\n",
       " '10',\n",
       " '100',\n",
       " 'travel',\n",
       " 'invite',\n",
       " 'finale',\n",
       " 'old',\n",
       " 'answer',\n",
       " 'cube',\n",
       " 'fill',\n",
       " 'revelation',\n",
       " 'stereotypical',\n",
       " 'typical',\n",
       " 'camp',\n",
       " 'sex',\n",
       " 'create',\n",
       " 'masterfully',\n",
       " 'blame',\n",
       " 'translation',\n",
       " 'haunting',\n",
       " 'charle',\n",
       " 'school',\n",
       " 'marquis',\n",
       " 'drink',\n",
       " 'earnest',\n",
       " 'elling',\n",
       " 'due',\n",
       " 'lot',\n",
       " 'dimension',\n",
       " 'japanese',\n",
       " 'dreadful',\n",
       " '90-minute',\n",
       " 'charming',\n",
       " 'sketch',\n",
       " 'disbelief',\n",
       " 'chinese',\n",
       " 'skill',\n",
       " 'portrait',\n",
       " 'desire',\n",
       " 'memorable',\n",
       " 'shelf',\n",
       " 'depths',\n",
       " 'sequel',\n",
       " 'boat',\n",
       " 'reference',\n",
       " 'extraordinary',\n",
       " 'empire',\n",
       " 'chance',\n",
       " 'theory',\n",
       " 'sweet',\n",
       " 'assemble',\n",
       " 'swim',\n",
       " 'author',\n",
       " 'repetition',\n",
       " 'crime',\n",
       " 'sense',\n",
       " 'hair',\n",
       " 'fly',\n",
       " 'truly',\n",
       " 'rental',\n",
       " 'root',\n",
       " 'slow',\n",
       " 'understate',\n",
       " 'irwin',\n",
       " 'short',\n",
       " 'manhattan',\n",
       " 'wife',\n",
       " 'runner',\n",
       " 'book',\n",
       " 'morally',\n",
       " 'yourself',\n",
       " 'version',\n",
       " 'elaborate',\n",
       " '20th',\n",
       " 'soggy',\n",
       " 'shine',\n",
       " 'treatment',\n",
       " 'abstract',\n",
       " 'convoluted',\n",
       " 'increasingly',\n",
       " 'influence',\n",
       " 'low-budget',\n",
       " 'entire',\n",
       " 'I',\n",
       " 'industry',\n",
       " 'collision',\n",
       " 'blind',\n",
       " 'dead',\n",
       " 'mattei',\n",
       " 'slap',\n",
       " 'blockbuster',\n",
       " 'positive',\n",
       " 'argument',\n",
       " 'although',\n",
       " 'utter',\n",
       " 'meaningless',\n",
       " 'buoyant',\n",
       " 'impeccable',\n",
       " 'morning',\n",
       " 'intense',\n",
       " 'mention',\n",
       " 'police',\n",
       " 'thin',\n",
       " 'unsettling',\n",
       " 'four',\n",
       " 'buddy',\n",
       " 'vein',\n",
       " 'fish',\n",
       " 'outside',\n",
       " 'edgy',\n",
       " 'chelsea',\n",
       " 'xxx',\n",
       " 'expertly',\n",
       " ...]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token_sum_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sum_matrix = np.array(token_sum_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10662, 3441)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_sum_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(\"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(fname_smt, 'r', fencoding) as rf:\n",
    "    y = [float(l[0:2]) for l in rf ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10662,)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 回帰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://ailaby.com/logistic_reg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X = token_sum_matrix , y = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87263177640217593"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X=token_sum_matrix , y = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_sum.astype(float)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(token_sum_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token_sum_matrix[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 2)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=0)\n",
    "X_0 = np.random.multivariate_normal( [2,2],  [[2,0],[0,2]],  50 )\n",
    "y_0 = np.zeros(len(X_0))\n",
    " \n",
    "X_1 = np.random.multivariate_normal( [6,7],  [[3,0],[0,3]],  50 )\n",
    "y_1 = np.ones(len(X_1))\n",
    " \n",
    "X = np.vstack((X_0, X_1))\n",
    "ytmp = np.append(y_0, y_1)\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ytmp, test_size=0.3)\n",
    " \n",
    "# 特徴データを標準化(平均 0、標準偏差 1)\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://note.nkmk.me/python-numpy-eye-identity-one-hot/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.eye(10)[[1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.to_categorical()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xmls[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(x, 'lxml')\n",
    "lemmas = list()\n",
    "for x in soup.find_all(\"lemma\"):\n",
    "    y = x.get_text()\n",
    "    if y not in stop_words:\n",
    "        lemmas.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = cv.transform(lemmas).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.sum(axis = 0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.sum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 52ca2b3] from notebook\n",
      " 1 file changed, 970 insertions(+), 226532 deletions(-)\n",
      " rewrite nlp100_70.ipynb (99%)\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "git commit -a -m\"from notebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
